# Come valutare e rifinire la soluzione fattoriale {#ch:val_sol_fattoriale}

```{r, include = FALSE}
source("_common.R")
library("lavaan")
```

## Valutazione della matrice pattern

La maggior parte di strumenti usati nell'assessment psicologico e
neuropsicologico non valuta una singola dimensione psicologica, ma
piuttosto misura molteplici aspetti di un costrutto. Di conseguenza,
l'analisi fattoriale produce solitamente una soluzione a più 
fattori. Idealmente, dopo la rotazione, ciascun item saturerà fortemente
su un singolo fattore e debolmente sugli altri. In realtà, anche dopo la
rotazione degli assi fattoriali, spesso si presentano item che
saturano debolmente su tutti i fattori, oppure item che saturano
fortemente su più di un fattore.

Uno dei primi passi da compiere per  rifinire la soluzione
fattoriale è quello di valutare la matrice struttura e intervenire utilizzando il 
criterio della "struttura semplice", per poi valutare gli effetti delle
azioni intraprese (es., eliminazione di alcuni item) nella matrice pattern.
Ricordiamo che la matrice struttura contiene le correlazioni tra item
e fattori, mentre la matrice pattern contiene le saturazioni fattoriali.

### Item con basse saturazioni su tutti i fattori

Prima di procedere con l'analisi fattoriale è auspicabile esaminare la
matrice di correlazioni tra gli item ed eliminare quegli item che sono
insufficientemente correlati con gli altri item della matrice.
Tuttavia, anche dopo questo screening iniziale, è possibile che vi siano item
caratterizzati da saturazioni basse su tutti i fattori. Dal punto di vista pratico,
si considerano "basse" le saturazioni il cui valore assoluto è
minore di 0.30 (Hair et al., 1995). Hair e collaboratori suggeriscono
due soluzioni nel caso di item con  saturazioni basse su tutti i fattori:

- eliminare gli item con basse saturazioni, 
- valutare le comunalità degli item problematici e il contributo specifico che forniscono allo strumento. 

Se un item ha una bassa comunalità, o se il contributo di un item nei confronti del significato generale dello strumento è di poca importanza, allora l'item dovrebbe essere eliminato. Dopo l'eliminazione degli item critici, si procede calcolando una nuova soluzione fattoriale e si esaminano i risultati ottenuti.

Se vi sono degli item con basse saturazioni su tutti i fattori che però
contribuiscono in maniera importante a determinare il significato della
scala nel suo complesso, allora questi item dovrebbero essere mantenuti.
Alle volte, per tali item è possibile creare delle sottoscale separate
dalle altre.

### Item con saturazioni evevate su più di un fattore

È comune anche il caso opposto, ovvero quello nel quale ci sono item che saturano su fattori multipli (con saturazioni fattoriali $>$ .30), specialmente nel caso di soluzioni fattoriali ottenutie dopo una rotazione obliqua. Kline (2000) suggerisce di eliminare tali item in quanto rendono difficile da interpretare il significato della scala che così si ottiene. Hair e collaboratori (1995) ritengono invece che tali
item dovrebbero essere mantenuti, dato possono chiarire il significato dei
fattori che la scala identifica.

## Valutazione dell'attendibilità

All'interno del problema della costruzione di uno strumento vengono
esaminati tre aspetti dell'attendibilità: la consistenza interna, la
stabilità e l'equivalenza.

### Consistenza interna

#### La procedura split-half

La consistenza interna misura il grado di coerenza tra gli item che
costituiscono lo strumento o le sottoscale dello strumento. Se tutti gli
item che costituiscono uno strumento o una sua sottoscala misurano la
stessa cosa, allora saranno fortemente associati tra loro. 

È possibile misurare la consistenza interna con il metodo dello split-half, ovvero mediante la correlazione di Pearson tra i punteggi ottenuti
utilizzando ciascuna delle due metà degli item dello strumento. Usando
un software, è meglio trovare la media delle correlazioni inter-item
ricavabili a partire da tutte le possibili divisioni a metà dell'insieme
di item che costituiscono lo strumento. La correlazione trovata in
questo modo viene poi corretta utilizzando la formula "profetica" di
Spearman-Brown per tenere in considerazione il fatto che l'attendibilità
è stata calcolata utilizzando soltanto metà degli item dello strumento.

Si noti che la formula di Spearman-Brown è basata sull'assunzione che le
due metà dello strumento siano parallele, ovvero che abbiano identici
punteggi veri e uguali varianze d'errore (questa assunzione comporta la
conseguenza per cui le due metà degli item devono producono punteggi aventi la stessa media e la stessa varianza). Se queste assunzioni molto stringenti non vengono
soddisfatte, allora la procedura descritta sopra conduce ad una sovrastima dell'attendibilità quale consisenza interna della scala.

#### L'analisi della varianza

Se tutti gli item di uno strumento o di una sottoscala sono espressione
dello stesso costrutto, allora ci dobbiamo aspettare che anche le medie
dei punteggi sugli item siano uguali. Come è stato detto sopra, questa è
infatti una delle assunzioni delle forme strettamente parallele di un
test. È dunque possibile verificare questa assunzione mediante un'ANOVA
che sottopone a test l'ipotesi nulla dell'uguaglianza delle
medie di gruppi. Nel caso degli item di un test, dato che ciascun soggetto
completa tutti gli item che costituiscono lo strumento, è appropriato
usare un'ANOVA per misure ripetute che, nella sua declinazione più
moderna, corrisponde ad un modello multi-livello (*mixed-effect model*).

#### L'indice $\alpha$ di Cronbach

L'indice $\alpha$ di Cronbach è comunque la misura più utilizzata per valutare
l'attendibilità quale consistenza interna di uno strumento. L'$\alpha$
di Cronbach è stato interpretato come la proporzione di varianza della
scala che può essere attribuita al fattore comune (DeVellis, 1991). Può
anche essere interpretato come la correlazione stimata tra i punteggi
della scala e un'altro strumento della stessa lunghezza tratto
dall'universo degli item possibili che costituiscono il dominio del
costrutto (Kline, 1986). La radice quadrata del coefficiente $\alpha$ di
Cronbach rappresenta la correlazione stimata tra i punteggi ottenuti
tramite lo strumento e i punteggi veri (Nunnally & Bernstein, 1994).

In precedenza abbiamo descritto una serie di limiti del coefficiente $\alpha$ di Cronbach. In generale, molti ricercatori suggeriscono di usare al suo posto l'indice $\omega$ di McDonald.

### Stabilità temporale

La stabilità temporale viene valutata attraverso la procedura di
test-retest. La correlazione tra le misure ottenute in due momenti negli
stessi rispondenti ci fornisce l'attendibilità di test-retest.

Kline (2000) ha messo in evidenza come l'attendibilità di test-retest
sia influenzata da molteplici fattori, tra cui le caratteristiche del
campione, la maturità dei rispondenti, i cambiamenti nello stato
emozionale, le differenze nelle condizioni di somministrazione del test,
la possibilità di ricordare le risposte date in precedenza, la
difficoltà degli item, la grandezza del campione e le caratteristiche
del costrutto (ad esempio, stato vs. tratto).

Particolare attenzione deve essere rivolta all'intervallo temporale
usato nella procedura di test-retest. Se il periodo di tempo che
intercorre tra le due somministrazioni è troppo corto, i risultati
possono risultare distorti a causa del fatto che i soggetti si ricordano
le risposte date in precedenza. Questo può condurre ad una sovrastima
dell'attendibilità test-retest (Pedhazur & Schmelkin, 1991). Un
intervallo temporale troppo lungo tra le due somministrazioni ha invece
come limite il fatto che, in questo caso, vi è un'alta possibilità che
intervengano dei cambiamenti nei rispondenti rispetto al costrutto in
esame. Alla luce di queste considerazioni è stato suggerito di
utilizzare un intervallo temporale abbastanza breve, ovvero di una o due
settimane (Nunnally & Bernstein, 1994; Pedhazur & Schmelkin, 1991). Se è
necessario valutare la stabilità temporale nel corso di un lungo arco
temporale, Nunnally e Bernstein (1994) suggeriscono di utilizzare un
intervallo di sei mesi o maggiore.

### Equivalenza

Per cercare di evitare i problemi associati all'attendibilità quale
stabilità temporale, alcuni autori si sono posti il problema di
esaminare la correlazione tra forme parallele (o equivalenti) dello
strumento. La correlazione tra forme parallele di uno strumento va sotto
il nome di coefficiente di equivalenza e fornisce una misura alternativa
dell'attendibilità dello strumento (Burns & Grove, 2001; Pedhazur &
Schmelkin, 1991; Polit & Hungler, 1999).

Nunnally e Bernstein (1994) suggeriscono di confrontare i risultati
ottenuti con la somministrazione delle forme parallele lo stesso giorno
con quelli ottenuti nel caso di un intervallo temporale di due
settimane. Kline (2000) ritiene che l'attendibilità tra due forme
parallele debba essere di almeno 0.9 perché, per valori inferiori,
sarebbe difficile sostenere che le forme sono veramente parallele.

È tuttavia molto oneroso predisporre due forme parallele di uno
strumento. Per questa ragione, il coefficiente di equivalenza viene
raramente usato.

## Selezione di un sottoinsieme di item

Tipicamente, la costruzione di un test viene realizzata somministrando
un grande numero di item per poi selezionare gli item "migliori" che
andranno a fare parte del test vero e proprio. Si supponga di
somministrare inizialmente $m$ item, quando si desidera che il test
finale sia costituito da $p < m$ item. Un modo di affrontare questo
problema potrebbe essere quello di calcolare l'attendibilità del test
(coefficiente $\omega$) per tutti i possibili sottoinsiemi di $p$ item,
così da individuare il sottoinsieme migliore. Questo modo di procedere,
però, è problematico perché richiede la valutazione di un elevatissimo
numero di possibilità. Per esempio, da un insieme iniziale neanche
troppo numeroso di 100 item, il numero di sottoinsiemi di 20 item è
uguale a 

$$
\binom{100}{20} = 5.36 \times 10^{20}.
$$ 

È dunque necessario trovare metodi alternativi che evitino una tale esplosione combinatoria. A questo fine, ovvero per procedere alla selezione del sottoinsieme dei  "migliori" item, @mcdonald2013test suggerisce di calcolare la *quantità di informazione* di ciascun item. La quantità di informazione di un item è definita come rapporto tra segnale/rumore, in relazione alla scomposizione della varianza dell'item:

$$
\frac{\lambda_i^2}{\psi_{ii}}.
$$ 

@mcdonald2013test mostra come l'omissione di uno o più item produce sempre una riduzione dell'attendibilità del test (ovvero, una riduzione nel valore del coefficiente $\omega$). Tuttavia, tale riduzione è tanto più piccola quanto più
piccola è la quantità di informazione degli item omessi. Il processo di
selezione degli item può dunque essere guidato da un semplice principio:
si selezionano gli item aventi la quantità di informazione maggiore. Ovvero, in altre parole, si rimuovono gli item aventi la quantità di informazione più bassa.

::: {.example}
Per fare un esempio, consideriamo nuovamente la matrice di varianze e di covarianze della scala SWLS.

```{r}
varnames <- c("Y1", "Y2", "Y3", "Y4", "Y5")
SWLS <- matrix(c(
  2.565, 1.424, 1.481, 1.328, 1.529,
  1.424, 2.493, 1.267, 1.051, 1.308,
  1.481, 1.267, 2.462, 1.093, 1.360,
  1.328, 1.051, 1.093, 2.769, 1.128,
  1.529, 1.308, 1.360, 1.128, 3.355
),
ncol = 5, byrow = TRUE,
dimnames = list(varnames, varnames)
)
SWLS
```

Utilizzando la funzione `cfa()` contenuta nel pacchetto `lavaan`, il modello
ad un fattore viene definito nel modo seguente.

```{r}
mod_1 <- "
  F =~ Y1 + Y2 + Y3 + Y4 + Y5
"
```

Otteniamo così una stima dei pesi fattoriali e delle unicità.

```{r}
fit <- lavaan::cfa(
  mod_1,
  sample.cov = SWLS,
  sample.nobs = 215,
  std.lv = TRUE
)
```

Calcoliamo la quantità di informazione fornita da ciascun item. Iniziamo a estrarre dall'oggetto `fit` la matrice delle saturazioni fattoriali.

```{r}
lambda <- inspect(fit, what="std")$lambda
lambda
```

Estraiamo da `fit` le specificità.

```{r}
theta <- diag(inspect(fit, what="std")$theta)
theta
```

Possiamo ora calcolare quantità di informazione degli item facendo il rapporto tra ciascuna saturazione fattoriale innalzata al quadrato e la corrispondente specificità.

```{r}
for (i in 1:5) {
  print(lambda[i]^2 / theta[i])
}
```

Il risultato ottenuto indica che il quarto item è il meno informativo e che il
quinto item è il secondo meno informativo. Se un solo item deve essere
eliminato, dunque, elimineremo il quarto item. Se devono essere
eliminati due item, andranno eliminati il quarto e il quinto
item.
:::

### Attendibilità e numero di item 

Di quanto cambia l'attendibilità di uno strumento se viene variato il
numero di item? Una risposta a questa domanda può essere fornita dalla
formula profetica di Spearman-Brown. Supponiamo che nella formula di
Spearman-Brown, 

\begin{equation}
  \rho_p = \frac{p \rho_1}{(p-1)\rho_1 + 1},
  (\#eq:spearman-brown)
\end{equation}
 
$\rho_1$ rappresenti l'attendibilità di un test costituito da un certo numero di item. Se poniamo $p=2$, la \@ref(eq:spearman-brown) ci fornisce una stima dell'attendibilità che si otterrebbe raddoppiando il numero di item nel test. Valori di $p$ minori di $1$, invece, vengono usati per predire la diminuizione dell'attendibilità conseguente ad una diminuzione nel numero degli item del test.

Ricordiamo però che le predizioni della formula di Spearman-Brown sono
accurate solo se la forma allungata o accorciata del test è parallela rispetto al
test considerato. Per esempio, se ad un test con un coefficiente di
attendibilità molto alto vengono aggiunti item aventi una bassa
attendibilità, allora l'attendibilità del test allungato sarà minore di
quella predetta dalla formula di Spearman-Brown.

Anche se la formula di Spearman-Brown ha un ruolo centrale nella teoria
classica dei test, si tenga conto che non rappresenta l'unico strumento
che può essere utilizzato per valutare la relazione tra attendibilità e numero degli item del test. La quantità detta *informazione dell'item* (*item information*), formulata dai modelli IRT, consente di predire i cambiamenti nella qualità della misura a seguito dell'aggiunta o della cancellazione di un sottoinsieme di item.

::: {.example}
Si consideri la scala SWLS. Chiediamoci come varia l'attendibilità della
scala se il numero di item aumenta da 5 a 20. Poniamo che l'attendibilità della
scala SWLS costituita da 5 item sia uguale a 0.824. Applicando la formula di
Spearman-Brown otteniamo la stima seguente.

```{r}
(4 * 0.824) / ((4 - 1) * 0.824 + 1)
```
:::

::: {.example}
Possiamo giungere al risultato precedente in un altro modo. Supponiamo che i 15 item aggiuntivi abbiano le stesse saturazioni fattoriali medie ($\bar{\lambda}$) e le stesse varianze specifiche medie ($\bar{\psi}$) rispetto agli item originali.
Mediante gli item di cui disponiamo, stimiamo l'attendibilità di un
"item medio" nel modo seguente

$$
\rho_1 = \frac{\bar{\lambda}^2}{\bar{\lambda}^2 + \bar{\psi}},
$$

ovvero otteniamo la stima di 0.48:

```{r}
rho_1 <- mean(lambda)^2 / (mean(lambda)^2 + mean(theta)) 
rho_1
```

L'attendibilità predetta di un test costituito da 20 item sarà dunque
uguale a

```{r}
(20 * rho_1) / ((20 - 1) * rho_1 + 1) 
```

il che replica il risultato ottenuto precedentemente.
:::

::: {.example}
Un altro modo ancora per ottenere lo stesso risultato è quello di utilizzare 
un modello mono-fattoriale per item paralleli.

```{r}
mod_2 <- "
  F =~ a*Y1 + a*Y2 + a*Y3 + a*Y4 + a*Y5
  Y1 ~~ b*Y1
  Y2 ~~ b*Y2
  Y3 ~~ b*Y3
  Y4 ~~ b*Y4
  Y5 ~~ b*Y5
"
```

Adattiamo il modello ai dati.

```{r}
fit2 <- lavaan::cfa(
  mod_2,
  sample.cov = SWLS,
  sample.nobs = 215,
  std.lv = TRUE
)
```

Estraiamo dall'oggetto `fit2` le saturazioni fattoriali.

```{r}
lambda <- inspect(fit2, what="std")$lambda
lambda
```

Estraiamo da `fit2` le specificità.

```{r}
theta <- diag(inspect(fit2, what="std")$theta)
theta
```

Calcoliamo l'attendibilità dell'item "medio" usando $\lambda$ e $\psi$ (chiamato `theta` da `lavaan`).

```{r}
rho_1 <- lambda[1]^2 / (lambda[1]^2 + theta[2])
rho_1 
```

Posso ora applicare la formula di Spearman-Brown.

```{r}
(20 * rho_1) / ((20 - 1) * rho_1 + 1) 
```

Il risultato è praticamente identico a quelli trovati in precedenza.
:::

### Numero di item e affidabilità

La formula di Spearman-Brown può anche essere riarrangiata in maniera tale da consentirci di predire il numero degli item necessari per raggiungere un determinato livello di affidabilità: 

\begin{equation}
p = \frac{\rho_p (1-\rho_1)}{\rho_1(1-\rho_p)}, 
(\#eq:s-b-inv)
\end{equation}

dove $\rho_1$ è l'attendibilità stimata di un "item medio," $\rho_p$ è il livello desiderato di attendibilità del test allungato e $p$ è il numero di item del test allungato.

::: {.example}
L'attendibilità della scala SWLS costituita da 5 item è
$\omega = 0.824$. Quanti item devono essere aggiunti se si vuole
raggiungere un livello di attendibilità pari a $0.95$?

Ponendo $\rho_p = 0.95$ e $\rho_1= 0.479$, in base alla \@ref(eq:s-b-inv) si ottiene che

```{r}
(.95 * (1 - rho_1)) / (rho_1 * (1 - .95))
```

il test dovrà essere costituito da 21 item.
:::

## Analisi degli item

L'analisi degli item esamina le risposte fornite ai singoli item del
questionario allo scopo di valutare la qualità degli item e del
questionario nel suo complesso. Sotto al rubrica di analisi degli item
possiamo raggruppare le procedure che possono essere utilizzate per
descrivere la difficoltà degli item, le relazioni tra coppie di item, il
punteggio totale del test, le relazioni tra gli item e il punteggio
totale del test. Tali procedure vengono utilizzate per la selezione
degli item al fine di costruire un questionario omogeneo, attendibile e
dotato di validità predittiva.

L'esame delle caratteristiche psicometriche degli item che compongono il
test non può però essere automatizzata ed eseguita solo sulla base dei
risultati delle analisi statiche. L'analisi degli item, invece, va
integrata con considerazioni di ordine teorico basate sulla centralità
dell'item rispetto alla definizione del costrutto, agli scopi della
misurazione e al modo in cui l'item è stato formulato e costruito. Se
alcuni aspetti di un costrutto non vengono rappresentanti da item che
soddisfano i criteri descritti sopra, o se c'è un numero insufficiente
di item per produrre uno strumento attendibile, allora alcuni item
devono essere riscritti. Nello scrivere gli item, risultano utili le
intuizioni che si sono guadagnate nello scrivere gli item che non hanno
funzionato.

### Difficoltà degli item

La più ovvia caratteristica psicometrica di un item in un test di
prestazione massima è la proporzione di soggetti che risponde
correttamente all'item.La proporzione $p_j$ di soggetti che rispondono
correttamente all'item $j$-esimo, o si dichiarano in accordo con
l'affermazione espressa dall'item, fornisce una stima di $\pi_j$, ovvero
il *livello di difficoltà* dell'item. In realtà, $p_j$ dovrebbe essere
chiamato "facilità dell'item" in quanto assume il suo valore maggiore
(ovvero $1$) quando tutti i rispondenti rispondono correttamente
all'item considerato e il suo valore minimo (ovvero $0$) quando le
risposte sono tutte sbagliate. I valori $p_j$ giocano un ruolo
importante nelle procedure di selezione degli item.

La difficoltà degli item deve essere interpretata in riferimento alla
probabilità di indovinare la risposta corretta. Si suppone, infatti, che
i rispondenti tirino ad indovinare quando non conoscono la risposta alla
domanda di un questionario. Nel caso di item dicotomici, per esempio, ci
possiamo aspettare un valore $p_j$ pari a $0.50$ sulla base del caso
soltanto; nel caso di item a risposta multipla con quattro opzioni di
scelta, invece, $p_j$ assume un valore pari a $0.25$ quando i
rispondenti tirano ad indovinare.

Se il test è composto, per la maggior parte, da item "facili," allora il
test non sarà in grado di discriminare tra rispondenti con livelli di
abilità diversi, in quanto quasi tutti i rispondenti saranno in grado di
fornire una risposta corretta alla maggioranza degli item. Lo stesso si
può dire per un test composto da item "difficili." Se consideriamo un
test composto unicamente da item di difficoltà media, è facile rendersi
conto che non potrà differenziare i rispondenti di abilità media da
quelli con abilità superiori alla media (dato che non ci sono item
"difficili"), e neppure da quelli con abilità inferiori alla media (dato
che non ci sono item "facili").

In generale è buona pratica costruire test composti da item che coprano
tutti i livelli di difficoltà. La scelta che viene usualmente fatta è
quella di una dispersione moderata e simmetrica del livello di
difficoltà attorno ad un valore leggermente superiore al valore che sta
a metà tra il livello del caso ($1.0$ diviso per il numero di
alternative) e il punteggio pieno ($1.0$). Per item che presentano
quattro alternative di risposta, ad esempio, il livello del caso è pari
a $1.00/4 = 0.25$. Il livello ottimale di difficoltà media è dunque
uguale a 

$$0.25 + (1.00 - 0.25) / 2 = 0.62.$$ 
    
Per item dicotomici, il livello del caso è $1.00/2 = 0.50$ e il livello ottimale di difficoltà media è

$$0.50+(1.00-.50)/2 = 0.75.$$ 

In generale, item con livelli di difficoltà superiore a $0.90$ o inferiore a $0.20$ dovrebbero essere utilizzati con cautela.

### Correzione per guessing

Alle volte i valori $p_j$ sono calcolati introducendo una correzione per
le risposte fornite casualmente dai soggetti (*guessing*). Si consideri
un test a scelta multipla composto da item aventi ciascuno $C$
alternative di risposta ed una sola risposta corretta. Si supponga che
un rispondente risponda correttamente a $R$ item e risponda in maniera
sbagliata a $W$ item.

La correzione per guessing si ottiene applicando una formula basata sul
seguente ragionamento. Se assumiamo che un rispondente si limita a
tirare ad indovinare allora, ogni $C$ risposte, ci aspettiamo 1 risposta
giusta e $C-1$ risposte sbagliate. Per calcolare il punteggio totale del
test in modo da eliminare il numero di risposte corrette ottenute
tirando ad indovinare è necessario sottrarre 1 punto per ogni $C-1$ item
a cui è stata fornita una risposta corretta. Questo ragionamento conduce
alla seguente formula: 

\begin{equation}
FS = R - \frac{W}{C - 1},
(\#eq:guessing)
\end{equation}

con $R$ = \# risposte corrette, $W$ = \# risposte
sbagliate, $C$ = \# alternative di risposta. Per esempio, se $C=5$,
allora è necessario sottrarre un punto ogni 4, il che è proprio quello
che fa la \@ref(eq:guessing). La \@ref(eq:guessing) per la correzione per guessing produce un punteggio totale corretto per il guessing identico a quello che si
otterrebbe pesando ciascuna risposta con 1 punto per le risposte
corrette e $- \frac{1}{C-1}$ punti per le risposte sbagliate -- senza
considerare le risposte non date.

La correzione per guessing rappresenta il tentativo di scomporre il
numero totale di risposte corrette in due componenti: le risposte
corrette dovute alle conoscenze del soggetto, le risposte che risultano
corrette come effetto del caso. La stessa formula può anche essere
utilizzata per calcolare la difficoltà degli item corretta per guessing.

### Discriminatività

La discriminatività è una misura di quanto ogni item è in grado di
distinguere i soggetti con elevati livelli nel costrutto da quelli con
un livello basso. L'indice di discriminatività $D$ per i test di
prestazione massima si trova nel modo seguente. Dopo avere calcolato il
punteggio totale al test, si dividono i soggetti in due gruppi: soggetti
con basso punteggio e soggetti con alto punteggio. Una volta definiti i
due gruppi, l'indice di discriminatività $D$ sarà dato da:

$$D = P(\text{alto}) - P(\text{basso}),$$ 

dove $P(\text{alto}$ è la proporzione di soggetti che ha risposto correttamente all'item nel gruppo con punteggi alti e $P(\text{basso}$ è la proporzione di soggetti
che ha risposto correttamente all'item nel gruppo con punteggi bassi. Il
valore di $D$ può variare da -1 a +1. Nella tabella seguente sono fornite le linee guida per l'interpretazione di questo indice (Ebel, 1965).

::: {#tab:ebel_1965}
  Valore di $D$          Commento
  ---------------------- ----------------------------------------------
  $D \geq 0.40$          Ottima, nessuna revision
  $0.30 \leq D < 0.40$   Buona, revisioni minime
  $0.20 \leq D < 0.30$   Sufficiente, revisioni parziali
  $D < 0.20$             Insufficiente, riformulazione o eliminazione

  : Linee guida per l'interpretazione dell'indice di discriminatività
  $D$.
:::

[]{#tab:ebel_1965 label="tab:ebel_1965"}

La discriminatività degli item di tipo Likert viene valutata con la
medesima procedura degli item dei testi di prestazione massima, anche se
cambiano le procedure statistiche da utilizzare. Si può dividere la
distribuzione dei punteggi totali (o punteggi medi) in quartili e
confrontare il punteggio medio o mediano del quartile superiore con
quello del quartile inferiore, oppure, se il test è orientato al
criterio e lo scopo è selezionare gli item che discriminano meglio due
gruppi precostituiti di soggetto, eseguire i medesimi confronti tra il
gruppo target (ad esempio, pazienti) e quello "di controllo" (per
esempio, popolazione generale). È consigliabile valutare la dimensione
dell'effetto, ad esempio attraverso l'indice $d$ di Cohen. La dimensione
dell'effetto dovrebbe essere almeno moderata ($d > |0.50|$).

### Potere discriminante dell'item e analisi fattoriale

Secondo McDondald (1999), la nozione di potere discriminante dell'item
può essere trattata in maniera più precisa nell'ambito del modello
monofattoriale. Se l'insieme di item a disposizione non è eccessivamente
grande (200 o meno), infatti, è possibile procedere alla selezione degli
item migliori tramite l'analisi fattoriale -- ovvero, scegliendo gli
item con le saturazioni maggiori.

<!-- Se gli item sono ben rappresentati dal modello monofattoriale -->
<!-- ($Y_i = \mu_i + \lambda_i \xi + \delta_i$, con varianza specifica -->
<!-- $\psi_{ii}$), allora al crescere del numero di item la covarianza -->
<!-- semistandardizzata e la correlazione tra il punteggio sull'item e il -->
<!-- punteggio totale convergono alle saturazioni fattoriali calcolate sugli -->
<!-- item standardizzati. McDondald (1999) nota quindi che la covarianza -->
<!-- semistandardizzata e la correlazione risultano appropriate per stimare -->
<!-- il potere discriminante dell'item se il punteggio totale viene calcolato -->
<!-- utilizzando i punteggi standardizzati degli item. Tuttavia, la pratica -->
<!-- usuale questo non succede. È stato osservato, infine, che gli indici -->
<!-- discussi sopra sono spuri in quanto il punteggio totale è calcolato -->
<!-- anche nei termini dell'item che viene analizzato. A questo scopo, è -->
<!-- possibile procedere ad un aggiustamento che introduce un fattore di -->
<!-- correzione. -->

### Punteggio sull'item e punteggio totale

Il grado di associazione tra il punteggio sull'item e il punteggio
totale viene considerato dalla teoria classica dei test quale indice del
potere discriminante dell'item. Se il test fornisce una misura
attendibile di un unico attributo, e se un item è fortemente associato
al punteggio del test, allora l'item medesimo sarà in grado di
distinguere tra rispondenti che ottengono un punteggio basso nel test e
rispondenti che ottengono un punteggio alto nel test.

Nel caso di una forte associazione positiva tra il punteggio sull'item e
il punteggio totale, la probabilità di risposta corretta sull'item è
alta per rispondenti che ottengono un punteggio totale alto, e bassa per
i rispondenti che ottengono un punteggio totale basso. Nel caso di una
debole associazione tra il punteggio sull'item e il punteggio totale,
invece, la probabilità di risposta corretta all'item non è predittiva
del punteggio totale. Gli item con un basso potere discriminante
dovrebbero dunque essere rimossi dal reattivo.

È necessario distinguere i casi in cui gli item sono dicotomici dal caso
di item continui. Nel caso di item dicotomici e di un test
unidimensionale, il potere discriminante viene calcolato mediante la
correlazione biseriale o punto-biseriale.

### Relazioni tra coppie di item

Le relazioni tra coppie di item sono importanti sia per la costruzione
sia per l'analisi dei test. La teoria classica dei test definisce
l'attendibilità di un test (o di un item) come il rapporto tra la
varianza del punteggio vero e la varianza del punteggio osservato. Il
coefficiente di attendibilità può però essere calcolato anche trovando
la correlazione tra due forme parallele di un test (o tra due item).
Inoltre, è possibile interpretare la correlazione tra due forme
parallele di un test (o tra due item) come il quadrato del coefficiente
di correlazione tra i punteggi osservati e i punteggi veri di un test (o
di un item).

Molti indici sono disponibili per misurare il grado di associazione tra
gli item. Per item quantitativi, possiamo usare la correlazione di
Pearson o la covarianza. Per item qualitativi politomici ordinali,
usiamo la correlazione policorica. Per item ordinali dicotomici, usiamo
la correlazione tetracorica. Per item dicotomici usiamo, ad esempio,
l'indice $\phi$.

### Ridondanza

Nel processo di raffinamento del test occorre tenere conto degli item
ridondanti, ossia di quelli che sono troppo associati tra loro. La
ridondanza può essere valutata con indici statistici quali la
correlazione: se due o più item hanno tra loro una correlazione maggiore
di $|0.70|$ viene mantenuto nell'item pool solo un item, dato che gli
altri apportano la stessa informazione.

### Massimizzazione della varianza del punteggio totale

Uno dei criteri che possono essere utilizzati per selezionare gli item
che andranno a costituire la versione finale di un test è quello di
massimizzare la varianza del punteggio totale. Più in particolare, si
vuole massimizzare il rapporto tra la varianza del punteggio totale e la
somma delle varianze dei punteggi dei $p$ item. Dato che il coefficiente
$\alpha$ di Cronbach ha la seguente forma:

$$\alpha = \frac{p}{p-1}\left[1- \frac{\sum \sigma^2_{X_i}}{\sigma^2_T} \right],$$

la scelta di massimizzare il rapporto definito in precedenza avrà anche
la conseguenza di massimizzare $\alpha$.

@mcdonald2013test fa notare che una procedura di selezione degli item
basata sul principio della massimizzazione di $\alpha$ ha però dei
limiti. In primo luogo, tale procedura è appropriata solo quando
l'insieme di item è troppo grande per selezionare gli item in base
all'esame dell saturazioni fattoriali ottenute applicando il modello
mono-fattoriale. Inoltre, anche se il modello mono-fattoriale non può
essere applicato per problemi pratici, a causa del numero di item troppo
grande, tale modello deve essere comunque adeguato anche se si procede
selezionando gli item sulla base di $\alpha$. Ovviamente, però, tale
assunzione non può essere verificata. In secondo luogo, @mcdonald2013test
nota che la procedura di selezione basata sulla massimizzazione di
$\alpha$ non è adeguata neanche nel caso in cui il costrutto latente
abbia una struttura gerarchica, con un fattore generale ed una serie di
fattori subordinati. In quel caso, infatti, la selezione degli item
basata sulla massimizzazione di $\alpha$ può portare all'eliminazione
degli item che definiscono tutti i fattori subordinati tranne uno. Nel
caso in cui il costrutto abbia una struttura gerarchica, dunque, la
selezione degli item basata sulla massimizzazione di $\alpha$ deve
essere accompagnata da considerazione relative al contenuto del
costrutto.



