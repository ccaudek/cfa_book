# Un'applicazione concreta

```{r, include = FALSE}
source("_common.R")
library("lavaan")
library("semPlot")
library("knitr")
library("markdown")
library("patchwork")
library("DT")
library("kableExtra")
library("lme4")
set.seed(12345)
```

Consideriamo un altro esempio che ci consentirà di approfondire la nostra conoscenza dei modelli LGM. L'esempio che discuteremo utilizza un campione di dati artificiali chiamato `Demo.growth` in cui si ipotizza che un determinato punteggio venga misurato su quattro punti temporali. I dati sono i seguenti:

```{r}
data(Demo.growth)
glimpse(Demo.growth)
```

La variabile dipendente è rappresentata dalle quattro colonne chiamate `t1`, `t2`, `t3` e `t4` che corrispondono alla serie temporale con quattro misurazioni per ciascun soggetto.

Trasformiamo i dati in formato `long`:

```{r}
demo_growth_long <- Demo.growth %>%
  dplyr::select(t1, t2, t3, t4) %>%
  pivot_longer(
    cols = starts_with("t"),
    names_to = "t",
    values_to = "y"
  ) %>% 
  as.data.frame()
demo_growth_long$time <- rep(0:3, 400)
demo_growth_long$id <- rep(1:400, each = 4)
```

Esaminiamo un campione casuale di 9 soggetti. È presente una notevole variazione da soggetto a soggetto:

```{r}
id_sel <- sample(1:400, 9)
d <- demo_growth_long[demo_growth_long$id %in% id_sel, ]
d %>% 
  ggplot(aes(x = time, y = y)) + 
  geom_line() +
  facet_wrap(~id)
```
Notiamo che una funzione lineare è appropriata per rendere conto della variazione temporale della variabile risposta:

```{r}
d %>% 
  ggplot(aes(x = time, y = y)) + 
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) + 
  facet_wrap(~id)
```

Complessivamente, i dati suggeriscono un andamento crescente della variabile risposta in funzione del tempo:

```{r}
demo_growth_long %>% 
  ggplot(aes(time, y, group = id)) + 
  geom_line(alpha = 0.1) + # add individual line with transparency
  stat_summary( # add average line
    aes(group = 1),
    fun = mean,
    geom = "line",
    size = 1.5,
    color = "black"
  ) +
  labs(x = "Time", y = "y") 
```


<!-- Per adattare a questi dati un modello lineare di crescita, specifichiamo il seguente modello a variabili latenti -->

<!-- $$ -->
<!-- y_j = \alpha_0 + \alpha_1 \lambda_j + \zeta_{00} + \zeta_{11} \lambda_j + \epsilon_j, -->
<!-- $$ -->
<!-- dove  -->

<!-- - $y_j$ è la variabile di interesse che cambia nel tempo, con $j = 0, \dots, 3$. -->
<!-- - $\alpha_0$ rappresenta l'intercetta della retta di regressione al tempo $t = 0$ (il punto di partenza della linea nera sopra). -->
<!-- - $\alpha_1 \lambda_j$ è il tasso medio di crescita nel tempo (la pendenza della linea nera nel grafico sopra). Qui $\lambda_j$ è solo l'indice dei punti temporali considerati (0, 1, 2, 3). -->
<!-- - $\zeta_{00}$ è la varianza tra i soggetti nel punto $t = 0$. -->
<!-- - $\zeta_{11} \lambda_j$ è la varianza del tasso di crescita tra i soggetti. -->
<!-- - $\epsilon_j$ è la varianza di ciascun soggetto attorno alla sua retta di regressione. -->

<!-- Tali relazioni statistiche vengono rappresentate dal modello di equazioni strutturali della figura \@ref(fig:growth01).  -->

<!-- ```{r growth01, echo=FALSE, fig.cap="Modello di crescita latente.", out.width = '90%'} -->
<!-- knitr::include_graphics("images/lgm.png") -->
<!-- ``` -->

Abbiamo visto come un modello lineare di crescita latente corrisponde ad un modello fattoriale con due variabili latenti: un fattore ($\eta_0$) che specifica il "punteggio vero" delle intercette individuali e un fattore ($\eta_1$) che speficita il "punteggio vero" delle pendenze delle rette di regressione per i singoli individui. Nella sintassi `lavaan` il modello LGM diventa:

```{r}
model <- "
 i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
 s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
"
```

Si noti che, per il fattore $\eta_0$ (che rappresenta le intercette), i valori delle saturazioni fattoriali sono fissate a 1 -- questo è il motivo per cui $\alpha_0$ e $\zeta_{00}$ compaiono da soli nell'equazione precedente: in maniera esplicita sono $1 \cdot \alpha_0$ e $1 \cdot \zeta_{00}$. 

Le saturazioni per il fattore $\eta_1$ (che specifica le pendenze delle funzioni lineari) sono fissate ai valori che descrivono la variazione temporale: qui i valori $\lambda_j$ da 0 a 3.

Il modello include anche la correlazione tra $\eta_0$ e $\eta_1$, rappresentata dalla doppia freccia $\zeta_{01}$. Se $\zeta_{01} > 0$, questo significa che, con il passare del tempo, i partecipanti tendono a diventare sempre più diversi tra loro; un'interpretazione opposta si ha se $\zeta_{01}< 0$.

Adattiamo il modello ai dati:

```{r}
fit <- growth(model, data = Demo.growth)
summary(fit)
```

Esaminiamo il path diagram.

```{r}
semPaths(
  fit,
  layout = "tree",
  intercepts = FALSE,
  posCol = c("black"),
  edge.label.cex = 0.00001,
  sizeMan = 7,
  what = "path",
  optimizeLatRes = TRUE,
  residuals = TRUE,
  style = "lisrel"
)
```

Ci sono 6 tipi di parametri di interesse:

```{r}
kable(coef(fit), booktabs =TRUE, format = "markdown") 
```

- l'intercetta $i$ = 0.615 è il valore atteso della variabile risposta al momento $t_0$;
- la pendenza $s$ = 1.006 è il tasso di cambiamento medio della variabile risposta nel tempo. Ad ogni successivo momento temporale, il valore medio della variabile risposta aumenta in media di 1.006 punti;
- varianza $i$ = 1.932 misura la variazione tra i soggetti al momento $t_0$ (ci dice quanto sono diverse le intercette delle rette di regressione tra i soggetti);
- varianza $s$ = 0.587 misura la variazione del tasso di crescita tra i soggetti (ci dice quanto sono diverse le pendenze delle rette di regressione tra i soggetti);
- varianze `t1`, ..., `t4`: i valori da 0.595 a 0.508 descrivono la variazione tra i soggetti in ciascun momento del tempo;
- la covarianza tra `i` e `s` = 0.618 ci dice che i valori della variabile risposta diventano via via più diversi nel tempo tra i rispondenti (un valore negativo avrebbe l'interpretazione opposta).

Le stime dell'intercetta e della pendenza della funzione di crescita per ciascun partecipante si ottengono nel modo seguente:

```{r}
rand_eff <- as.data.frame(lavPredict(fit))
head(rand_eff)
```

Istogrammi delle stime individuali dell'intercetta e della pendenza della curva di crescita si ottengono nel modo seguente:

```{r}
gi <- rand_eff %>% 
  ggplot(aes(x=i)) + 
  geom_histogram()
gh <- rand_eff %>% 
  ggplot(aes(x=s)) + 
  geom_histogram()
gi + gh
```

### Visualizzare il cambiamento

È utile visualizzare i punteggi previsti dal modello. A tal fine, useremo qui la funzione `predict()` per creare un nuovo oggetto $\mathsf{R}$ che contiene i punteggi previsti a livello individuale per l'intercetta e la pendenza.

```{r}
pred_lgm <- predict(fit) 
head(pred_lgm)
```

Questi sono i valori previsti dal modello per ciascun partecipante. Se calcoliamo la media di queste variabili otteniamo gli stessi risultati che sono stati riportati sopra:

```{r}
# average of the intercepts (first column)
mean(pred_lgm[, 1]) 
# average of the slope (second column)
mean(pred_lgm[, 2])
```

Il cambiamento nel tempo previsto dal modello per il soggetto $j$-esimo è

$$
y_j = \eta_0 + \lambda_j \eta_1.
$$

Il cambiamento previsto per tutti i soggetti può essere visualizzato nel modo seguente:

```{r}
# create long data for each individual
pred_lgm_long <- map(
  0:3, # loop over time
  function(x) pred_lgm[, 1] + x * pred_lgm[, 2]
) %>%
  reduce(cbind) %>% # bring together the wave predictions
  as.data.frame() %>% # make data frame
  setNames(str_c("time", 0:3)) %>% # give names to variables
  mutate(id = row_number()) %>% # make unique id
  gather(-id, key = time, value = pred) # make long format
# make graph
pred_lgm_long %>%
  ggplot(aes(time, pred, group = id)) + # what variables to plot?
  geom_line(alpha = 0.1) + # add a transparent line for each person
  stat_summary( # add average line
    aes(group = 1),
    fun = mean,
    geom = "line",
    size = 1.5,
    color = "black"
  ) +
  labs(y = "y", x = "time")
```

La linea nera più spessa rappresenta l'intercetta media e la pendenza media della curva di crescita del modello LGM. Ogni individuo ha una sua specifica intercetta e uno specifico tasso di cambiamento e questa diversità è catturata nelle componenti di varianza del modello.

### Dati ordinali vs. intervalli temporali

La specificazione `s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4` assume che il tempo sia misurato per intervalli costanti. Questa sintassi può essere usata anche quando il tempo è misurato su una scala a livello ordinale, ovvero quando supponiamo diverse rilevazioni temporali e ci chiediamo cosa succede passando da una alla successiva, senza specificare precisamente qual è la distanza temporale tra le varie rilevazioni. In alternativa è possibile specificare in termini assoluti il tempo trascorso tra le diverse rilevazioni temporali -- questo è possibile solo se tali distanze temporali sono costanti tra i vari soggetti. Ad esempio, se passano 2, 3 e 9 mesi dalla prima rilevazione, avremo il seguente modello.

```{r}
model_a <- "
 i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
 s =~ 0*t1 + 2*t2 + 6*t3 + 9*t4
"
```

La scelta di utilizzare il tempo su scala ordinale rispetto a quello assoluto non cambia il numero di parametri del modello o i gradi di libertà. Tuttavia la media dei punteggi fattoriali che rappresentano l'intercetta e la pendenza può cambiare e l'interpretazione di questi termini cambierà di conseguenza.

```{r}
fit_a <- growth(model_a, data = Demo.growth)
kable(coef(fit_a), booktabs = TRUE, format = "markdown")
```

Mentre in precedenza l'interpretazione era che la media della variabile risposta aumenta in media 1.006 punti passando da una rilevazione temporale alla successiva, nel caso della presente specificazione temporale in mesi possiamo dire che media della variabile risposta aumenta in media 0.319 punti dopo l'incremento temporale di un mese.

### Verifica di ipotesi

È anche possibile utilizzare le funzionalità di `lavaan` per verificare specifiche ipotesi di interesse relative ai dati longitudinali.  Ad esempio, una possibile domanda riguarda l'uguaglianza degli errori nel tempo. Tale domanda può essere affrontata introducendo dei vincoli nella specificazione del modello LGM (come abbiamo anche visto in precedenza) e, successivamente, confrontando la bontà dell'adattamento del modello vincolato e del modello generale.

Il modello vincolato è

```{r}
model_eqerr <- "
 i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
 s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
 t1 ~~ a*t1
 t2 ~~ a*t2
 t3 ~~ a*t3
 t4 ~~ a*t4
"
```

Adattiamo il modello vincolato:

```{r}
fit_eqerr <- growth(model_eqerr, data = Demo.growth)
```

Confrontiamo il modello vincolato con il modello libero:

```{r}
anova(fit, fit_eqerr)
```

Il test del rapporto di verosimiglianza (*likelihood ratio*) eseguito dalla funzione `anova()` produce un $p$-valore di 0.6573. Ciò significa che, per i dati esaminati, non emerge una decremento nella bontà dell'adattamento degna di nota se passiamo dal modello libero al modello vincolato. Per questi dati, dunque, sembra ragionevole assumere l'equaglianza della varianza degli errori nel tempo.

## L'effetto delle covariate

Un modello leggermente più complesso aggiunge due regressori (`x1` e `x2`) che influenzano i fattori di crescita latenti. Inoltre, è stata aggiunta al modello una covariata `c` variabile nel tempo che influenza la misura del risultato (la variabile dipendente) nei quattro punti temporali.

```{r}
model2 <- '
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
  # regressions
    i ~ x1 + x2
    s ~ x1 + x2
  # time-varying covariates
    t1 ~ c1
    t2 ~ c2
    t3 ~ c3
    t4 ~ c4
'
```

```{r}
fit2 <- growth(model2, data = Demo.growth)
summary(fit2)
```

```{r}
kable(coef(fit2), booktabs =TRUE, format = "markdown")
```

I risultati mostrano che le due covariate $x$ influenzano sia l'intercetta sia la pendenza della curva di crescita. Inoltre, vi sono evidenze di un effetto della covariata `c`.

## Crescita non lineare

Ripetiamo la procedura di analisi descritta sopra introducendo un cambiamento relativo alla descrizione del cambiamento: verrà considerato un modello nel quale la crescita non è lineare.

Nelle analisi seguenti useremo i seguenti indici di bontà di adattamento.

```{r}
selected_fit_stats <-   
  c("chisq.scaled",
    "df.scaled", ## must be >0 to test G.O.F.
    "pvalue.scaled", ## ideally n.s.
    "cfi.scaled", ## ideally ≥ 0.95
    "rmsea.scaled", ## ideally ≤ 0.05
    "rmsea.pvalue.scaled", ## ideally n.s.
    "srmr" ## ideally < 0.08
    )
```

Supponiamo che la popolazione possa essere descritta dal seguente modello.

```{r}
growth_mod <- 
'
  ## intercept & slope growth terms for X
  iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
  sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4
  
  ## intercept, slope, & quadratic terms for Y
  iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
  sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
  qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4
  
  ## set variances 
  
  y4 ~~ 2*y4
  x4 ~~ 1*x4
  
  ## set latent means/intercepts
  iX ~ 2*1
  sX ~ 1*1
  sY ~ -1*1
  qY ~ -1.5*1
  
  sY ~ 2*predictor
  
  outcome ~ 2*iX + 3*sY 
'
```

È possibile usare la funzione `simulateData()` di `lavaan` per simulare un campione di dati estratto da una popolazione definita come abbiamo fatto sopra. La simulazione include due variabili misurate in quattro punti temporali. 

```{r}
sim_growth_dat <- lavaan::simulateData(
  model = growth_mod, 
  model.type = "growth", 
  seed = 82020, 
  orthogonal = FALSE,
  auto.cov.y = TRUE, 
  auto.var = TRUE
  )
```

Le variabili sono chiamate `x1`, `x2`, `x3`, `x4`, per la misurazione di `x` ai tempi 1, 2, 3, 4. Lo stesso per la `y`.

```{r}
head(sim_growth_dat)
```

Aggiungo qui un codice identificativo per ciascun partecipante.

```{r}
sim_growth_dat$participant_n <- 
  1:nrow(sim_growth_dat) 
```

Nei dati simulati, la variabile `x` cambia linearmente nel tempo e la variabile `y`  cambia seguendo un andamento quadratico. Esaminiamo i dati con un grafico.

```{r}
x_plot <-
  pivot_longer(sim_growth_dat,
    cols = x1:x4,
    names_to = "x",
    names_prefix = "x"
  )

individual_x_trajectories <-
  ggplot(
    x_plot,
    aes(
      x = as.numeric(x),
      y = value,
      group = participant_n,
      color = participant_n
    )
  ) +
  geom_line(alpha = 0.2) +
  labs(
    title = "Observed Trajectories of x",
    x = "Timepoint",
    y = "x"
  ) +
  xlim(1, 4) +
  theme(legend.position = "none")

individual_x_trajectories
```

Su può vedere che la variabile `x` segue una crescita lineare, ma un modello quadratico molto "debole" potrebbe adattarsi meglio ai dati, quindi sarà necessario controllare se in effetti questo è vero. 

Esaminiamo ora la variabile `y`.

```{r}
y_plot <-
  pivot_longer(sim_growth_dat,
    cols = y1:y4,
    names_to = "y",
    names_prefix = "y"
  )

individual_y_trajectories <-
  ggplot(
    y_plot,
    aes(
      x = as.numeric(y),
      y = value,
      group = participant_n,
      color = participant_n
    )
  ) +
  geom_line(alpha = 0.2) +
  labs(
    title = "Observed trajectories of y",
    x = "Timepoint",
    y = "y"
  ) +
  xlim(1, 4) +
  theme(legend.position = "none")

individual_y_trajectories
```

È chiaro che la `y` diminuisce in media con il tempo, ma c'è anche molta variabilità in ciò che accade nei singoli casi. Per questi dati, sia un modello lineare sia un modello quadratico sembrano appropriati per descrivere il cambiamento nei dati, anche se un modello quadratico sembra più appropriato.

## Stimatore

Tutti i dati utilizzati sono continui, quindi si potrebbe usare la stima della massima verosimiglianza (stimatore = ML). Tuttavia, quando possibile, è preferibile usare la variante "robusta" di questo stimatore allo scopo di rendere conto della possibile non normalità dei dati (stimatore = MLR).

## Assenza di crescita

Iniziamo con un modello per la `x` che assume che non vi sia variazione in funzione del tempo.

```{r}
int_x_mod <- 
'
  iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
'
```

Adattiamo il modello ai dati ed esaminiamo i risultati.

```{r}
int_x_fit <-
  growth(
    model = int_x_mod,
    estimator = "MLR",
    data = sim_growth_dat
  )

int_x_fit_stats <-
  fitmeasures(
    int_x_fit,
    selected_fit_stats
  ) %>%
  data.frame()

round(int_x_fit_stats, 2)
```

È chiaro che il modello di assenza di crescita non spiega i dati `x`. Consideriamo la variabile `y`. Anche in questo caso prendiamo in considerazione un modello di assenza di variazione nel tempo. 

```{r}
int_y_mod <- 
'
  iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
'

int_y_fit <-
  growth(
    model = int_y_mod,
    estimator = "MLR",
    data = sim_growth_dat
  )

int_y_fit_stats <-
  fitmeasures(
    int_y_fit,
    selected_fit_stats
  ) %>%
  data.frame()

round(int_y_fit_stats, 2)
```

Il modello di assenza di crescita non è adeguato neppure per la variabile `y`.

### Crescita lineare

Esaminiamo ora un modello di crescita lineare per la `x`.

```{r}
linear_x_mod <- 
'
iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4
'

linear_x_fit <-
  growth(
    model = linear_x_mod,
    estimator = "MLR",
    data = sim_growth_dat
  )

linear_x_fit_stats <-
  fitmeasures(
    linear_x_fit,
    selected_fit_stats
  ) %>%
  data.frame()

round(linear_x_fit_stats, 2)
```

Il modello di crescita lineare per la `x` fornisce un buon adattamento ai dati rispetto a tutti gli indici. Questo è ciò che ci aspettavamo esaminando il grafico dei dati.

Esaminiamo un modello di crescita lineare per la `y`.

```{r}
linear_y_mod <- 
'
  iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
  sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
'

linear_y_fit <-
  growth(
    model = linear_y_mod,
    estimator = "MLR",
    data = sim_growth_dat
  )

linear_y_fit_stats <-
  fitmeasures(
    linear_y_fit,
    selected_fit_stats
  ) %>%
  data.frame()

round(linear_y_fit_stats, 2)
```

Il modello lineare è inadeguato per la `y` rispetto a tutti gli indici considerati.

## Crescita quadratica

I termini quadratici descrivono il tasso medio di *variazione della pendenza* tra le diverse rilevazioni temporali. Nei modelli SEM, una tale caratteristica viene detta "crescita quadratica". Consideriamo dunque un modello di crescita quadratica per la `x`.

```{r}
quad_x_mod <- 
'
  iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
  sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4
  qX =~ 0*x1 + 1*x2 + 4*x3 + 9*x4
'

quad_x_fit <-
  growth(
    model = quad_x_mod,
    estimator = "MLR",
    data = sim_growth_dat
  )

quad_x_fit_stats <-
  fitmeasures(
    quad_x_fit,
    selected_fit_stats
  )

round(quad_x_fit_stats, 2)
```

Qui abbiamo un problema interessante, poiché la `x` si adatta bene sia a un modello lineare sia a uno quadratico. Abbiamo però un caso di Heywood: la varianza stimata della variabile `x4` è negativa.

```{r}
lavInspect(quad_x_fit, "est")$theta
```

Questo è ovviamente un problema, in quanto il modello non sembra adeguato per i dati. È possibile introdurre dei vincoli sui parametri per poi valutare se, evitando il caso di Heywood, l'adattamento si mantiene a livelli adeguati. Tuttavia, per i fini di questo tutorial, ignoreremo il caso di Heywood e proseguiremo nell'analisi.

Quando si hanno due modelli nidificati (nel nostro caso, modelli lineari e quadratici), possiamo confrontare formalmente l'adattamento relativo dei due modelli con il test del rapporto di verosimiglianza (LRT). L'ipotesi nulla è che non vi sia alcuna differenza nella covarianza spiegata dai due modelli. Se la covarianza spiegata è equivalente, preferiamo il modello più semplice.

```{r}
lavTestLRT(linear_x_fit, quad_x_fit)
```

Qui, un p-value $> 0.05$ (ovvero, nessuna evidenza di perdita di adattamento) ci dice che il modello lineare (ovvero, quello più semplice) è da preferire.

Esaminiamo la `y`. 

```{r}
quad_y_mod <-
"
  iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
  sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
  qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4
"

quad_y_fit <-
  growth(
    model = quad_y_mod,
    estimator = "MLR",
    data = sim_growth_dat
  )

quad_y_fit_stats <-
  fitmeasures(
    quad_y_fit,
    selected_fit_stats
  )

quad_y_fit_stats
```

Gli indici di bontà di adattamento sono eccellenti. 

```{r}
lavTestLRT(linear_y_fit, quad_y_fit)
```

Il test del rapporto di verosimiglianza ci dice che, per la `y`, il modello quadratico è sicuramente da preferire al modello lineare.

## Parallel Process Model

Esaminiamo ora un modello che include sia i termini lineari che quadratici (quando richiesti) delle variabili `x` e `y`.

```{r}
full_model <- 
'
  # intercept & slope growth terms for X
  iX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4
  sX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4
  
  # intercept, slope, & quadratic terms for Y
  iY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4
  sY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4
  qY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4
  
  # regress growth terms on predictor
  qY + iY + sX + iX ~ predictor
  sY ~ a1*predictor
  
  # regress outcome on growth terms
  outcome ~ iX + sX + iY + b1*sY + qY
  
  # testing indirect effect 
  # predictor --> sY --> outcome
  predictor_sY_outcome := a1*b1
'
```

Adattiamo il modello ai dati.

```{r}
full_fit <-
  growth(
    model = full_model,
    estimator = "MLR",
    data = sim_growth_dat
  )
```

Esaminiamo il path diagram.

```{r}
semPaths(
  full_fit,
  layout = "tree",
  intercepts = FALSE,
  posCol = c("black"),
  edge.label.cex = 0.00001,
  sizeMan = 7,
  what = "path",
  optimizeLatRes = TRUE,
  residuals = TRUE,
  style = "lisrel"
)
```

Valutiamo l'adattamento.

```{r}
full_fit_stats <-
  fitmeasures(
    full_fit,
    selected_fit_stats
  )

round(full_fit_stats, 2)
```

Si ottiene un ottimo adattamento del modello ai dati (il che non è sorprendente, in quanto i dati sono stati simulati in base a tale modello). 

## Interpretazione

Dato che il modello si adatta bene ai dati, interpretiamo i risultati ottenuti.

```{r}
summary(full_fit)
```

Tra i coefficienti di regressione, quando viene considerata la variabile di esito (`outcome`), vi sono solo due risultati con $p < .05$: quelli relativi a `iX` e `sY`. Ciò significa che il modello offre evidenze di una associazione tra `iX` e `sY` e la variabile di esito. La grandezza dei coefficienti suggerisce che `sY` ha un impatto maggiore su `outcome` rispetto a `iX`. In altre parole, sia i livelli iniziali della `x`, sia il tasso lineare di cambiamento della `y` predicono `outcome`. L'effetto è positivo per entrambe le variabili: livelli maggiori dei valori `x` nella prima rilevazione temporale e tassi di cambiamento più rapidi nella `y` portano entrambi un aumento nel valore atteso di `outcome`.

L'esempio discusso include anche la covariata `predictor`, la quale è invariante nel tempo. Poiché vi sono evidenze di un'associazione tra `predictor` e `sY`, e poiché vi sono evidenze di un'associazione tra `sY` e `outcome`, è ragionevole esaminare con più attenzione un tale effetto di mediazione. 

Per testare gli effetti indiretti è consigliato usare la tecnica di bootstrapping, in quanto gli intervalli di confidenza ottenuti con questa tecnica risultano più affidabili di quelli ottenibili mediante tecniche parametriche basate su ipotesi distributive stringenti relative alle distribuzioni dei parametri.

Il bootstrapping è una tecnica statistica di ricampionamento con reimmissione spesso usata per la costruzione di intervalli di confidenza delle stime degli effetti indiretti. Il bootstrapping produce intervalli di confidenza più affidabili rispetto a quelli che possono essere ottenuti usando metodi parametrici. Per velocizzare la simulazione, esaminiamo qui solo 500 bootstrap samples (in generale, è meglio usare un numero di simulazioni molto maggiore).

```{r}
final_fit_boot <-
  growth(full_fit,
    data = sim_growth_dat,
    estimator = "ML",
    meanstructure = T,
    se = "bootstrap",
    bootstrap = 500, # ~5000 better
    parallel = "multicore"
  )

parameterEstimates(
  final_fit_boot,
  level = .95,
  boot.ci.type = "bca.simple",
  stand = TRUE
)[61, c(4, 5, 9, 10)]
```

Possiamo dunque concludere che il livello della `y` nella prima rilevazione temporale funge da mediazione tra `predictor` e `outcome`, con un effetto non standardizzato pari a 6.01, 95% bootstrap CI [5.71, 6.34].

## Conclusioni

In questo tutorial, dunque, abbiamo visto come sia possibile valutare se un predittore influenza la variazione temporale di un costrutto. Abbiamo anche verificato la presenza di un effetto indiretto e abbiamo quantificato l'entità di tale effetto.




