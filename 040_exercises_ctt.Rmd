# Esercizi sulla CTT

Le applicazioni della CTT discusse in questo capitolo sono presentate da @desjardins2018handbook. Tuttavia, i fondamenti teorici delle applicazioni qui considerate saranno spiegati nei capitoli successivi. In questo capitolo, ci limiteremo ad utilizzare le funzioni e i dati contenuti nel pacchetto `hemp`. 

```{r, include = FALSE}
source("_common.R")
library("hemp")
```

## Attendibilità

Nell'esempio seguente, utilizziamo il set di dati SAPA per dimostrare come calcolare varie stime di coerenza interna in R. Ricordiamo che il data.frame SAPA (*Synthetic Aperture Personality Assessment*) è costituito da 1525 risposte a 16 item per la valutazione della personalità. Gli item misurano il ragionamento di base, la manipolazione di serie alfanumeriche, il ragionamento con matrici e la capacità di rotazione mentale.

Carichiamo i dati.

```{r}
library("hemp")
data(SAPA)
```

Esaminiamo i dati mancanti.

```{r}
num_miss(SAPA)
```

Una semplice misura di coerenza interna è la affidabilità *split-half*. Una stima dell'affidabilità *split-half* può essere ottenuta dividendo un test in due parti equivalenti, calcolando i punteggi totali delle due parti e correlandoli. Ci sono molti modi per creare la suddivisione (ad esempio, selezionando gli item pari e quelli dispari, oppure in modo casuale, ecc.). Di seguito calcoliamo l'affidabilità *split-half* selezionando gli item pari e quelli dispari.

```{r}
split_half(SAPA, type = "alternate")
```

In questo secondo caso, l'affidabilità *split-half* è calcolata selezionando due sottoinsiemi casuali di item della stessa numerosità.

```{r}
set.seed(1)
split_half(SAPA, type = "random")
```

È noto che la stima dell'affidabilità split-half è distorta verso il basso (R. J. Cohen, Swerdlik e Sturman, 2013). La correzione Spearman-Brown può essere applicata per superare questo problema. Per fare ciò, basta passare l'argomento `sb = TRUE` alla funzione `split_half()`.

```{r}
split_half(SAPA, type = "alternate", sb = TRUE)
```

Dopo aver applicato la correzione di Spearman-Brown, l'affidabilità split-half è ora stimata a 0.862, che è un po' più alta. 

Data la nostra attuale stima di affidabilità, possiamo anche determinare la lunghezza di un test per ottenere l'affidabilità desiderata. Possiamo farlo usando la funzione `test_length()` in `hemp`. Supponendo di volere un'affidabilità di 0.95, possiamo determinare di quanti item dovrebbe essere costituito il test. Nella chiamata a `test_length()`, specifichiamo `r_type = "split"` in modo che l'affidabilità attuale venga calcolata utilizzando l'affidabilità split-half con la correzione di Spearman-Brown. 

```{r}
test_length(SAPA, r = .95, r_type = "split")
```

Se vogliamo un test con un'affidabilità di 0.95, dato che il nostro test attuale ha un'affidabilità di 0.862 basata su 16 item, avremmo bisogno di un test che consiste di almeno 49 item.

La misura più comune della consistenza interna è il coefficiente alfa (Cronbach, 1951). Il coefficiente alfa rappresenta la media di tutte le possibili correlazioni split-half. Può essere calcolato utilizzando la funzione `coef_alpha()` in `hemp`.

```{r}
coef_alpha(SAPA)
```

Anche se le stime puntuali di affidabilità possono essere utili, è generalmente utile calcolare un intervallo di confidenza. Quando la dimensione del campione è piccola, le ipotesi dei modelli statistici non sono soddisfatte o la distribuzione campionaria di un parametro è sconosciuta, allora il bootstrap può essere utilizzato per costruire una distribuzione campionaria empirica, che possiamo quindi utilizzare per creare intervalli di confidenza (Efron & Tibshirani, 1986 ).

La nostra motivazione per l'introduzione del bootstrapping è che consente la creazione di intervalli di confidenza e incertezza indipendentemente dal parametro stimato. Pertanto, mentre illustriamo l'uso del bootstrap per il coefficiente alfa, questo può essere facilmente applicato per l'affidabilità split-half di cui sopra, la validità o le statistiche dell'analisi degli elementi presentate più avanti in questo capitolo e in molti altri contesti presentati in tutto il libro.
Per eseguire il bootstrap per il coefficiente alfa, possiamo usare, ad esempio, il pacchetto `boot` (Canty & Ripley, 2017).

```{r}
library(boot)
```

Dobbiamo creare una funzione da passare alla funzione `boot`, che chiamiamo `alpha_fun()`. Questa funzione accetta due argomenti: un set di dati, chiamato `data`, e una matrice di indici, chiamata `row` Questi argomenti vengono quindi passati alla funzione `coef_alpha`. Ciò consentirà alla funzione `boot` di eseguire un campionamento con rimpiazzo delle righe di `data` e creare una distribuzione empirica per il coefficiente alfa.

```{r}
alpha_fun <- function(data, row){
  coef_alpha(data[row, ])}
```

```{r}
alpha_boot <- boot(SAPA, alpha_fun, R = 1e3)
alpha_boot
```
La figura seguente mostra un istogramma e un Q-Q plot della distribuzione empirica per il coefficiente alfa basato sui 1000 campioni.

```{r}
plot(alpha_boot)
```

Utilizziamo la funzione `boot.ci()` per calcolare gli intervalli di confidenza al 95% utilizzando gli intervalli `norm`, `basic` e `perc`. In breve, gli intervalli `basic` e `perc` fanno meno assunzioni rispetto agli intervalli `norm` (ovvero, nessuna assunzione di normalità asintotica). Se la distribuzione empirica si discosta dalla normalità, allora gli intervalli `basic` e `perc` sono una scelta migliore.

```{r}
boot.ci(alpha_boot, type = c("norm", "basic", "perc"))
```

Si può concludere che per la maggior parte degli scopi l'attendibilità sembra essere sufficientemente alta da non richiedere l'onere aggiuntivo per i rispondenti di aumentare la lunghezza dello strumento.

## Validità

La validità può essere esaminato in molti modi e quantificare l'evidenza di validità è abbastanza semplice con R. Una forma comune di evidenza di validità è l'opinione di un esperto. Le opinioni degli esperti possono aiutare a valutare l'adeguatezza del contenuto degli item, se lo strumento sta campionando adeguatamente tutte le dimensioni del dominio del costrutto e se gli item sono necessari per misurare il costrutto. Un modo per quantificare questo tipo di validità è con il *content validity ratio*, CVR (Lawshe, 1975). Il CVR è definito come:

$$
CVR = \frac{n_e - (N/2)}{N/2},
$$

dove $n_e$ è il numero di esperti che ritengono essenziale l'item e $N$ è il numero totale di esperti. Ad esempio, possiamo costruire uno strumento che interroghi i genitori sull'aggressività nei loro figli. Un item potrebbe chiedere: "Tuo figlio morde altri bambini?" Se chiediamo a 20 esperti se pensano che questo elemento sia essenziale per misurare l'aggressività nei bambini e 17 concordano che lo sia, allora il CVR può essere calcolato usando la funzione `cvr` di `hemp`:

```{r}
cvr(N = 20, n_e = 17)
```
Scopriamo che il CVR è 0.70 per questo particolare item, ma non sappiamo se 0.70 sia abbastanza grande da mantenere l'item nello strumento. La tabella 1 in Lawshe (1975) fornisce valori soglia di CVR dato un determinato numero di esperti. Per 20 esperti, il CVR minimo è 0.42 e per cui potremmo concludere che gli esperti ritengono che questo item sia utile e potremmo mantenere questo item nel nostro strumento.

Altre forme di prova di validità valutano la misura in cui i punteggi del test si riferiscono a qualche criterio esterno (validità di criterio). Il supporto statistico per questa forma di validità può comportare il calcolo di correlazioni semplici o l'uso della regressione.

Esaminiamo al set di dati `interest`, un set di dati artificiale che contiene misure di personalità, di capacità cognitive e di interessi vocazionali. Per questi dati potremmo aspettarci che il test del vocabolario (`vocab`) sia correlato con valutazioni che misurano la comprensione della lettura (`reading`) e il completamento di frasi (`sent-comp`). Pertanto, possiamo utilizzare la funzione `cor` per calcolare la correlazione di Pearson tra queste variabili.

```{r}
cor(interest[, c("vocab", "reading", "sentcomp")])
```

La correlazione di Pearson tra `vocab` e `reading` è 0.803, mentre la correlazione tra `vocab` e `sentcomp` è 0.813. Ciò rappresenterebbe una prova di validità concorrente se i test di vocabolario fossero somministrati al momento della valutazione della lettura e del completamento della frase. Se il test del vocabolario precede i test di lettura e di completamento della frase, allora la correlazione di Pearson rappresenta una prova a sostegno della validità predittiva.

Supponiamo che stiamo misurando l'interesse di qualcuno a diventare un insegnante (`teacher`) utilizzando una misura della personalità del dominio sociale (`socdom`) e vogliamo capire se esiste un'ulteriore capacità predittiva di somministrare la valutazione della comprensione della lettura oltre la sola misura della personalità. Stiamo cercando di valutare se la comprensione della lettura ha una validità incrementale, e ciò può essere valutato usando l'analisi di regressione.

```{r}
mod_old <- lm(teacher ~ socdom, interest)
mod_new <- lm(teacher ~ socdom + reading, interest)
```


I modelli risultanti sono nidificati perché `mod_new` include le stesse variabili dipendenti e indipendenti utilizzate in `mod_old` includendo anche la variabile di lettura. Per esaminare il contributo della lettura oltre il socdom nella previsione dell'insegnante, possiamo estrarre il cambiamento nel valore R-quadrato (R2) tra i due modelli. Inoltre, possiamo confrontare statisticamente i modelli utilizzando la funzione `anova` in R. Questo test esamina se la variazione di R2 tra i due modelli è statisticamente significativa (ovvero, R2 > 0). Se la variazione R2 è statisticamente significativa, allora possiamo concludere che la lettura spiega una quantità significativa di variazione nella variabile dipendente `teacher` oltre `socdom`.

```{r}
summary(mod_new)$r.squared - summary(mod_old)$r.squared
anova(mod_old, mod_new)
```

Dall'output sopra, vediamo che la valutazione della comprensione della lettura ha una validità incrementale oltre la sola misura di dominanza sociale (p <.001) e che spiega circa il 9% in più di variabilità nell'interesse per la professione di insegnante.

```{r}
summary(mod_old)
```
```{r}
summary(mod_new)
```

## Analisi degli item

### Difficoltà degli item

L'analisi degli item svolge un ruolo importante nello sviluppo e nella revisione dei test psicometrici. L'analisi degli item può essere eseguita utilizzando le funzioni di base fornite da R. Torniamo al set di dati SAPA per dimostrare come eseguire l'analisi degli item per item a risposta dicotomica.

Una statistica comune da calcolare durante l'analisi degli item è la proporzione di esaminandi che rispondono correttamente ad ogni item. Questa è nota come *difficoltà dell'item*, *p*. Questo valore non va confuso con la difficoltà dell'item nella teoria della risposta agli item o con il valore-$p$ dei test di ipotesi frequentisti. L'item con la maggiore difficoltà, ironicamente, è l'item più facile per gli esaminandi, motivo per cui la difficoltà dell'item è talvolta indicata come facilità dell'item.

Possiamo utilizzare la funzione `colMeans` per calcolare la difficoltà degli item. Poiché abbiamo dei partecipanti che hanno risposte mancanti su alcuni item, dobbiamo passare l'argomento `na.rm = TRUE` per ignorare i dati mancanti. In caso contrario, la funzione `colMeans` restituirebbe `NA` per gli item che hanno almeno un valore mancante. Per rendere più leggibili i valori di difficoltà degli item, arrotondiamo a tre decimali utilizzando la funzione `round`.

```{r}
item_diff <- colMeans(SAPA, na.rm = TRUE)
round(item_diff, 3)
```
L'output mostra che gli item `reason.16` e `reason.17` ottengono i livelli di difficoltà  più alti, mentre `rotate.8` ha il livello di difficoltà più basso. Circa il 70% degli studenti è stato in grado di rispondere correttamente a `reason.16` e `reason.17`, mentre solo il 19% ha risposto correttamente a `rotate.8`.

### Potere discriminante degli item

Un'altra statistica ampiamente utilizzata nell'analisi degli item è il potere discriminante degli item, che si riferisce alla capacità dell'item nel distinguere gli esaminandi con una alta abilità da quelli con una bassa abilità. Sebbene esistano molti modi per calcolare la discriminazione degli item, la forma più comune è la correlazione punto-biserial tra le risposte degli esaminandi all'item e il loro punteggio totale nel test. Valori grandi e positivi indicano una forte relazione tra il rispondere correttamente all'item e avere un punteggio alto nel test, mentre valori vicini allo zero indicano nessuna relazione e valori negativi indicano che il rispondere correttamente all'item è associato a un punteggio complessivo del test più basso. Valori vicini allo zero o negativi suggeriscono che l'item potrebbe non funzionare correttamente. Alcune delle ragioni per ottenere una discriminazione degli item bassa o negativa potrebbero essere l'utilizzo di una chiave di risposta errata per l'item o l'assenza di risposte corrette. Indipendentemente dalla causa, gli item con correlazioni punto-biserial basse o negative devono essere modificati, se il test/strumento è in fase di revisione, o rimossi dal test e dal punteggio.

Per calcolare il potere discriminante dell'item per i dati SAPA, prima calcoliamo il punteggio totale del test utilizzando la funzione `rowSums` insieme all'opzione `na.rm = TRUE` e lo salviamo come `total_score`. Successivamente, correlaziamo gli item in SAPA con il punteggio totale del test utilizzando la funzione `cor`. Specificamente, usiamo l'argomento `use = "pairwise.complete.obs"` nella funzione `cor` a causa della presenza di risposte mancanti. Infine, salviamo la matrice di correlazione come `item_discr` e la stampiamo.

```{r}
total_score <- rowSums(SAPA, na.rm = TRUE)
item_discr <- cor(SAPA, total_score, use = "pairwise.complete.obs")
round(item_discr, 2)
```

I risultati mostrano che tutti gli item del test SAPA sono moderatamente e positivamente correlati con il punteggio totale del test. Questo indica che tutti gli item funzionano correttamente e non fornisce informazioni salienti su quali item rimuovere o modificare.

Un altro modo per calcolare il potere discriminante degli item consiste nel dividere i candidati in due gruppi (ad esempio, 1 = alto rendimento e 0 = basso rendimento) in base ai loro punteggi totali nel test e correlare questa variabile di raggruppamento con le risposte agli item. Questo è noto come *indice di discriminazione degli item*. Un'opzione per creare gruppi di alto e basso rendimento è selezionare il 27% più alto e il 27% più basso dei candidati in base ai loro punteggi totali nel test. Va notato che la decisione di utilizzare il 27% è in qualche modo arbitraria. Potremmo facilmente utilizzare un altro valore (ad esempio, il 10% o il 20%) per definire i gruppi di alto e basso rendimento. Dopo aver definito il punto di cut-off per i gruppi, calcoliamo la proporzione di candidati che hanno risposto correttamente all'elemento nei gruppi di alto e basso rendimento.

Nell'esempio seguente, calcoliamo l'indice di discriminazione dell'elemento `reason.4` nel set di dati SAPA utilizzando la funzione `idi` del pacchetto `hemp`. Per specificare i gruppi di alto e basso rendimento, utilizziamo il valore `perc_cut = .27` nella funzione `idi`.

```{r}
idi(SAPA, SAPA$reason.4, perc_cut = .27)
```

Abbiamo scoperto che l'81% dei candidati nel gruppo di alto rendimento ha risposto correttamente all'item `reason.4`, mentre solo il 19% dei candidati nel gruppo di basso rendimento ha risposto correttamente. Questo suggerisce che l'item era più facile per i candidati di alto rendimento e più difficile per quelli di basso rendimento. Pertanto, possiamo dire che questo particolare item risulta utile per differenziare i due gruppi, ma non necessariamente all'interno di ciascun gruppo.


### Indice di affidabilità dell'item

Oltre agli indici di difficoltà e discriminazione degli elementi, un'altra statistica utile per l'analisi degli elementi è l'indice di affidabilità dell'elemento. L'indice di affidabilità dell'elemento (IRI) è definito come:

$$
IRI = S_i \cdot r_{i,tt},
$$

dove $S_i$ è la deviazione standard dell'item $i$ e $r_{i,tt}$ è la correlazione tra l'item $i$ e il punteggio totale del test. L'IRI può teoricamente variare tra -0.5 e 0.5, con valori grandi e positivi indicativi di alta affidabilità. 

Di seguito calcoliamo l'IRI per tutti gli item nel set di dati SAPA. Possiamo farlo utilizzando la funzione `iri` in `hemp`.

```{r}
iri(SAPA)
```

I risultati restituiti dalla funzione `iri` mostrano che l'IRI varia da circa 0.19 a 0.29 per il set di dati SAPA. Tutti questi sono valori ragionevoli per l'IRI (ovvero nessuno è negativo o vicino allo zero).

### Indice di validità dell'item

Quando invece del punteggio totale del test viene utilizzato un criterio esterno, questo indice è noto come *indice di validità dell'item* (IVI). L'IVI può variare anche tra -0.5 e 0.5, con valori elevati (in valore assoluto) che indicano una validità maggiore. Valori negativi elevati indicano una maggiore validità quando ci si aspetta che gli elementi siano correlati in modo negativo con il criterio.

Nell'esempio seguente, utilizziamo la funzione `ivi` in `hemp` con "reason.17" come criterio esterno e "reason.4" come elemento di interesse e troviamo che l'IVI è 0.19.

```{r}
ivi(item = SAPA$reason.4, crit = SAPA$reason.17)
```

### Distrattori

Un altro aspetto importante degli elementi che deve essere analizzato sono le opzioni di risposta. Nel contesto dei test a scelta multipla, le opzioni di risposta alternative (cioè sbagliate) vengono definite "distrattori". I distrattori svolgono un ruolo importante in un elemento a scelta multipla. Per garantire elementi a scelta multipla di alta qualità, è cruciale includere distrattori plausibili e ben funzionanti che siano più probabili di attirare i candidati con conoscenze parziali. I distrattori non plausibili potrebbero dover essere riscritti o sostituiti con un distrattore migliore. La qualità dei distrattori viene tipicamente valutata attraverso l'analisi dei distrattori. L'analisi dei distrattori viene spesso condotta osservando la proporzione di candidati che scelgono un distrattore particolare.

Per illustrare l'analisi dei distrattori, utilizziamo gli item del data set `multiplechoice` in `hemp`. Si tratta di un ipotetico test a scelta multipla composto da 27 item somministrati a 496 candidati. Le quattro opzioni di risposta sono codificate come 1, 2, 3 e 4 nel data set. Utilizziamo la funzione `distract` in `hemp` per calcolare la proporzione di candidati che selezionano ciascun distrattore. 

```{r}
distractors <- distract(multiplechoice)
head(distractors)
```

Nella tabella sopra, vediamo che tutti gli item avevano distrattori selezionati circa il 5% delle volte o meno. Questi distrattori potrebbero essere candidati per una revisione in quanto sono stati approvati ad un livello così basso da suggerire che la maggior parte degli esaminandi non li ha considerati come opzioni plausibili. Per l'item 1, i distrattori funzionavano tutti più o meno allo stesso modo (ovvero circa il 5% delle volte ogniuno è stato approvato), suggerendo che funzionavano tutti bene rispetto l'uno all'altro, ma che l'item era troppo facile (la risposta corretta era l'opzione 4, selezionata dall'84.5% degli esaminandi). Al contrario, l'item 5 era un item più difficile, con la risposta corretta che ancora una volta era l'opzione 4. Le opzioni 1 e 2 erano molto probabilmente fraintendimenti, mentre l'opzione 3 potrebbe essere rivista o potenzialmente eliminata da questo item a causa del basso tasso di approvazione (solo il 4.2%). Dato l'approvazione molto alta dell'opzione 1 (35.1%), è molto probabile che anche questa opzione fosse corretta. Per ottenere una visione più completa del funzionamento dell'item, sarebbe consigliabile calcolare l'indice di discriminazione specifico per quell'item. Questo ci permetterebbe di ottenere ulteriori informazioni sulla capacità dell'item di distinguere tra candidati di alto e basso livello.

