[["il-modello-unifattoriale.html", "Capitolo 13 Il modello unifattoriale", " Capitolo 13 Il modello unifattoriale In questo capitolo saranno presentate le nozioni fondamentali dell’analisi fattoriale, un modello statistico che consente di spiegare le correlazioni tra variabili osservate mediante la loro saturazione in uno o più fattori generali. In questo modello, le \\(p\\) variabili osservate (item) sono considerate condizionalmente indipendenti rispetto a \\(m\\) variabili latenti chiamate fattori. L’obiettivo dell’analisi fattoriale è di interpretare questi fattori come costrutti teorici inosservabili. Ad esempio, l’analisi fattoriale può essere utilizzata per spiegare le correlazioni tra le prestazioni di un gruppo di individui in una serie di compiti mediante il concetto di intelligenza. In questo modo, l’analisi fattoriale aiuta a identificare i costrutti cui gli item si riferiscono e a stabilire in che misura ciascun item rappresenta il costrutto. Il modello può essere unifattoriale (\\(m = 1\\)) o multifattoriale (\\(m &gt; 1\\)), e in questo capitolo si introdurrà il modello unifattoriale che assume l’esistenza di un unico fattore comune latente. "],["modello-monofattoriale.html", "13.1 Modello monofattoriale", " 13.1 Modello monofattoriale Con \\(p\\) variabili manifeste \\(y_i\\), il caso più semplice è quello di un solo fattore comune: \\[\\begin{equation} y_i = \\mu_i + \\lambda_{i} \\xi + 1 \\cdot \\varepsilon_i \\qquad i=1, \\dots, p, \\tag{13.1} \\end{equation}\\] dove \\(\\xi\\) rappresenta il fattore comune a tutte le \\(y_i\\), \\(\\varepsilon_i\\) sono i fattori specifici o unici di ogni variabile osservata e \\(\\lambda_i\\) sono le saturazioni (o pesi) fattoriali le quali stabiliscono il peso del fattore latente su ciascuna variabile osservata. Il modello di analisi fattoriale e il modello di regressione possono sembrare simili, ma presentano alcune differenze importanti. In primo luogo, sia il fattore comune \\(\\xi\\) sia i fattori specifici \\(\\varepsilon_i\\) sono inosservabili, il che rende tutto ciò che si trova a destra dell’uguaglianza incognito. In secondo luogo, l’analisi di regressione e l’analisi fattoriale hanno obiettivi diversi. L’analisi di regressione mira a individuare le variabili esplicative, osservabili direttamente, che sono in grado di spiegare la maggior parte della varianza della variabile dipendente. Al contrario, il problema dell’analisi unifattoriale consiste nell’identificare la variabile esplicativa inosservabile che è in grado di spiegare la maggior parte della covarianza tra le variabili osservate. Solitamente, per comodità, si assume che la media delle variabili osservate \\(y_i\\) sia zero, ovvero \\(\\mu_i=0\\). Ciò equivale a considerare gli scarti delle variabili rispetto alle rispettive medie. Il modello unifattoriale assume che le variabili osservate siano il risultato della combinazione lineare di un fattore comune \\(\\xi\\) e dei fattori specifici \\(\\varepsilon_i\\), ovvero: \\[\\begin{equation} y_i -\\mu_i = \\lambda_i \\xi + 1 \\cdot \\varepsilon_i, \\tag{13.2} \\end{equation}\\] dove \\(\\lambda_i\\) è il carico o il peso della variabile \\(i\\)-esima sul fattore comune e \\(\\varepsilon_i\\) rappresenta il fattore specifico della variabile \\(i\\)-esima. Si assume che il fattore comune abbia media zero e varianza unitaria, mentre i fattori specifici abbiano media zero, varianza \\(\\psi_{i}\\) e siano incorrelati tra loro e con il fattore comune. Nel modello unifattoriale, l’interdipendenza tra le variabili è completamente spiegata dal fattore comune. Le ipotesi precedenti consentono di ricavare la covarianza tra la variabile osservata \\(y_i\\) e il fattore comune, la varianza della variabile osservata \\(y_i\\) e la covarianza tra due variabili osservate \\(y_i\\) e \\(y_k\\). L’obiettivo della discussione in questo capitolo è appunto quello di analizzare tali grandezze statistiche. "],["correlazione-parziale.html", "13.2 Correlazione parziale", " 13.2 Correlazione parziale Prima di entrare nel dettaglio del modello statistico dell’analisi fattoriale, è importante chiarire il concetto di correlazione parziale. Si attribuisce spesso a Charles Spearman la nascita dell’analisi fattoriale. Nel 1904, Spearman pubblicò un articolo intitolato “General Intelligence, Objectively Determined and Measured” in cui propose la Teoria dei Due Fattori. In questo articolo, dimostrò come fosse possibile identificare un fattore inosservabile a partire da una matrice di correlazioni, utilizzando il metodo dell’annullamento della tetrade (tetrad differences). L’annullamento della tetrade è un’applicazione della teoria della correlazione parziale, che mira a stabilire se, controllando un insieme di variabili inosservabili chiamate fattori \\(\\xi_j\\), le correlazioni tra le variabili osservabili \\(Y_i\\), al netto degli effetti lineari delle \\(\\xi_j\\), diventino statisticamente nulle. Possiamo considerare un esempio con tre variabili: \\(Y_1\\), \\(Y_2\\) e \\(F\\). La correlazione tra \\(Y_1\\) e \\(Y_2\\), \\(r_{1,2}\\), può essere influenzata dalla presenza di \\(F\\). Per calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\), dobbiamo trovare le componenti di \\(Y_1\\) e \\(Y_2\\) che sono linearmente indipendenti da \\(F\\). Per fare ciò, dobbiamo trovare la componente di \\(Y_1\\) che è ortogonale a \\(F\\). Possiamo calcolare i residui \\(E_1\\) del modello: \\[ Y_1 = b_{01} + b_{11}F + E_1. \\] La componente di \\(Y_1\\) linearmente indipendente da \\(F\\) è quindi data dai residui \\(E_1\\). Possiamo eseguire un’operazione analoga per \\(Y_2\\) per trovare la sua componente ortogonale a \\(F\\). Calcolando la correlazione tra le due componenti così ottenute si ottiene la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto lineare di \\(F\\). L’eq. (13.3) permette di calcolare la correlazione parziale tra \\(Y_1\\) e \\(Y_2\\) al netto dell’effetto di \\(F\\) a partire dalle correlazioni semplici tra le tre variabili \\(Y_1\\), \\(Y_2\\) e \\(F\\). \\[\\begin{equation} r_{1,2 \\mid F} = \\frac{r_{12} - r_{1F}r_{2F}}{\\sqrt{(1-r_{1F}^2)(1-r_{2F}^2)}} \\tag{13.3} \\end{equation}\\] In particolare, la correlazione parziale \\(r_{1,2 \\mid F}\\) è data dalla differenza tra la correlazione \\(r_{12}\\) tra \\(Y_1\\) e \\(Y_2\\) e il prodotto tra le correlazioni \\(r_{1F}\\) e \\(r_{2F}\\) tra ciascuna delle due variabili e \\(F\\), il tutto diviso per la radice quadrata del prodotto delle differenze tra 1 e i quadrati delle correlazioni tra \\(Y_1\\) e \\(F\\) e tra \\(Y_2\\) e \\(F\\). In altre parole, la formula tiene conto dell’effetto di \\(F\\) sulle correlazioni tra \\(Y_1\\) e \\(Y_2\\) per ottenere una stima della relazione diretta tra le due variabili, eliminando l’effetto del fattore comune. Facciamo un esempio numerico. Sia \\(f\\) una variabile su cui misuriamo \\(n\\) valori set.seed(123) n &lt;- 1000 f &lt;- rnorm(n, 24, 12) Siano \\(y_1\\) e \\(y_2\\) funzioni lineari di \\(f\\), a cui viene aggiunta una componente d’errore gaussiano: y1 &lt;- 10 + 7 * f + rnorm(n, 0, 50) y2 &lt;- 3 + 2 * f + rnorm(n, 0, 50) La correlazione tra \\(y_1\\) e \\(y_2\\) (\\(r_{12}= 0.355\\)) deriva dal fatto che \\(\\hat{y}_1\\) e \\(\\hat{y}_2\\) sono entrambe funzioni lineari di \\(f\\): Y &lt;- cbind(y1, y2, f) R &lt;- cor(Y) round(R, 3) #&gt; y1 y2 f #&gt; y1 1.000 0.380 0.867 #&gt; y2 0.380 1.000 0.423 #&gt; f 0.867 0.423 1.000 Eseguiamo le regressioni di \\(y_1\\) su \\(f\\) e di \\(y_2\\) su \\(F\\): fm1 &lt;- lm(y1 ~ f) fm2 &lt;- lm(y2 ~ f) Nella regressione, ciascuna osservazione \\(y_{i1}\\) viene scomposta in due componenti linearmente indipendenti, i valori adattati \\(\\hat{y}_{i}\\) e i residui, \\(e_{i}\\): \\(y_i = \\hat{y}_i + e_1\\). Nel caso di \\(y_1\\) abbiamo round(head(cbind(y1, y1.hat = fm1$fit, e = fm1$res, fm1$fit + fm1$res)), 3) #&gt; y1 y1.hat e #&gt; 1 81.130 130.505 -49.375 81.130 #&gt; 2 106.667 159.704 -53.037 106.667 #&gt; 3 308.032 317.846 -9.813 308.032 #&gt; 4 177.314 186.285 -8.971 177.314 #&gt; 5 61.393 191.482 -130.089 61.393 #&gt; 6 374.094 331.668 42.426 374.094 Lo stesso può dirsi di \\(y_2\\). La correlazione parziale \\(r_{12 \\mid f}\\) tra \\(y_1\\) e \\(y_2\\) dato \\(f\\) è uguale alla correlazione di Pearson tra i residui \\(e_1\\) e \\(e_2\\) calcolati mediante i due modelli di regressione descritti sopra: cor(fm1$res, fm2$res) #&gt; [1] 0.02828618 La correlazione parziale tra \\(y_1\\) e \\(y_2\\) al netto di \\(f\\) è .02829. Per i dati esaminati sopra, dunque, la correlazione parziale tra le variabili \\(y_1\\) e \\(y_2\\) diventa uguale a zero se la variabile \\(f\\) viene controllata (ovvero, se escludiamo da \\(y_1\\) e da \\(y_2\\) l’effetto lineare di \\(f\\)). Il fatto che la correlazione parziale sia zero significa che la correlazione che abbiamo osservato tra \\(y_1\\) e \\(y_2\\) (\\(r = 0.355\\)) non dipendeva dall’effetto che una variabile \\(y\\) esercitava sull’altra, ma bensì dal fatto che c’era una terza variabile, \\(f\\), che influenzava sia \\(y_1\\) sia \\(y_2\\). In altre parole, le variabili \\(y_1\\) e \\(y_2\\) sono condizionalmente indipendenti dato \\(f\\). Ciò significa, come abbiamo visto sopra, che la componente di \\(y_1\\) linearmente indipendente da \\(f\\) è incorrelata con la componente di \\(y_2\\) linearmente indipendente da \\(f\\). La correlazione che abbiamo calcolato tra i residui di due modelli di regressione è identica alla correlazione che viene calcolata applicando la (13.3): (R[1, 2] - R[1, 3] * R[2, 3]) / sqrt((1 - R[1, 3]^2) * (1 - R[2, 3]^2)) %&gt;% round(3) #&gt; [1] 0.02827513 In conclusione, possiamo attribuire alla formula (13.3) la seguente interpretazione: la correlazione parziale tra le variabili \\(y_1\\) e \\(y_2\\) condizionata alla variabile \\(f\\) rappresenta la correlazione tra le componenti di \\(y_1\\) e \\(y_2\\) da cui è stato rimosso l’effetto lineare di \\(f\\). Ricordiamo il ragionamento che abbiamo svolto in precedenza per fornire un’interpretazione al coefficiente parziale del modello di regressione. "],["principio-base-dellanalisi-fattoriale.html", "13.3 Principio base dell’analisi fattoriale", " 13.3 Principio base dell’analisi fattoriale Nell’attuale pratica dell’inferenza statistica nell’analisi fattoriale, spesso si utilizzano stime della massima verosimiglianza, calcolate attraverso procedure iterative come l’algoritmo EM (Rubin &amp; Thayer, 1982). Tuttavia, all’inizio dell’analisi fattoriale, la procedura di estrazione dei fattori si basava sulle relazioni invarianti che il modello fattoriale imponeva agli elementi della matrice di covarianza delle variabili osservate. Uno dei più noti tra questi invarianti è la tetrade, presente nei modelli ad un fattore. La tetrade consiste in una combinazione di quattro correlazioni. Se l’associazione tra le variabili osservate dipende dal fatto che queste sono state generate causalmente da un fattore comune inosservabile, allora è possibile generare una combinazione delle correlazioni che annulla la tetrade. In altre parole, l’analisi fattoriale si propone di individuare un insieme di sole \\(m&lt;p\\) variabili latenti che, al netto dei fattori comuni, annullano significativamente tutte le correlazioni parziali tra le \\(p\\) variabili osservate. Se il metodo della correlazione parziale consente di identificare \\(m\\) variabili latenti, allora lo psicologo può concludere che tali fattori corrispondono agli \\(m\\) costrutti che intende misurare. Per illustrare il metodo dell’annullamento della tetrade, consideriamo la matrice di correlazioni riportata nella Tabella seguente. Matrice di correlazioni nella quale tutte le correlazioni parziali tra le variabili \\(Y\\) al netto dell’effetto di \\(\\xi\\) sono nulle. \\(\\xi\\) \\(y_1\\) \\(y_2\\) \\(y_3\\) \\(y_4\\) \\(y_5\\) \\(\\xi\\) 1.00 \\(y_1\\) 0.90 1.00 \\(y_2\\) 0.80 0.72 1.00 \\(y_3\\) 0.70 0.63 0.56 1.00 \\(y_4\\) 0.60 0.54 0.48 0.42 1.00 \\(y_5\\) 0.50 0.45 0.40 0.35 0.30 1.00 Nella tabella, la correlazione parziale tra ciascuna coppia di variabili \\(y_i\\), \\(y_j\\) (con \\(i \\neq j\\)) dato \\(\\xi\\) è sempre pari a zero. Ad esempio, la correlazione parziale tra \\(y_3\\) e \\(y_5\\) condizionata a \\(\\xi\\) è: \\[\\begin{equation} \\begin{aligned} r_{35 \\mid \\xi} &amp;= \\frac{r_{35} - r_{3\\xi}r_{5\\xi}} {\\sqrt{(1-r_{3\\xi}^2)(1-r_{5\\xi}^2)}} \\notag \\\\[12pt] &amp;= \\frac{0.35 - 0.7 \\times 0.5} {\\sqrt{(1-0.7^2)(1-0.5^2)}} = 0. \\notag \\end{aligned} \\end{equation}\\] Lo stesso risultato si trova per qualunque altra coppia di variabili \\(y_i\\) e \\(y_j\\), ovvero \\(r_{ij \\mid \\xi} = 0\\). Possiamo dunque dire che, per la matrice di correlazioni della Tabella, esiste un’unica variabile \\(\\xi\\) la quale, quando viene controllata, spiega tutte le \\[ p(p-1)/2 = 5(5-1)/2=10 \\] correlazioni tra le variabili \\(y\\). Questo risultato non è sorprendente, in quanto la matrice di correlazioni è stata costruita in modo tale da possedere tale proprietà. Immaginiamo invece di trovarci in una situazione diversa, ovvero di avere a disposizione solo le variabili \\(y_i\\) senza conoscere \\(\\xi\\). In questo caso, ci poniamo la domanda: “Esiste una variabile latente \\(\\xi\\) tale che, se fosse osservabile, renderebbe nulle tutte le correlazioni parziali tra le variabili \\(y\\)?”. Se esiste una tale variabile latente che spiega tutte le correlazioni tra le variabili osservate \\(y\\), allora viene chiamata fattore latente. Definizione 13.1 Un fattore è una variabile inosservabile in grado di rendere significativamente nulle tutte le correlazioni parziali tra le variabili manifeste. 13.3.1 Vincoli sulle correlazioni Come si può stabilire se esiste una variabile inosservabile in grado di rendere nulle tutte le correlazioni parziali tra le variabili osservate? Riscriviamo la (13.3) per specificare la correlazione parziale tra le variabili \\(y_i\\) e \\(y_j\\) dato \\(\\xi\\): \\[ r_{ij \\mid \\xi} = \\frac{r_{ij} - r_{i\\xi}r_{j\\xi}} {\\sqrt{(1-r_{i\\xi}^2)(1-r_{j\\xi}^2)}} \\] Affinché \\(r_{ij \\mid \\xi}\\) sia uguale a zero è necessario che \\[ r_{ij} - r_{i\\xi}r_{j\\xi}=0 \\] ovvero \\[ r_{ij} = r_{i\\xi}r_{j\\xi}. \\] In altri termini, se esiste un fattore non osservato \\(\\xi\\) in grado di rendere uguali a zero tutte le correlazioni parziali \\(r_{ih \\mid \\xi}\\), allora la correlazione tra ciascuna coppia di variabili \\(y\\) deve essere uguale al prodotto delle correlazioni tra ciascuna \\(y\\) e il fattore latente \\(\\xi\\). Questo è il principio base dell’analisi fattoriale. 13.3.2 Teoria dei Due Fattori Per fornire un esempio concreto del metodo dell’annullamento della tetrade, possiamo esaminare la matrice di correlazioni utilizzata da Spearman (1904) nella sua ricerca sulle capacità intellettuali di alcuni studenti di una scuola superiore. In particolare, sono state considerate le prestazioni degli studenti in tre materie scolastiche (studio dei classici, letteratura inglese, abilità matematiche) e in un compito di discriminazione dell’altezza di suoni. Secondo la Teoria dei Due Fattori di Spearman, le prestazioni in ogni compito intellettuale sono costituite da due componenti: un fattore generale comune a tutte le attività intellettuali (fattore “g”) e un fattore specifico relativo al compito in questione (fattore “s”). In questo modello, il fattore “g” rappresenta la componente invariante dell’abilità intellettiva, mentre il fattore “s” è una componente che varia da condizione a condizione. Per verificare l’esistenza di una variabile latente in grado di spiegare le correlazioni tra le variabili osservate da Spearman, è stato proposto uno strumento chiamato “annullamento della tetrade”. Tale strumento si basa sui vincoli sulle correlazioni derivanti dalla definizione di correlazione parziale. Come abbiamo visto in precedenza, la correlazione parziale tra le variabili \\(y\\) indicizzate da \\(i\\) e \\(j\\), al netto dell’effetto di \\(\\xi\\), è nulla se la seguente relazione è soddisfatta: \\[ r_{ij} = r_{i\\xi}r_{j\\xi}. \\] In altre parole, se la correlazione tra due variabili osservate può essere spiegata dall’effetto di una terza variabile latente, allora la correlazione parziale tra tali variabili sarà nulla una volta che si tiene conto dell’effetto della variabile latente. Utilizzando questo strumento, Spearman ha dimostrato che le correlazioni tra le prestazioni degli studenti nei vari compiti intellettuali possono essere spiegate da due fattori: un fattore generale comune a tutti i compiti (fattore “g”) e un fattore specifico a ciascun compito (fattore “s”). Nel caso dei dati di Spearman, le correlazioni parziali sono nulle quando la correlazione tra “studi classici” e “letteratura inglese” è pari al prodotto della correlazione tra “studi classici” e la variabile latente \\(\\xi\\) e della correlazione tra “letteratura inglese” e la variabile latente \\(\\xi\\). Inoltre, la correlazione tra “studi classici” e “abilità matematica” deve essere uguale al prodotto della correlazione tra “studi classici” e la variabile latente \\(\\xi\\) e della correlazione tra “abilità matematica” e la variabile latente \\(\\xi\\), e così via per tutte le altre coppie di variabili. Le correlazioni tra le variabili manifeste e il fattore latente sono dette saturazioni fattoriali e vengono denotate con la lettera \\(\\lambda\\). Se il modello di Spearman è corretto, avremo che \\[ r_{ec}=\\lambda_e \\times \\lambda_{c}, \\] dove \\(r_{ec}\\) è la correlazione tra “letteratura inglese” (e) e “studi classici” (c), \\(\\lambda_e\\) è la correlazione tra “letteratura inglese” e \\(\\xi\\), e \\(\\lambda_{c}\\) è la correlazione tra “studi classici” e \\(\\xi\\). Allo stesso modo, la correlazione tra “studi classici” e “matematica” (m) dovrà essere uguale a \\[ \\lambda_c \\times \\lambda_m, \\] eccetera. 13.3.3 Annullamento della tetrade Utilizzando il metodo dell’annullamento della tetrade è possibile stimare i valori delle saturazioni fattoriali \\(\\lambda\\), partendo dalle correlazioni tra le tre coppie di variabili manifeste \\(c\\), \\(m\\) ed \\(e\\). In particolare, si possono scrivere tre equazioni in tre incognite, che consentono di calcolare le saturazioni \\(\\lambda\\). Ad esempio, per le tre variabili sopracitate, tali equazioni possono essere espresse nel seguente modo: \\[\\begin{equation} \\begin{aligned} r_{cm} &amp;= \\lambda_c \\times \\lambda_m, \\notag \\\\ r_{em} &amp;= \\lambda_e \\times \\lambda_m, \\\\ r_{ce} &amp;= \\lambda_c \\times \\lambda_e. \\notag \\end{aligned} \\end{equation}\\] Calcolando il determinante del sistema di equazioni lineari composto dalle correlazioni tra le variabili manifeste \\(c\\), \\(m\\) ed \\(e\\), possiamo ottenere il valore della saturazione fattoriale \\(\\lambda\\) e, in particolare, il coefficiente di saturazione \\(\\lambda_m\\) della variabile \\(y_m\\) nel fattore comune \\(\\xi\\). In altre parole, risolvendo il sistema di equazioni lineari, possiamo stimare il valore delle saturazioni fattoriali, compreso il coefficiente di saturazione \\(\\lambda_m\\), a partire dalle correlazioni tra le variabili manifeste: \\[\\begin{equation} \\lambda_m = \\sqrt{ \\frac{r_{cm} r_{em}}{r_{ce}}}. \\tag{13.4} \\end{equation}\\] Lo stesso vale per le altre due saturazioni \\(\\lambda_c\\) e \\(\\lambda_e\\). Nel suo articolo del 1904, Spearman osservò le seguenti correlazioni tra le variabili \\(Y_c\\), \\(Y_e\\), \\(Y_m\\) e \\(Y_p\\): \\[ \\begin{array}{ccccc} \\hline &amp; Y_C &amp; Y_E &amp; Y_M &amp; Y_P \\\\ \\hline Y_C &amp; 1.00 &amp; 0.78 &amp; 0.70 &amp; 0.66 \\\\ Y_E &amp; &amp; 1.00 &amp; 0.64 &amp; 0.54 \\\\ Y_M &amp; &amp; &amp; 1.00 &amp; 0.45 \\\\ Y_P &amp; &amp; &amp; &amp; 1.00 \\\\ \\hline \\end{array} \\] Utilizzando la (13.4), mediante le correlazioni \\(r_{cm}\\), \\(r_{em}\\), e \\(r_{ce}\\) fornite dalla tabella precedente, la saturazione \\(\\lambda_m\\) diventa uguale a: \\[ \\hat{\\lambda}_m = \\sqrt{ \\frac{r_{cm} r_{em}}{r_{ce}} } = \\sqrt{ \\frac{0.70 \\times 0.64}{0.78} } = 0.76. \\] È importante notare che il metodo dell’annullamento della tetrade produce risultati falsificabili. Infatti, ci sono modi diversi per calcolare la stessa saturazione fattoriale. Se il modello fattoriale è corretto si deve ottenere lo stesso risultato in tutti i casi. Nel caso presente, la saturazione fattoriale \\(\\lambda_m\\) può essere calcolata in altri due modi: \\[\\begin{equation} \\begin{aligned} \\hat{\\lambda}_m &amp;= \\sqrt{ \\frac{r_{cm} r_{mp}}{r_{cp}} } = \\sqrt{ \\frac{0.78 \\times 0.45}{0.66} } = 0.69, \\notag \\\\ \\hat{\\lambda}_m &amp;= \\sqrt{ \\frac{r_{em} r_{mp}}{r_{ep}} } = \\sqrt{ \\frac{0.64 \\times 0.45}{0.54} } = 0.73. \\notag\\end{aligned} \\end{equation}\\] I tre valori che sono stati ottenuti sono molto simili. Qual è allora la stima migliore di \\(\\lambda_m\\)? 13.3.4 Metodo del centroide La soluzione più semplice è quella di fare la media di questi tre valori (\\(\\bar{\\lambda}_m = 0.73\\)). Un metodo migliore (meno vulnerabile ai valori anomali) è dato dal rapporto tra la somma dei numeratori e dei denominatori: \\[ \\hat{\\lambda}_m = \\sqrt{ \\frac{0.70 \\times 0.64 + 0.78 \\times 0.45 + 0.64 \\times 0.45}{0.78+0.66+0.54} } = 0.73 \\] In questo caso, i due metodi danno lo stesso risultato. Le altre tre saturazioni fattoriali trovate mediante il metodo del centroide sono: \\[ \\hat{\\lambda}_c = 0.97, \\quad \\hat{\\lambda}_e = 0.84, \\quad \\hat{\\lambda}_p = 0.65. \\] In conclusione, \\[ \\boldsymbol{\\hat{\\Lambda}}^\\prime= (\\hat{\\lambda}_c, \\hat{\\lambda}_e, \\hat{\\lambda}_m, \\hat{\\lambda}_p) = (0.97, 0.84, 0.73, 0.65). \\] 13.3.5 Introduzione a lavaan Analizziamo ora nuovamente gli stessi dati usando un metodo di stima moderno (massima verosimiglianza), mediante le funzioni del pacchetto lavaan. La matrice completa dei dati di Spearman è messa a disposizione da Kan, Maas, and Levine (2019). Iniziamo a caricare i pacchetti necessari: library(&quot;lavaan&quot;) library(&quot;semPlot&quot;) library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) library(&quot;tidyr&quot;) library(&quot;corrplot&quot;) Specifichiamo il nome delle variabili manifeste varnames &lt;- c( &quot;Classics&quot;, &quot;French&quot;, &quot;English&quot;, &quot;Math&quot;, &quot;Pitch&quot;, &quot;Music&quot; ) e il loro numero ny &lt;- length(varnames) Leggiamo la matrice di correlazione: spearman_cor_mat &lt;- matrix( c( 1.00, .83, .78, .70, .66, .63, .83, 1.00, .67, .67, .65, .57, .78, .67, 1.00, .64, .54, .51, .70, .67, .64, 1.00, .45, .51, .66, .65, .54, .45, 1.00, .40, .63, .57, .51, .51, .40, 1.00 ), ny, ny, byrow = TRUE, dimnames = list(varnames, varnames) ) Specifichiamo l’ampiezza campionaria: n &lt;- 33 Esaminiamo la sintassi usata da lavaan a livello degli item: # Regression y ~ f1 + f2 + x1 + x2 f1 ~ f2 + f3 f2 ~ f3 + x1 + x2 # Latent variables f1 &lt;- ~ y1 + y2 + y3 f2 &lt;- ~ y4 + y5 + y6 f3 &lt;- ~ y7 + y8 + y9 + y10 # Variances and covariances y1 ~ ~y1 y1 ~ ~y2 f1 ~ ~f2 # Intercepts y1 ~ 1 f1 ~ 1 Definiamo il modello unifattoriale in lavaan. L’operatore =~ si può leggere dicendo che la variabile latente a sinistra dell’operatore viene identificata dalle variabili manifeste elencate a destra dell’operatore e separate dal segno +: spearman_mod &lt;- &quot; g =~ Classics + French + English + Math + Pitch + Music &quot; Adattiamo il modello ai dati con la funzione cfa(): fit1 &lt;- lavaan::cfa( spearman_mod, sample.cov = spearman_cor_mat, sample.nobs = n, std.lv = TRUE ) L’argomento std.lv = TRUE specifica che imponiamo una varianza pari a 1 a tutte le variabili latenti comuni (nel caso presente, solo una). Ciò consente di stimare le saturazioni fattoriali. Possiamo esaminare la soluzione ottenuta con la seguente istruzione: summary( fit1, fit.measures = TRUE, standardized = TRUE ) #&gt; lavaan 0.6.15 ended normally after 23 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 12 #&gt; #&gt; Number of observations 33 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 2.913 #&gt; Degrees of freedom 9 #&gt; P-value (Chi-square) 0.968 #&gt; #&gt; Model Test Baseline Model: #&gt; #&gt; Test statistic 133.625 #&gt; Degrees of freedom 15 #&gt; P-value 0.000 #&gt; #&gt; User Model versus Baseline Model: #&gt; #&gt; Comparative Fit Index (CFI) 1.000 #&gt; Tucker-Lewis Index (TLI) 1.086 #&gt; #&gt; Loglikelihood and Information Criteria: #&gt; #&gt; Loglikelihood user model (H0) -212.547 #&gt; Loglikelihood unrestricted model (H1) -211.091 #&gt; #&gt; Akaike (AIC) 449.094 #&gt; Bayesian (BIC) 467.052 #&gt; Sample-size adjusted Bayesian (SABIC) 429.622 #&gt; #&gt; Root Mean Square Error of Approximation: #&gt; #&gt; RMSEA 0.000 #&gt; 90 Percent confidence interval - lower 0.000 #&gt; 90 Percent confidence interval - upper 0.000 #&gt; P-value H_0: RMSEA &lt;= 0.050 0.976 #&gt; P-value H_0: RMSEA &gt;= 0.080 0.016 #&gt; #&gt; Standardized Root Mean Square Residual: #&gt; #&gt; SRMR 0.025 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Expected #&gt; Information saturated (h1) model Structured #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all #&gt; g =~ #&gt; Classics 0.942 0.129 7.314 0.000 0.942 0.956 #&gt; French 0.857 0.137 6.239 0.000 0.857 0.871 #&gt; English 0.795 0.143 5.545 0.000 0.795 0.807 #&gt; Math 0.732 0.149 4.923 0.000 0.732 0.743 #&gt; Pitch 0.678 0.153 4.438 0.000 0.678 0.689 #&gt; Music 0.643 0.155 4.142 0.000 0.643 0.653 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all #&gt; .Classics 0.083 0.051 1.629 0.103 0.083 0.086 #&gt; .French 0.234 0.072 3.244 0.001 0.234 0.242 #&gt; .English 0.338 0.094 3.610 0.000 0.338 0.349 #&gt; .Math 0.434 0.115 3.773 0.000 0.434 0.447 #&gt; .Pitch 0.510 0.132 3.855 0.000 0.510 0.526 #&gt; .Music 0.556 0.143 3.893 0.000 0.556 0.573 #&gt; g 1.000 1.000 1.000 È possibile semplificare l’output dalla funzione summary() in maniera tale da stampare solo la tabella completa delle stime dei parametri e degli errori standard, ecc. kable(coef(fit1), booktabs = TRUE, format = &quot;markdown&quot;) x g=~Classics 0.94152885 g=~French 0.85748943 g=~English 0.79479800 g=~Math 0.73208069 g=~Pitch 0.67826478 g=~Music 0.64324900 Classics~~Classics 0.08322018 French~~French 0.23440834 English~~English 0.33799292 Math~~Math 0.43375436 Pitch~~Pitch 0.50965401 Music~~Music 0.55592713 Anziché stampare direttamente la tabella dei parametri, è meglio riformattarla con kable quando utilizziamo RMarkdown. Senza usare kable, l’output diventa: parameterEstimates(fit1, standardized = TRUE) #&gt; lhs op rhs est se z pvalue ci.lower ci.upper std.lv #&gt; 1 g =~ Classics 0.942 0.129 7.314 0.000 0.689 1.194 0.942 #&gt; 2 g =~ French 0.857 0.137 6.239 0.000 0.588 1.127 0.857 #&gt; 3 g =~ English 0.795 0.143 5.545 0.000 0.514 1.076 0.795 #&gt; 4 g =~ Math 0.732 0.149 4.923 0.000 0.441 1.024 0.732 #&gt; 5 g =~ Pitch 0.678 0.153 4.438 0.000 0.379 0.978 0.678 #&gt; 6 g =~ Music 0.643 0.155 4.142 0.000 0.339 0.948 0.643 #&gt; 7 Classics ~~ Classics 0.083 0.051 1.629 0.103 -0.017 0.183 0.083 #&gt; 8 French ~~ French 0.234 0.072 3.244 0.001 0.093 0.376 0.234 #&gt; 9 English ~~ English 0.338 0.094 3.610 0.000 0.154 0.522 0.338 #&gt; 10 Math ~~ Math 0.434 0.115 3.773 0.000 0.208 0.659 0.434 #&gt; 11 Pitch ~~ Pitch 0.510 0.132 3.855 0.000 0.251 0.769 0.510 #&gt; 12 Music ~~ Music 0.556 0.143 3.893 0.000 0.276 0.836 0.556 #&gt; 13 g ~~ g 1.000 0.000 NA NA 1.000 1.000 1.000 #&gt; std.all std.nox #&gt; 1 0.956 0.956 #&gt; 2 0.871 0.871 #&gt; 3 0.807 0.807 #&gt; 4 0.743 0.743 #&gt; 5 0.689 0.689 #&gt; 6 0.653 0.653 #&gt; 7 0.086 0.086 #&gt; 8 0.242 0.242 #&gt; 9 0.349 0.349 #&gt; 10 0.447 0.447 #&gt; 11 0.526 0.526 #&gt; 12 0.573 0.573 #&gt; 13 1.000 1.000 Se invece usiamo kable, con gli opportuni parametri, otteniamo: parameterEstimates(fit1, standardized = TRUE) %&gt;% dplyr::filter(op == &quot;=~&quot;) %&gt;% dplyr::select( &quot;Latent Factor&quot; = lhs, Indicator = rhs, B = est, SE = se, Z = z, &quot;p-value&quot; = pvalue, Beta = std.all ) %&gt;% knitr::kable( digits = 3, booktabs = TRUE, format = &quot;markdown&quot;, caption = &quot;Factor Loadings&quot; ) TABELLA 13.1: Factor Loadings Latent Factor Indicator B SE Z p-value Beta g Classics 0.942 0.129 7.314 0 0.956 g French 0.857 0.137 6.239 0 0.871 g English 0.795 0.143 5.545 0 0.807 g Math 0.732 0.149 4.923 0 0.743 g Pitch 0.678 0.153 4.438 0 0.689 g Music 0.643 0.155 4.142 0 0.653 Esaminiamo la matrice delle correlazioni residue: cor_table &lt;- residuals(fit1, type = &quot;cor&quot;)$cov knitr::kable( cor_table, digits = 3, format = &quot;markdown&quot;, booktabs = TRUE ) Classics French English Math Pitch Music Classics 0.000 -0.003 0.008 -0.011 0.001 0.005 French -0.003 0.000 -0.033 0.023 0.050 0.001 English 0.008 -0.033 0.000 0.040 -0.016 -0.017 Math -0.011 0.023 0.040 0.000 -0.062 0.024 Pitch 0.001 0.050 -0.016 -0.062 0.000 -0.050 Music 0.005 0.001 -0.017 0.024 -0.050 0.000 Creiamo un qq-plot dei residui: res1 &lt;- residuals(fit1, type = &quot;cor&quot;)$cov res1[upper.tri(res1, diag = TRUE)] &lt;- NA v1 &lt;- as.vector(res1) v2 &lt;- v1[!is.na(v1)] tibble(v2) %&gt;% ggplot(aes(sample = v2)) + stat_qq() + stat_qq_line() Il pacchetto semPlot consente di disegnare diagrammi di percorso per vari modelli SEM. La funzione semPaths prende in input un oggetto creato da lavaan e disegna il diagramma, con diverse opzioni disponibili. Il diagramma qui prodotto controlla le dimensioni dei caratteri/etichette, la visualizzazione dei residui e il colore dei percorsi/coefficienti. Sono disponibili queste e molte altre opzioni di controllo. semPaths( fit1, residuals = FALSE, sizeMan = 7, &quot;std&quot;, posCol = c(&quot;black&quot;), edge.label.cex = 1.2, layout = &quot;circle2&quot; ) In una versione alternativa del diagramma di percorso aggiungiamo anche le specificità: semPaths( fit1, &quot;std&quot;, posCol = c(&quot;black&quot;), edge.label.cex = 1.2, sizeMan = 7 ) Il calcolo delle saturazioni fattoriali con il metodo del centroide aveva prodotto il risultato: \\(\\boldsymbol{\\hat{\\Lambda}}&#39;= (0.97, 0.84, 0.73, 0.65)\\). Si noti la somiglianza con i valori ottenuti mediante il metodo di massima verosimiglianza. References "],["conclusioni-2.html", "Conclusioni", " Conclusioni Nel presente capitolo abbiamo introdotto il metodo dell’annullamento della tetrade che consente di stimare le saturazioni di un modello monofattoriale. Abbiamo anche visto che il metodo dell’annullamento della tetrade non è altro che un’applicazione della correlazione parziale. Possiamo dire che un tema cruciale nella costruzione dei test psicologici è quello di stabilire il numero di fattori/tratti che sono soggiacenti all’insieme degli indicatori che vengono considerati. La teoria classica dei test richiede che il test sia monofattoriale, ovvero che gli indicatori considerati siano l’espressione di un unico tratto latente. La violazione della monodimensionalità rende problematica l’applicazione dei principi della teoria classica dei test ai punteggi di un test che non possiede tale proprietà. L’esame della dimensionalità di un gruppo di indicatori rappresenta dunque una fase cruciale nel processo di costruzione di un test e, solitamente, questo esame è affrontato mediante l’analisi fattoriale. In questo capitolo abbiamo presentato le proprietà di base del modello unifattoriale. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
