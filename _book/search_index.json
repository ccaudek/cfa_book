[["ch:rotazione.html", "Capitolo 2 La rotazione fattoriale", " Capitolo 2 La rotazione fattoriale Nel capitolo ?? abbiamo visto come sia possibile ottenere la soluzione fattoriale non ruotata per il numero di fattori comuni che meglio riassume l’informazione contenuta nella matrice di correlazioni (o covarianze). La soluzione non ruotata non garantisce l’identificazione di aggregati omogenei e interpretabili di variabili osservate. Si tende dunque a ricorrere alla rotazione degli assi fattoriali nella ricerca di una soluzione più facilmente interpretabile di quella ottenuta in prima istanza. "],["indeterminatezza-della-soluzione-fattoriale.html", "2.1 Indeterminatezza della soluzione fattoriale", " 2.1 Indeterminatezza della soluzione fattoriale Il problema della rotazione si pone perché la matrice delle saturazioni non presenta un’unica soluzione e, attraverso la sua trasformazione matematica, si possono ottenere infinite matrici dello stesso ordine. Tale fatto va sotto il nome di indeterminatezza della soluzione fattoriale. La matrice delle saturazioni fattoriali \\(\\boldsymbol{\\Lambda}\\) non risulta univocamente definita in quanto non esiste una soluzione unica alla determinazione delle saturazioni fattoriali. Una matrice di correlazioni \\(\\boldsymbol{R}\\) consente di determinare soluzioni fattoriali diverse, ovvero matrici aventi lo stesso numero di fattori comuni ma una diversa configurazione di saturazioni fattoriali, oppure matrici di saturazioni fattoriali corrispondenti ad un diverso numero di fattori comuni. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe e colonne, ma contenenti saturazioni fattoriali diverse. \\(\\boldsymbol{\\Lambda}_1\\) è definita dai valori seguenti l1 &lt;- matrix(c( 0.766, -0.232, 0.670, -0.203, 0.574, -0.174, 0.454, 0.533, 0.389, 0.457, 0.324, 0.381 ), byrow = TRUE, ncol = 2 ) mentre per \\(\\boldsymbol{\\Lambda}_2\\) abbiamo l2 &lt;- matrix(c( 0.783, 0.163, 0.685, 0.143, 0.587, 0.123, 0.143, 0.685, 0.123, 0.587, 0.102, 0.489 ), byrow = TRUE, ncol = 2 ) Esaminiamo la matrice delle correlazioni riprodotte dalle due matrici di pesi fattoriali (con le comunalità sulla diagonale di \\(\\boldsymbol{R}\\)): l1 %*% t(l1) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.640580 0.560316 0.480052 0.224108 0.191950 0.159792 ## [2,] 0.560316 0.490109 0.419902 0.195981 0.167859 0.139737 ## [3,] 0.480052 0.419902 0.359752 0.167854 0.143768 0.119682 ## [4,] 0.224108 0.195981 0.167854 0.490205 0.420187 0.350169 ## [5,] 0.191950 0.167859 0.143768 0.420187 0.360170 0.300153 ## [6,] 0.159792 0.139737 0.119682 0.350169 0.300153 0.250137 l2 %*% t(l2) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.639658 0.559664 0.479670 0.223624 0.191990 0.159573 ## [2,] 0.559664 0.489674 0.419684 0.195910 0.168196 0.139797 ## [3,] 0.479670 0.419684 0.359698 0.168196 0.144402 0.120021 ## [4,] 0.223624 0.195910 0.168196 0.489674 0.419684 0.349551 ## [5,] 0.191990 0.168196 0.144402 0.419684 0.359698 0.299589 ## [6,] 0.159573 0.139797 0.120021 0.349551 0.299589 0.249525 Come si vede, viene ottenuto lo stesso risultato utilizzando matrici \\(\\boldsymbol{\\Lambda}\\) con lo stesso numero \\(m\\) di colonne ma saturazioni fattoriali diverse. Si consideri ora il caso di matrici \\(\\boldsymbol{\\Lambda}\\) corrispondenti a soluzioni fattoriali con un diverso numero di fattori comuni. Siano \\(\\boldsymbol{\\Lambda}_1\\) e \\(\\boldsymbol{\\Lambda}_2\\) due matrici aventi lo stesso numero di righe ma un numero diverso di colonne: l1 &lt;- matrix(c( 0.9, 0.7, 0.5, 0.3 ), byrow = TRUE, ncol = 1 ) l2 &lt;- matrix(c( 0.78, 0.45, 0.61, 0.35, 0.43, 0.25, 0.25, 0.15 ), byrow = TRUE, ncol = 2 ) Si noti che la stessa matrice di correlazioni riprodotte (con le comunalità sulla diagonale principale) viene generata dalle saturazioni fattoriali corrispondenti ad un numero diverso di fattori comuni: l1 %*% t(l1) ## [,1] [,2] [,3] [,4] ## [1,] 0.81 0.63 0.45 0.27 ## [2,] 0.63 0.49 0.35 0.21 ## [3,] 0.45 0.35 0.25 0.15 ## [4,] 0.27 0.21 0.15 0.09 l2 %*% t(l2) ## [,1] [,2] [,3] [,4] ## [1,] 0.8109 0.6333 0.4479 0.2625 ## [2,] 0.6333 0.4946 0.3498 0.2050 ## [3,] 0.4479 0.3498 0.2474 0.1450 ## [4,] 0.2625 0.2050 0.1450 0.0850 "],["parsimonia-e-semplicità.html", "2.2 Parsimonia e semplicità", " 2.2 Parsimonia e semplicità Come si raggiunge allora una qualche certezza sui risultati dell’analisi fattoriale? Il problema dell’indeterminazione fattoriale si affronta scegliendo la soluzione che soddisfa i seguenti due criteri: criterio della parsimonia: se sia un modello ad un fattore comune sia un modello a due fattori comuni possono spiegare la covariazione tra le variabili si deve accettare quello ad un fattore; criterio della semplicità: a parità di numero di fattori, sono da preferire le strutture più semplici della matrice \\(\\boldsymbol{\\Lambda}\\) (Thurstone, 1947). Il criterio della parsimonia è facilmente applicabile: se due soluzioni fattoriali aventi un numero diverso di fattori riproducono allo stesso modo la matrice S o R, si sceglie la soluzione con il numero minore di fattori. D’altra parte, se vi sono diverse soluzioni fattoriali con lo stesso numero \\(m\\) di fattori, il criterio della semplicità ci guida nella scelta della trasformazione più appropriata della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\). La trasformazione della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) va sotto il nome di rotazione. A seconda che i fattori ruotati risultino o meno incorrelati, si distingue tra metodi di rotazione ortogonale o obliqua dei fattori. 2.2.1 Il criterio della “struttura semplice” Tramite la rotazione degli assi fattoriali miriamo alla “struttura semplice” della matrice delle saturazioni fattoriali: poche ma forti saturazioni diverse da zero e assenza di variabili saturate da più di un fattore. Il criterio della “struttura semplice” è stato originariamente proposto da Thurstone (1947) secondo il quale tale criterio viene raggiunto quando: nella matrice fattoriale ruotata, ogni variabile deve avere almeno un peso nullo; ogni fattore deve avere almeno \\(m\\) saturazioni nulle (\\(m\\): numero dei fattori comuni); per ciascuna coppia di fattori vi devono essere saturazioni basse su un fattore e saturazioni alte sull’altro; nel caso di molti fattori, per ciascuna coppia di fattori una grande proporzione di saturazioni dovrebbe essere nulla; per ciascuna coppia di fattori, vi dovrebbero essere solo poche saturazioni di entità non trascurabile su entrambi i fattori. Nella pratica, il requisito della struttura semplice viene perseguito, non tanto seguendo le indicazioni di Thursone, quanto bensì cercando di massimizzare il numero di saturazioni nulle o quasi nulle nella matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\). Uno dei grandi vantaggi che derivano dall’ottenimento della struttura semplice è la facilitazione nell’interpretazione dei fattori (Cattell, 1978). L’esame delle saturazioni fattoriali contenute nella matrice \\(\\hat{\\boldsymbol{\\Lambda}}^*\\) ruotata consente infatti di fornire un’interpretazione ai fattori. Per poter interpretare un fattore, dobbiamo chiederci quali sono le variabili che risultano maggiormente associate con tale fattore e quanto forti siano tali legami. Se i coefficienti di impatto di un fattore sono positivi e piuttosto elevati su un sottoinsieme di variabili osservate, da ciò deduciamo che il fattore rappresenta ciò che hanno in comune le variabili che saturano sul fattore. Ovviamente, l’interpretazione si complica nel caso di variabili che saturano su più fattori. "],["rotazione-nello-spazio-geometrico.html", "2.3 Rotazione nello spazio geometrico", " 2.3 Rotazione nello spazio geometrico 2.3.1 Rotazione ortogonale Come è stato notato nella sezione precedente, la matrice \\(\\boldsymbol{\\Lambda}\\) non è identificabile poiché non esiste una soluzione unica alla determinazione delle saturazioni fattoriali: qualunque matrice \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\), dove T è una matrice ortonormale di ordine \\(m\\), è in grado di riprodurre la matrice di varianze-covarianze allo stesso modo di \\(\\hat{\\boldsymbol{\\Lambda}}\\). La matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) è pertanto determinata a meno della moltiplicazione per una matrice ortonormale. Definizione 2.1 Geometricamente, i pesi fattoriali costituiscono le coordinate di un punto (ci sono tanti punti quante sono le \\(p\\) variabili manifeste) in uno spazio avente un numero di dimensioni pari al numero \\(m\\) dei fattori. Dal punto di vista geometrico, il problema dell’indeterminazione fattoriale si può descrivere facendo riferimento alla rotazione rigida dei punti che rappresentano le saturazioni fattoriali attorno l’origine del sistema di coordinate. Tale rotazione rigida lascia invariate le distanze tra i punti (ed è equivalente ad una rotazione (contraria) del sistema di assi cartesiani) e dà luogo ad un nuovo insieme di valori per i pesi fattoriali. Ciascuno di questi insiemi di pesi fattoriali così ottenuti produce la medesima matrice di correlazioni riprodotte dal modello fattoriale. L’indeterminazione fattoriale nasce dal fatto che sono possibili infinite rotazioni diverse degli assi. 2.3.2 Vincoli alla rotazione Il problema della non identificabilità di \\(\\hat{\\boldsymbol{\\Lambda}}\\) viene generalmente risolto imponendo dei vincoli alla rotazione. Il criterio che ci guida nella scelta di una delle possibili trasformazioni della matrice dei pesi fattoriali è quello della semplicità della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) (Thurstone, 1947), ovvero la vicinanza dei suoi elementi ai valori 0 e 1. Quanto più ciò si verifica tanto più risulta semplice l’interpretazione dei fattori comuni nei termini delle variabili. L’identificazione dei fattori risulta infatti semplificata se ciascuno di essi è fortemente correlato con un numero limitato di variabili ed è poco correlato con le altre. Le rotazioni ortogonali lasciano immutate le comunalità nel caso di fattori incorrelati. Questo accade perché qualunque rotazione rigida rispetto all’origine preserva le distanze tra i punti identificati dai pesi fattoriali e, nel caso di fattori incorrelati, la comunalità non è nient’altro che la distanza dall’origine (al quadrato): \\[\\hat{h}^2_i = \\sum_{i=1}^m \\hat{\\lambda}_{ij}^2\\notag\\] Rotazioni non ortogonali, però, mutano la quota di varianza spiegata da ciascun fattore, essendo questa data da \\[\\frac{\\sum_{i=1}^p \\hat{\\lambda}_{ij}^2}{\\text{tr}(\\textbf{S})}\\notag\\] oppure da \\[\\frac{\\sum_{i=1}^p \\hat{\\lambda}_{ij}^2}{\\text{tr}(\\textbf{R})}\\notag\\] laddove \\(\\text{tr}(\\textbf{R})=p\\), con \\(i=1, \\dots, p\\) (numero di item) e \\(j=1, \\dots, m\\) (numero di fattori). Diversi algoritmi sono stati proposti per la rotazione ortogonale dei fattori. Inizieremo ad esaminare una possibile soluzione al problema dell’indeterminazione fattoriale mediante il metodo grafico. Esamineremo poi i metodi Quartimax e Varimax. 2.3.3 Metodo grafico Come si può ruotare il sitema degli assi? Se ci sono solo \\(m=2\\) fattori, per ottenere la loro rappresentazione geometrica utilizziamo un sistema di coordinate bidimensionale. L’ispezione visiva del diagramma delle saturazioni fattoriali ci può guidare alla scelta della rotazione più appropriata. Le righe di \\(\\hat{\\boldsymbol{\\Lambda}}\\) corrispondono a coppie di pesi fattoriali (\\(\\hat{\\lambda}_{i1}, \\hat{\\lambda}_{i2}\\), con \\(i=1, \\dots, p\\)) che possono essere interpretate come le coordinate di \\(p\\) punti (tanti quanti le variabili manifeste). Gli assi del diagramma vengono ruotati di un angolo \\(\\phi\\) in modo tale da portarli il più vicino possibile ai punti presenti nel grafico. Le nuove coordinate (\\(\\hat{\\lambda}_{i1}^*, \\hat{\\lambda}_{i2}^*\\)) vengono calcolate come \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\), dove \\[\\textbf{T} = \\left[ \\begin{array}{ c c } \\cos{\\phi} &amp; - \\sin{\\phi}\\\\ \\sin{\\phi} &amp; \\cos{\\phi} \\end{array} \\right] \\notag \\] è una matrice ortogonale \\(2 \\times 2\\). Si considerino i dati di Brown, Williams e Barlow (1984) discussi da Rencher (2002). Ad una bambina di dodici anni è stato chiesto di valutare sette dei suoi conoscenti su cinque variabili: kind, intelligent, happy, likeable e just. Per queste cinque variabili, la matrice di correlazioni è R &lt;- matrix(c( 1.00, .296, .881, .995, .545, .296, 1.000, -.022, .326, .837, .881, -.022, 1.000, .867, .130, .995, .326, .867, 1.000, .544, .545, .837, .130, .544, 1.00 ), ncol = 5, byrow = TRUE, dimnames = list( c(&quot;K&quot;, &quot;I&quot;, &quot;H&quot;, &quot;L&quot;, &quot;J&quot;), c(&quot;K&quot;, &quot;I&quot;, &quot;H&quot;, &quot;L&quot;, &quot;J&quot;) ) ) Dalla matrice R estraiamo due fattori con il metodo delle componenti principali: library(&quot;psych&quot;) f.pc &lt;- principal(R, 2, rotate = FALSE) ## Specified rotation not found, rotate=&#39;none&#39; used f.pc ## Principal Components Analysis ## Call: principal(r = R, nfactors = 2, rotate = FALSE) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## K 0.97 -0.23 0.99 0.0067 1.1 ## I 0.52 0.81 0.92 0.0792 1.7 ## H 0.78 -0.59 0.96 0.0391 1.9 ## L 0.97 -0.21 0.99 0.0135 1.1 ## J 0.70 0.67 0.94 0.0597 2.0 ## ## PC1 PC2 ## SS loadings 3.26 1.54 ## Proportion Var 0.65 0.31 ## Cumulative Var 0.65 0.96 ## Proportion Explained 0.68 0.32 ## Cumulative Proportion 0.68 1.00 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.03 ## ## Fit based upon off diagonal values = 1 Nella figura 1.1, i punti rappresentano le cinque coppie di pesi fattoriali non ruotati. plot( f.pc$load[,1], f.pc$load[,2], bty = &#39;n&#39;, xaxt = &#39;n&#39;, xlab = &quot;Primo Fattore&quot;, ylab = &quot;Secondo Fattore&quot;, ylim = c(-.6, 1), xlim = c(0,1), pch = 19) axis(1, pos = c(0,0)) abline(0, 0) Rencher (2002) nota che, per questi dati, una rotazione ortogonale di \\(-35^{\\circ}\\) ci porterebbe ad avvicinare gli assi ai punti nel diagramma. Per verificare questo, disegnamo sul diagramma i nuovi assi dopo una rotazione di \\(-35^{\\circ}\\). Le istruzioni R sono le seguenti: ar &lt;- matrix(c( 0, 0, 0, 1, 0, 0, 1, 0 ), ncol = 2, byrow = TRUE) ar ## [,1] [,2] ## [1,] 0 0 ## [2,] 0 1 ## [3,] 0 0 ## [4,] 1 0 angle &lt;- 35 rad &lt;- angle * pi / 180 T &lt;- matrix(c( cos(rad), -sin(rad), sin(rad), cos(rad)), ncol = 2, byrow = TRUE) round(T, 3) ## [,1] [,2] ## [1,] 0.819 -0.574 ## [2,] 0.574 0.819 #&gt; [,1] [,2] #&gt; [1,] 0.819 -0.574 #&gt; [2,] 0.574 0.819 round(ar %*% T, 3) #&gt; [,1] [,2] #&gt; [1,] 0.000 0.000 #&gt; [2,] 0.574 0.819 #&gt; [3,] 0.000 0.000 #&gt; [4,] 0.819 -0.574 arrows(0, 0, 0.574, 0.819, lwd = 2) arrows(0, 0, 0.819, -0.574, lwd = 2) Nella figura 1.1 le due frecce rappresentano gli assi ruotati. È chiaro come tale rotazione di \\(-35^{\\circ}\\) ha effettivamente l’effetto di avvicinare gli assi ai punti del diagramma. Se usiamo dunque il valore \\(\\phi = -35^{\\circ}\\) nella matrice di rotazione, possiamo calcolare le saturazioni fattoriali della soluzione ruotata \\(\\hat{\\boldsymbol{\\Lambda}}^* = \\hat{\\boldsymbol{\\Lambda}} \\textbf{T}\\). Le saturazioni fattoriali ruotate non sono altro che la proiezione ortogonale dei punti sugli assi ruotati. angle &lt;- -35 rad &lt;- angle * pi / 180 T &lt;- matrix(c( cos(rad), -sin(rad), sin(rad), cos(rad)), ncol = 2, byrow = TRUE) round(T, 3) #&gt; [,1] [,2] #&gt; [1,] 0.819 0.574 #&gt; [2,] -0.574 0.819 round(f.pc$load %*% T, 3) #&gt; [,1] [,2] #&gt; K 0.927 0.368 #&gt; I -0.039 0.962 #&gt; H 0.977 -0.036 #&gt; L 0.915 0.384 #&gt; J 0.189 0.950 La soluzione ottenuta in questo modo riproduce quella riportata da Rencher (2002). Rotazione di -35\\(^{\\circ}\\) per le saturazioni fattoriali calcolate sui dati di Brown, Williams e Barlow (1984). 2.3.4 Medodi di rotazione ortogonale Una tipo di rotazione ortogonale molto utilizzato è la rotazione Varimax (Kaiser, 1958). La matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) è semplificata in modo tale che le varianze dei quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a colonne diverse di \\(\\hat{\\boldsymbol{\\Lambda}}\\) siano massime. Se le saturazioni fattoriali in una colonna di \\(\\hat{\\boldsymbol{\\Lambda}}\\) sono simili tra loro, la varianza sarà prossima a zero. Tale varianza è tanto più grande quanto più i quadrati degli elementi \\(\\lambda_{ij}\\) assumono valori prossimi a \\(0\\) e \\(1\\). Amplificando le correlazioni più alte e riducendo quelle più basse, la rotazione Varimax agevola l’interpretazione di ciascun fattore. Usando la funzione factanal() del modulo base, la rotazione Varimax può essere applicata alla soluzione ottenuta mediante il metodo di massima verosimiglianza. Usando le funzioni principal() e factor.pa() disponibili nel pacchetto psych, la rotazione Varimax può essere applicata alle soluzioni ottenute mediante il metodo delle componenti principali e il metodo del fattore principale. La figura 1.2 mostra i risultati della rotazione Varimax per la soluzione ottenuta con il metodo delle componenti principali sui dati di Brown et al. (1994): f_pc &lt;- principal(R, 2, n.obs = 7, rotate = &quot;varimax&quot;) f_pc #&gt; RC1 RC2 h2 u2 com #&gt; K 0.95 0.30 0.99 0.0067 1.2 #&gt; I 0.03 0.96 0.92 0.0792 1.0 #&gt; H 0.97 -0.10 0.96 0.0391 1.0 #&gt; L 0.94 0.32 0.99 0.0135 1.2 #&gt; J 0.26 0.93 0.94 0.0597 1.2 Saturazioni fattoriali calcolate sui dati di Brown, Williams e Barlow (1984). Il metodo Quartimax (Neuhaus e Wringley, 1954) opera una semplificazione della matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) massimizzando le covarianze tra i quadrati degli elementi \\(\\lambda_{ij}\\) appartenenti a righe diverse, subordinatamente alla condizione che la varianza delle righe rimanga inalterata. 2.3.5 Metodi di rotazione obliqua Parlare di rotazione obliqua significa usare un termine improprio: per definizione, infatti, una rotazione implica una trasformazione ortogonale che preserva le distanze. Secondo Rencher (2002), un termine migliore sarebbe trasformazione obliqua. Il termine rotazione obliqua, comunque, fa parte dell’uso corrente. Nella rotazione obliqua, gli assi della soluzione ruotata non devono rimanere ortogonali e quindi possono più facilmente avvicinarsi ai raggruppamenti di punti nello spazio delle saturazioni fattoriali (assumendo che dei raggruppamenti esistano). Vari metodi analitici sono stati proposti per ottenere una rotazione obliqua. Qui esamineremo brevemente solo uno di essi, il metodo Direct Oblimin. Il criterio usato nel metodo Direct Oblimin (Jennrich e Sampson, 1966) è il seguente: \\[\\sum_{ij} \\left(\\sum_v \\lambda_i^2 \\lambda_j^2 - w \\frac{1}{p} \\sum_v \\lambda_i^2 \\sum_v \\lambda_j^2\\right)\\notag\\] dove \\(\\sum_{ij}\\) si riferisce alla somma su tutte le coppie di fattori \\(ij\\). In questo caso si procede ad una minimizzazione piuttosto che a una masssimizzazione. Con le istruzioni seguenti vengono simulate 100 osservazioni su quattro variabili nel caso di una normale multivarata con le medie e la matrice \\(\\boldsymbol{\\Sigma}\\) specificata qui sotto: library(GPArotation) library(MASS) library(psych) sigma &lt;- matrix( c( 1, .8, .6, .7, .8, 1, .6, .7, .6, .6, 1, .9, .7, .7, .9, 1 ), ncol = 4, byrow = TRUE ) sigma #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1.0 0.8 0.6 0.7 #&gt; [2,] 0.8 1.0 0.6 0.7 #&gt; [3,] 0.6 0.6 1.0 0.9 #&gt; [4,] 0.7 0.7 0.9 1.0 mu &lt;- c(9, 16, 24, 32) set.seed(24) X &lt;- mvrnorm( n = 100, mu, sigma, empirical = FALSE ) R &lt;- cor(X) print(R, 3) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1.000 0.755 0.567 0.619 #&gt; [2,] 0.755 1.000 0.597 0.667 #&gt; [3,] 0.567 0.597 1.000 0.895 #&gt; [4,] 0.619 0.667 0.895 1.000 Eseguiamo l’analisi fattoriale senza rotazione: pr_none &lt;- principal(R, 2, rotate = &quot;none&quot;) pr_none #&gt; V PC1 PC2 #&gt; 1 0.84 0.44 #&gt; 2 0.86 0.36 #&gt; 3 0.88 -0.42 #&gt; 4 0.92 -0.33 Per usare la rotazione Oblimin (così come molte altre) è necessario caricare il pacchetto GPArotation. Le saturazioni fattoriali calcolate con il metodo delle componenti principali e la rotazione Oblimin si ottengono nel modo seguente: pr_oblimin &lt;- principal(R, 2, rotate = &quot;oblimin&quot;) pr_oblimin #&gt; TC2 TC1 #&gt; 0.96 -0.03 #&gt; 0.88 0.07 #&gt; -0.08 1.03 #&gt; 0.21 0.82 Tramite l’algoritmo Oblimin, gli assi vengono ruotati fino a formare l’angolo che separa le due frecce nella figura 1.3. Dall’output di R ricaviamo l’informazione secondo cui il coseno di tale angolo è 0.63. #&gt; With component correlations of #&gt; TC2 TC1 #&gt; TC2 1.00 0.63 #&gt; TC1 0.63 1.00 Le istruzioni usate per disegnare il grafico 1.3 sono le seguenti: plot( pr_none$load[, 1], pr_none$load[, 2], bty = &quot;n&quot;, xlab = &quot;Primo Fattore&quot;, ylab = &quot;Secondo Fattore&quot;, xlim = c(0, 1), ylim = c(-1, 1), pch = 19, cex.lab = 2 ) Per disegnare le frecce rappresentate nella figura (che rappresentano i nuovi assi coordinati) procediamo come segue. Si noti che le saturazioni fattoriali sono raggruppate in due cluster distinti. Disegniamo innanzitutto una freccia che rappresenta uno degli assi in modo tale che si avvicini il più possibile a uno dei cluster. arrows(0, 0, 1, 0.48, lwd = 2) Per disegnare la freccia che rappresenta il secondo asse facciamo uso della correlazione tra i due fattori (\\(\\phi\\) = 0.63). Tale correlazione corrisponde al coseno dell’angolo che separa i due assi. cos.phi &lt;- pr.oblimin$Phi[1, 2] sin.phi &lt;- sqrt(1 - cos.phi^2) La matrice di trasformazione per ruotare un vettore di un angolo prefissato \\(\\phi\\) è T &lt;- matrix(c( cos.phi, -sin.phi, sin.phi, cos.phi ), byrow = TRUE, ncol = 2) Mediante il prodotto tra il vettore che rappresenta l’apice del primo vettore e la matrice di trasformazione troviamo le coordinate del vettore ruotato dell’angolo \\(\\phi\\). Per disegnare la freccia che rappresenta il secondo asse usiamo dunque le seguenti istruzioni: P &lt;- c(1, .48) P1 &lt;- P %*% T arrows(0, 0, P1[1], P1[2], lwd = 2) Rotazione obliqua. "],["matrice-dei-pesi-fattoriali-e-matrice-di-struttura.html", "2.4 Matrice dei pesi fattoriali e matrice di struttura", " 2.4 Matrice dei pesi fattoriali e matrice di struttura Nella rotazione ortogonale i fattori sono incorrelati. In tali circostanze, le correlazioni tra le variabili e i fattori sono uguali alle saturazioni fattoriali. In un path diagram, infatti, vi è un unico percorso legittimo (in base alle regole di Wright) che collega le variabili manifeste ai fattori. Si consideri la situazione presentata nella figura \\[fig:2fact_ort\\], con due variabili latenti incorrelate (\\(\\xi_1\\) e \\(\\xi_2\\)) e quattro variabili manifeste (\\(y_1\\), \\(y_2\\), \\(y_3\\), \\(y_4\\)). Siano \\(\\lambda_{11}\\), \\(\\lambda_{12}\\), \\(\\lambda_{13}\\) e \\(\\lambda_{14}\\) le saturazioni fattoriali delle variabili nel primo fattore; siano \\(\\lambda_{21}\\), \\(\\lambda_{22}\\), \\(\\lambda_{23}\\) e \\(\\lambda_{24}\\) le saturazioni fattoriali delle variabili nel secondo fattore. In un diagramma di percorso, la correlazione tra due variabili contenute è uguale alla somma dei valori numerici di tutti i percorsi legittimi che collegano le variabili. Se i fattori comuni sono incorrelati (come nella figura \\[fig:2fact_ort\\]), allora c’è un unico percorso che collega ciascuna variabile manifesta a ciascun fattore comune. Le correlazioni tra variabili manifeste e fattori comuni sono dunque uguali ai pesi fattoriali. In queste circostanze, la soluzione fattoriale contenuta nella matrice delle saturazioni fattoriali rappresenta le correlazioni fra variabili e fattori. Tale matrice viene detta matrice “di struttura.” Le saturazioni possono essere interpretate in maniera equivalente ai pesi beta del modello di regressione multipla, i quali stimano il contributo specifico di ciascun fattore comune nel determinare la varianza spiegata degli item (Tabachnick &amp; Fidell, 2001). Invece, nel caso della rotazione obliqua, la soluzione fattoriale ruotata produce un insieme di fattori comuni fra loro correlati. Di conseguenza, i pesi contenuti nella matrice delle saturazioni fattoriali non rappresentano le correlazioni fra variabili e fattori. Nel caso di una rotazione obliqua è perciò necessario specificare tre matrici diverse: la matrice dei pesi fattoriali \\(\\hat{\\boldsymbol{\\Lambda}}\\), detta matrice pattern (factor pattern matrix, o “configurazione,” o “matrice dei modelli”), la matrice di struttura (factor structure matrix), che è la matrice delle correlazioni tra variabili manifeste e fattori, la matrice di intercorrelazione fattoriale \\(\\hat{\\boldsymbol{\\Phi}}\\) (factor intercorrelation matrix), che è la matrice che esprime le correlazioni tra i fattori. La matrice pattern rappresenta i coefficienti parziali di regressione della variabile sul fattore, al netto degli altri fattori. Nel caso della rotazione obliqua, è la matrice che viene usata per determinare in che grado viene raggiunta la “struttura semplice”. Esaminiamo ora più in dettaglio la soluzione fattoriale prodotta da una rotazione obliqua. Gli assi che rappresentano i fattori non sono ortogonali (ovvero, i fattori sono correlati) e, in un diagramma di percorso, le variabili manifeste sono collegate ai fattori attraverso due percorsi distinti. Tali percorsi rappresentano l’effetto “diretto” e “indiretto” dei fattori sulle variabili. Nel caso di una rotazione obliqua, come abbiamo detto sopra, le saturazioni fattoriali non coincidono con le correlazioni tra variabili e fattori. Si consideri la figura \\[fig:2fact_obli\\]. Nel caso di una rotazione obliqua, la correlazione tra i due fattori comuni viene rappresentata mediante la freccia non direzionata \\(\\phi_{12}\\) che collega \\(\\xi_1\\) e \\(\\xi_2\\). In tali circostanze, ci sono due percorsi legittimi (in base alle regole di Wright) che consentono di collegare ciascuna variabile manifesta ad un fattore comune. Nel caso della variabile \\(y_1\\) e il fattore \\(\\xi_1\\), ad esempio, i percorsi sono: la freccia causale \\(\\lambda_{11}\\) che rappresenta l’effetto diretto di \\(\\xi_1\\) su \\(y_1\\) e il percorso composto che rappresenta l’effetto indiretto di \\(\\xi_1\\) su \\(y_1\\). Il valore numerico di tale percorso composto è uguale al prodotto \\(\\lambda_{21}\\phi_{12}\\). Nei termini dell’analisi dei percorsi, dunque, la correlazione tra \\(\\xi_1\\) e \\(y_1\\) è uguale alla somma dei valori numerici dei percorsi legittimi che collegano \\(y_1\\) a \\(\\xi_1\\), ovvero \\(\\lambda_{11} + \\lambda_{21} \\phi_{12}\\). Per illustrare la rotazione obliqua, utilizziamo i dati presentati da Rencher (2002). Si consideri la seguente matrice di correlazione: R &lt;- matrix( c( 1.00, 0.735, 0.711, 0.704, 0.735, 1.00, 0.693, 0.709, 0.711, 0.693, 1.00, 0.839, 0.704, 0.709, 0.839, 1.00 ), ncol = 4, byrow = TRUE ) R #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 1.000 0.735 0.711 0.704 #&gt; [2,] 0.735 1.000 0.693 0.709 #&gt; [3,] 0.711 0.693 1.000 0.839 #&gt; [4,] 0.704 0.709 0.839 1.000 Iniziamo a calcolare una soluzione a due fattori usando il metodo delle componenti principali e una rotazione Varimax. I pesi fattoriali sono: f1_pc &lt;- principal(R, 2, rotate = &quot;varimax&quot;) f1_pc #&gt; PC1 PC2 #&gt; [1,] 0.50 0.78 #&gt; [2,] 0.47 0.81 #&gt; [3,] 0.90 0.33 #&gt; [4,] 0.89 0.35 Si noti che i due fattori non sono molto distinti. Se usiamo invece l’algoritmo Oblimin, gli assi vengono ruotati fino a formare l’angolo che separa le due frecce nella figura 1.4. La soluzione prodotta da una rotazione obliqua si ottiene nel modo seguente: pr_oblimin &lt;- principal(R, 2, rotate = &quot;oblimin&quot;) La matrice \\(\\hat{\\boldsymbol{\\Lambda}}\\) dei pesi fattoriali si ricava nel modo seguente: cbind(pr_oblimin$load[, 1], pr_oblimin$load[, 2]) #&gt; [,1] [,2] #&gt; [1,] -0.04 0.97 #&gt; [2,] 0.07 0.89 #&gt; [3,] 1.01 -0.05 #&gt; [4,] 0.92 0.08 Diagramma dei pesi fattoriali per una rotazione obliqua. La matrice \\(\\hat{\\boldsymbol{\\Phi}}\\) di intercorrelazione fattoriale è pr_oblimin$Phi #&gt; [,1] [,2] #&gt; [1,] 1.0000000 0.6564617 #&gt; [2,] 0.6564617 1.0000000 La matrice di struttura si ottiene premoltiplicando la matrice \\(\\boldsymbol{\\Lambda}\\) dei pesi fattoriali alla matrice \\(\\boldsymbol{\\Phi}\\) di intercorrelazione fattoriale: \\[\\text{matrice di struttura} = \\boldsymbol{\\Lambda}\\boldsymbol{\\Phi}.\\] Per esempio, la correlazione tra la prima variabile e il primo fattore è pr_oblimin$load[1, 1] + pr_oblimin$load[1, 2] * pr_oblimin$Phi[2, 1] #&gt; 0.5967679 L’intera matrice di struttura si trova nel modo seguente: round(pr_oblimin$load %*% pr_oblimin$Phi, 3) #&gt; [,1] [,2] #&gt; [1,] 0.597 0.944 #&gt; [2,] 0.654 0.936 #&gt; [3,] 0.977 0.613 #&gt; [4,] 0.973 0.684 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
