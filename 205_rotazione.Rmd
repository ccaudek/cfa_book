# La rotazione fattoriale 

```{r, include = FALSE}
source("_common.R")
library("lavaan")
```

Nel capitolo \@ref(ch:estrazione) abbiamo visto come sia possibile ottenere la
soluzione fattoriale non ruotata per il numero di fattori comuni che
meglio riassume l'informazione contenuta nella matrice di correlazioni
(o covarianze). La soluzione non ruotata non garantisce
l'identificazione di aggregati omogenei e interpretabili di variabili
osservate. Si tende dunque a ricorrere alla rotazione degli assi
fattoriali nella ricerca di una soluzione più facilmente interpretabile
di quella ottenuta in prima istanza.

## Indeterminatezza della soluzione fattoriale

Il problema della rotazione si pone perché la matrice delle saturazioni
non presenta un'unica soluzione e, attraverso la sua trasformazione
matematica, si possono ottenere infinite matrici dello stesso ordine.
Tale fatto va sotto il nome di *indeterminatezza della soluzione
fattoriale*.

La matrice delle saturazioni fattoriali $\boldsymbol{\Lambda}$ non
risulta univocamente definita in quanto non esiste una soluzione unica
alla determinazione delle saturazioni fattoriali. Una matrice di
correlazioni $\boldsymbol{R}$ consente di determinare soluzioni
fattoriali diverse, ovvero matrici aventi lo stesso numero di fattori
comuni ma una diversa configurazione di saturazioni fattoriali, oppure
matrici di saturazioni fattoriali corrispondenti ad un diverso numero di
fattori comuni.

::: exmp
Siano $\boldsymbol{\Lambda}_1$ e $\boldsymbol{\Lambda}_2$ due matrici
aventi lo stesso numero di righe e colonne, ma contenenti saturazioni
fattoriali diverse. $\boldsymbol{\Lambda}_1$ è definita dai valori
seguenti

```{r}
l1 <- matrix(c(
  0.766,  -0.232,
  0.670,  -0.203,
  0.574,  -0.174,
  0.454,   0.533,
  0.389,   0.457,
  0.324,   0.381
),
byrow = TRUE, ncol = 2
)
```

mentre per $\boldsymbol{\Lambda}_2$ abbiamo

```{r}
l2 <- matrix(c(
  0.783,  0.163,
  0.685,  0.143,
  0.587,  0.123,
  0.143,  0.685,
  0.123,  0.587,
  0.102,  0.489
),
byrow = TRUE, ncol = 2
)
```

Esaminiamo la matrice delle correlazioni riprodotte dalle due matrici di
pesi fattoriali (con le comunalità sulla diagonale di $\boldsymbol{R}$):

```{r}
l1 %*% t(l1)
l2 %*% t(l2)
```

Come si vede, viene ottenuto lo stesso risultato utilizzando matrici
$\boldsymbol{\Lambda}$ con lo stesso numero $m$ di colonne ma
saturazioni fattoriali diverse.

Si consideri ora il caso di matrici $\boldsymbol{\Lambda}$
corrispondenti a soluzioni fattoriali con un diverso numero di fattori
comuni. Siano $\boldsymbol{\Lambda}_1$ e $\boldsymbol{\Lambda}_2$ due
matrici aventi lo stesso numero di righe ma un numero diverso di
colonne:

```{r}
l1 <- matrix(c(
  0.9,
  0.7,
  0.5,
  0.3
),
byrow = TRUE, ncol = 1
)

l2 <- matrix(c(
  0.78, 0.45,
  0.61, 0.35,
  0.43, 0.25,
  0.25, 0.15
),
byrow = TRUE, ncol = 2
)
```

Si noti che la stessa matrice di correlazioni riprodotte (con le
comunalità sulla diagonale principale) viene generata dalle saturazioni
fattoriali corrispondenti ad un numero diverso di fattori comuni:

```{r}
l1 %*% t(l1)
```

```{r}
l2 %*% t(l2)
```
:::

## Parsimonia e semplicità

Come si raggiunge allora una qualche certezza sui risultati dell'analisi
fattoriale? Il problema dell'*indeterminazione fattoriale* si affronta
scegliendo la soluzione che soddisfa i seguenti due criteri: *criterio
della parsimonia*: se sia un modello ad un fattore comune sia un modello
a due fattori comuni possono spiegare la covariazione tra le variabili
si deve accettare quello ad un fattore; *criterio della semplicità*: a
parità di numero di fattori, sono da preferire le strutture più semplici
della matrice $\boldsymbol{\Lambda}$ (Thurstone, 1947).

Il criterio della parsimonia è facilmente applicabile: se due soluzioni
fattoriali aventi un numero diverso di fattori riproducono allo stesso
modo la matrice **S** o **R**, si sceglie la soluzione con il numero
minore di fattori. D'altra parte, se vi sono diverse soluzioni
fattoriali con lo stesso numero $m$ di fattori, il criterio della
semplicità ci guida nella scelta della trasformazione più appropriata
della matrice $\hat{\boldsymbol{\Lambda}}$. La trasformazione della
matrice $\hat{\boldsymbol{\Lambda}}$ va sotto il nome di *rotazione*. A
seconda che i fattori ruotati risultino o meno incorrelati, si distingue
tra metodi di rotazione ortogonale o obliqua dei fattori.

### Il criterio della "struttura semplice"

Tramite la rotazione degli assi fattoriali miriamo alla "struttura
semplice" della matrice delle saturazioni fattoriali: poche ma forti
saturazioni diverse da zero e assenza di variabili saturate da più di un
fattore. Il criterio della "struttura semplice" è stato originariamente
proposto da Thurstone (1947) secondo il quale tale criterio viene
raggiunto quando:

-   nella matrice fattoriale ruotata, ogni variabile deve avere almeno
    un peso nullo;
-   ogni fattore deve avere almeno $m$ saturazioni nulle ($m$: numero
    dei fattori comuni);
-   per ciascuna coppia di fattori vi devono essere saturazioni basse su
    un fattore e saturazioni alte sull'altro;
-   nel caso di molti fattori, per ciascuna coppia di fattori una grande
    proporzione di saturazioni dovrebbe essere nulla;
-   per ciascuna coppia di fattori, vi dovrebbero essere solo poche
    saturazioni di entità non trascurabile su entrambi i fattori.

Nella pratica, il requisito della struttura semplice viene perseguito,
non tanto seguendo le indicazioni di Thursone, quanto bensì cercando di
massimizzare il numero di saturazioni nulle o quasi nulle nella matrice
$\hat{\boldsymbol{\Lambda}}$. Uno dei grandi vantaggi che derivano
dall'ottenimento della struttura semplice è la facilitazione
nell'interpretazione dei fattori (Cattell, 1978).

L'esame delle saturazioni fattoriali contenute nella matrice
$\hat{\boldsymbol{\Lambda}}^*$ ruotata consente infatti di fornire
un'interpretazione ai fattori. Per poter interpretare un fattore,
dobbiamo chiederci quali sono le variabili che risultano maggiormente
associate con tale fattore e quanto forti siano tali legami. Se i
coefficienti di impatto di un fattore sono positivi e piuttosto elevati
su un sottoinsieme di variabili osservate, da ciò deduciamo che il
fattore rappresenta ciò che hanno in comune le variabili che saturano
sul fattore. Ovviamente, l'interpretazione si complica nel caso di
variabili che saturano su più fattori.

## Rotazione nello spazio geometrico

### Rotazione ortogonale

Come è stato notato nella sezione precedente, la matrice
$\boldsymbol{\Lambda}$ non è *identificabile* poiché non esiste una
soluzione unica alla determinazione delle saturazioni fattoriali:
qualunque matrice
$\hat{\boldsymbol{\Lambda}}^* = \hat{\boldsymbol{\Lambda}}
\textbf{T}$, dove **T** è una matrice ortonormale di ordine $m$, è in
grado di riprodurre la matrice di varianze-covarianze allo stesso modo
di $\hat{\boldsymbol{\Lambda}}$. La matrice $\hat{\boldsymbol{\Lambda}}$
è pertanto determinata a meno della moltiplicazione per una matrice
ortonormale.

::: {.definition}
Geometricamente, i pesi fattoriali costituiscono le coordinate di un
punto (ci sono tanti punti quante sono le $p$ variabili manifeste) in
uno spazio avente un numero di dimensioni pari al numero $m$ dei
fattori.
:::

Dal punto di vista geometrico, il problema dell'indeterminazione
fattoriale si può descrivere facendo riferimento alla rotazione rigida
dei punti che rappresentano le saturazioni fattoriali attorno l'origine
del sistema di coordinate. Tale rotazione rigida lascia invariate le
distanze tra i punti (ed è equivalente ad una rotazione (contraria) del
sistema di assi cartesiani) e dà luogo ad un nuovo insieme di valori per
i pesi fattoriali. Ciascuno di questi insiemi di pesi fattoriali così
ottenuti produce la medesima matrice di correlazioni riprodotte dal
modello fattoriale. L'indeterminazione fattoriale nasce dal fatto che
sono possibili infinite rotazioni diverse degli assi.

### Vincoli alla rotazione

Il problema della non identificabilità di $\hat{\boldsymbol{\Lambda}}$
viene generalmente risolto imponendo dei vincoli alla rotazione. Il
criterio che ci guida nella scelta di una delle possibili trasformazioni
della matrice dei pesi fattoriali è quello della *semplicità* della
matrice $\hat{\boldsymbol{\Lambda}}$ (Thurstone, 1947), ovvero la
vicinanza dei suoi elementi ai valori 0 e 1. Quanto più ciò si verifica
tanto più risulta semplice l'interpretazione dei fattori comuni nei
termini delle variabili. L'identificazione dei fattori risulta infatti
semplificata se ciascuno di essi è fortemente correlato con un numero
limitato di variabili ed è poco correlato con le altre.

Le rotazioni ortogonali lasciano immutate le comunalità nel caso di
fattori incorrelati. Questo accade perché qualunque rotazione rigida
rispetto all'origine preserva le distanze tra i punti identificati dai
pesi fattoriali e, nel caso di fattori incorrelati, la comunalità non è
nient'altro che la distanza dall'origine (al quadrato):

$$\hat{h}^2_i = \sum_{i=1}^m \hat{\lambda}_{ij}^2$$ 

Rotazioni non ortogonali, però, mutano la quota di varianza spiegata da ciascun
fattore, essendo questa data da

$$\frac{\sum_{i=1}^p \hat{\lambda}_{ij}^2}{\text{tr}(\textbf{S})}$$
oppure da

$$\frac{\sum_{i=1}^p \hat{\lambda}_{ij}^2}{\text{tr}(\textbf{R})}$$

laddove $\text{tr}(\textbf{R})=p$, con $i=1, \dots, p$ (numero di item)
e $j=1, \dots, m$ (numero di fattori).

Diversi algoritmi sono stati proposti per la rotazione ortogonale dei
fattori. Inizieremo ad esaminare una possibile soluzione al problema
dell'indeterminazione fattoriale mediante il metodo grafico. Esamineremo
poi i metodi Quartimax e Varimax.

### Metodo grafico

Come si può ruotare il sitema degli assi? Se ci sono solo $m=2$ fattori,
per ottenere la loro rappresentazione geometrica utilizziamo un sistema
di coordinate bidimensionale. L'ispezione visiva del diagramma delle
saturazioni fattoriali ci può guidare alla scelta della rotazione più
appropriata. Le righe di $\hat{\boldsymbol{\Lambda}}$ corrispondono a
coppie di pesi fattoriali ($\hat{\lambda}_{i1}, \hat{\lambda}_{i2}$, con
$i=1, \dots, p$) che possono essere interpretate come le coordinate di
$p$ punti (tanti quanti le variabili manifeste). Gli assi del diagramma
vengono ruotati di un angolo $\phi$ in modo tale da portarli il più
vicino possibile ai punti presenti nel grafico. Le nuove coordinate
($\hat{\lambda}_{i1}^*, \hat{\lambda}_{i2}^*$) vengono calcolate come
$\hat{\boldsymbol{\Lambda}}^* = \hat{\boldsymbol{\Lambda}} \textbf{T}$,
dove 

$$
\textbf{T} = 
\left[
  \begin{array}{ c c }
  \cos{\phi} & - \sin{\phi}\\
  \sin{\phi} & \cos{\phi}
  \end{array} 
\right] 
$$ 

è una matrice ortogonale $2 \times 2$.

::: exmp
Si considerino i dati di Brown, Williams e Barlow (1984) discussi da
Rencher (2002). Ad una bambina di dodici anni è stato chiesto di
valutare sette dei suoi conoscenti su cinque variabili: *kind*,
*intelligent*, *happy*, *likeable* e *just*. Per queste cinque
variabili, la matrice di correlazioni è

```{r}
R <- matrix(c(
  1.00, .296, .881, .995, .545,
  .296, 1.000, -.022, .326, .837,
  .881, -.022, 1.000, .867, .130,
  .995, .326, .867, 1.000, .544,
  .545, .837, .130, .544, 1.00
),
ncol = 5, byrow = TRUE, dimnames = list(
  c("K", "I", "H", "L", "J"), c("K", "I", "H", "L", "J")
)
)
```

Dalla matrice **R** estraiamo due fattori con il metodo delle componenti
principali:

```{r}
library("psych")
f.pc <- principal(R, 2, rotate = FALSE) 
f.pc
```

Nella seguente figura, i punti rappresentano le cinque coppie di pesi
fattoriali non ruotati:

```{r}
plot(
  f.pc$load[,1], f.pc$load[,2], bty = 'n', xaxt = 'n', 
  xlab = "Primo Fattore", ylab = "Secondo Fattore",
  ylim = c(-.6, 1), xlim = c(0,1), pch = 19)
  axis(1, pos = c(0,0))
  abline(0, 0)  
```

Rencher (2002) nota che, per questi dati, una rotazione ortogonale di
$-35^{\circ}$ ci porterebbe ad avvicinare gli assi ai punti nel
diagramma. Per verificare questo, disegnamo sul diagramma i nuovi assi
dopo una rotazione di $-35^{\circ}$. Le istruzioni `R` sono le seguenti:

```{r}
plot(
  f.pc$load[,1], f.pc$load[,2], bty = 'n', xaxt = 'n', 
  xlab = "Primo Fattore", ylab = "Secondo Fattore",
  ylim = c(-.6, 1), xlim = c(0,1), pch = 19)
  axis(1, pos = c(0,0))
  abline(0, 0) 
  
ar <- matrix(c(
  0, 0,
  0, 1,
  0, 0,
  1, 0
), ncol = 2, byrow = TRUE)

angle <- 35
rad <- angle * pi / 180
T <- matrix(c(
  cos(rad), -sin(rad),
  sin(rad),  cos(rad)
), ncol = 2, byrow = TRUE)

round(ar %*% T, 3)

arrows(0, 0, 0.574,  0.819, lwd = 2)
arrows(0, 0, 0.819, -0.574, lwd = 2)
```


Nella figura precedente, le due frecce rappresentano gli assi ruotati. È
chiaro come tale rotazione di $-35^{\circ}$ ha effettivamente l'effetto
di avvicinare gli assi ai punti del diagramma. Se usiamo dunque il
valore $\phi = -35^{\circ}$ nella matrice di rotazione, possiamo
calcolare le saturazioni fattoriali della soluzione ruotata
$\hat{\boldsymbol{\Lambda}}^* = \hat{\boldsymbol{\Lambda}} \textbf{T}$.
Le saturazioni fattoriali ruotate non sono altro che la proiezione
ortogonale dei punti sugli assi ruotati:

```{r}
angle <- -35
rad <- angle * pi / 180
T <- matrix(c(
  cos(rad), -sin(rad),
  sin(rad),  cos(rad)
), ncol = 2, byrow = TRUE)
round(f.pc$load %*% T, 3)
```

La soluzione ottenuta in questo modo riproduce quella riportata da
Rencher (2002).
:::

### Medodi di rotazione ortogonale

Un tipo di rotazione ortogonale molto utilizzato è la rotazione Varimax
(Kaiser, 1958). La matrice $\hat{\boldsymbol{\Lambda}}$ è semplificata
in modo tale che le varianze dei quadrati degli elementi $\lambda_{ij}$
appartenenti a colonne diverse di $\hat{\boldsymbol{\Lambda}}$ siano
massime. Se le saturazioni fattoriali in una colonna di
$\hat{\boldsymbol{\Lambda}}$ sono simili tra loro, la varianza sarà
prossima a zero. Tale varianza è tanto più grande quanto più i quadrati
degli elementi $\lambda_{ij}$ assumono valori prossimi a $0$ e $1$.
Amplificando le correlazioni più alte e riducendo quelle più basse, la
rotazione Varimax agevola l'interpretazione di ciascun fattore.

Usando la funzione `factanal()` del modulo base, la rotazione Varimax
può essere applicata alla soluzione ottenuta mediante il metodo di
massima verosimiglianza. Usando le funzioni `principal()` e
`factor.pa()` disponibili nel pacchetto `psych`, la rotazione Varimax
può essere applicata alle soluzioni ottenute mediante il metodo delle
componenti principali e il metodo del fattore principale. Ad esempio, otteniamo:

```{r}
f_pc <- principal(R, 2, n.obs = 7, rotate = "varimax")
f_pc
```

Il metodo Quartimax (Neuhaus e Wringley, 1954) opera una semplificazione
della matrice $\hat{\boldsymbol{\Lambda}}$ massimizzando le covarianze
tra i quadrati degli elementi $\lambda_{ij}$ appartenenti a righe
diverse, subordinatamente alla condizione che la varianza delle righe
rimanga inalterata.

### Metodi di rotazione obliqua

Parlare di rotazione obliqua significa usare un termine improprio: per
definizione, infatti, una rotazione implica una trasformazione
ortogonale che preserva le distanze. Secondo Rencher (2002), un termine
migliore sarebbe *trasformazione* obliqua. Il termine rotazione obliqua,
comunque, fa parte dell'uso corrente.

Nella rotazione obliqua, gli assi della soluzione ruotata non devono
rimanere ortogonali e quindi possono più facilmente avvicinarsi ai
raggruppamenti di punti nello spazio delle saturazioni fattoriali
(assumendo che dei raggruppamenti esistano). Vari metodi analitici sono
stati proposti per ottenere una rotazione obliqua. Qui esamineremo
brevemente solo uno di essi, il metodo Direct Oblimin.

Il criterio usato nel metodo Direct Oblimin (Jennrich e Sampson, 1966) è
il seguente:

$$
\sum_{ij} \left(\sum_v \lambda_i^2 \lambda_j^2 - w \frac{1}{p} \sum_v \lambda_i^2
\sum_v \lambda_j^2\right)
$$ 

dove $\sum_{ij}$ si riferisce alla somma su tutte le coppie di fattori $ij$. In questo caso si procede ad una minimizzazione piuttosto che a una masssimizzazione.


## Matrice dei pesi fattoriali e matrice di struttura

Nella rotazione ortogonale i fattori sono incorrelati. Si consideri la situazione presentata nella figura \@ref(fig:fact-rot4), con due variabili latenti incorrelate
($\xi_1$ e $\xi_2$) e quattro variabili manifeste ($y_1$, $y_2$, $y_3$,
$y_4$). Siano $\lambda_{11}$, $\lambda_{12}$, $\lambda_{13}$ e
$\lambda_{14}$ le saturazioni fattoriali delle variabili nel primo
fattore; siano $\lambda_{21}$, $\lambda_{22}$, $\lambda_{23}$ e
$\lambda_{24}$ le saturazioni fattoriali delle variabili nel secondo
fattore.

```{r fact-rot4, echo=FALSE, fig.cap="Modello fattoriale con due fattori comuni ortogonali.", out.width = '80%'}
knitr::include_graphics("images/rot_4.png")
```

In un diagramma di percorso, la correlazione tra due variabili contenute
è uguale alla somma dei valori numerici di tutti i percorsi legittimi
che collegano le variabili. Se i fattori comuni sono incorrelati (come
nella figura \@ref(fig:fact-rot4), allora in un *path diagram* c'è un unico percorso legittimo che collega ciascuna variabile manifesta a ciascun fattore comune in base alle regole di Wright. Le correlazioni tra variabili manifeste e fattori comuni sono dunque uguali alle saturazioni fattoriali. Nel caso di fattori comuni sono incorrelati, dunque, la matrice delle saturazioni fattoriali descrive le correlazioni fra variabili e fattori. Si ricordi che le saturazioni fattoriali possono essere interpretate in maniera equivalente ai pesi beta del modello di regressione multipla, ovvero come la stima del contributo specifico di ciascun fattore comune nella determinazione della varianza spiegata degli item (Tabachnick & Fidell, 2001).

Nel caso della rotazione obliqua, invece, la soluzione fattoriale
ruotata produce un insieme di fattori comuni fra loro correlati. Di
conseguenza, la matrice delle saturazioni fattoriali non descrive le correlazioni fra variabili e fattori. Infatti, in un *path diagram* ci sono almeno due percorsi legittimi che collegano ciascuna variabile manifesta a ciascun fattore comune in base alle regole di Wright. Nel caso di una rotazione obliqua è quindi necessario specificare tre diverse matrici:

-   la matrice delle saturazioni fattoriali, $\hat{\boldsymbol{\Lambda}}$, detta
    *matrice pattern* (*factor pattern matrix*, o "configurazione," o
    "matrice dei modelli");
-   la matrice delle correlazioni tra variabili manifeste e fattori, detta *matrice di struttura* (*factor structure matrix*);
-   la  matrice che esprime le correlazioni tra i fattori, $\hat{\boldsymbol{\Phi}}$, detta *matrice di intercorrelazione fattoriale*.

In questo caso, la matrice pattern rappresenta l'analogo dei coefficienti parziali di regressione della variabile sul fattore, al netto degli altri fattori. Nel caso
della rotazione obliqua, è la matrice che viene usata per determinare in
che grado è stata raggiunta la "struttura semplice".

Esaminiamo in dettaglio la soluzione fattoriale che viene prodotta da una
rotazione obliqua. In tali circostanze, gli assi che rappresentano i fattori non sono
ortogonali (ovvero, i fattori sono correlati) e, in un diagramma di
percorso, le variabili manifeste sono collegate ai fattori attraverso
due tipi distinti di percorsi. Tali percorsi rappresentano l'effetto "diretto" e
"indiretto" dei fattori sulle variabili. Nel caso di una rotazione
obliqua, come abbiamo detto sopra, le saturazioni fattoriali non
coincidono con le correlazioni tra variabili e fattori. Si consideri la figura \@ref(fig:fact-rot5). Nel caso di una rotazione obliqua, la
correlazione tra i due fattori comuni viene rappresentata mediante la
freccia non direzionata $\phi_{12}$ che collega $\xi_1$ e $\xi_2$. Nel diagramma di percorso della figura \@ref(fig:fact-rot5) ci sono due percorsi legittimi che, in base alle regole di Wright, consentono di collegare ciascuna variabile manifesta ad un
fattore comune. Ad esempio, nel caso della variabile $y_1$ e il fattore $\xi_1$, i percorsi sono: la freccia causale $\lambda_{11}$ che
rappresenta l'effetto diretto di $\xi_1$ su $y_1$ e il percorso composto
che rappresenta l'effetto indiretto di $\xi_1$ su $y_1$. Il valore
numerico di tale percorso composto è uguale al prodotto
$\lambda_{21}\phi_{12}$. Nei termini dell'analisi dei percorsi, dunque,
la correlazione tra $\xi_1$ e $y_1$ è uguale alla somma dei valori
numerici dei percorsi legittimi che collegano $y_1$ a $\xi_1$, ovvero
$\lambda_{11} + \lambda_{21} \phi_{12}$.

```{r fact-rot5, echo=FALSE, fig.cap="Modello fattoriale con due fattori comuni dopo una rotazione obliqua.", out.width = '80%'}
knitr::include_graphics("images/rot_5.png")
```

Per illustrare la rotazione obliqua, utilizziamo i dati discussi da
Rencher (2002). Si consideri la matrice di correlazione presentata qui sotto.

```{r}
R <- matrix(
      c( 
        1.00,  0.735, 0.711, 0.704,
        0.735, 1.00,  0.693, 0.709,
        0.711, 0.693, 1.00,  0.839,
        0.704, 0.709, 0.839, 1.00
      ), 
      ncol = 4, 
      byrow = TRUE
    ) 
R
```

Iniziamo calcolando la soluzione a due fattori mediante il metodo delle
componenti principali e una rotazione Varimax (ovvero, ortogonale). Otteniamo le seguenti saturazioni fattoriali.

```{r}
f1_pc <- principal(R, 2, rotate = "varimax") 
f1_pc
```

Si noti che i due fattori non sono molto distinti. Consideriamo dunque la soluzione  prodotta da una rotazione obliqua. Usiamo qui l'algoritmo Oblimin.

```{r}
pr_oblimin <- principal(R, 2, rotate = "oblimin")
```
     
La matrice $\hat{\boldsymbol{\Lambda}}$ delle saturazioni fattoriali si ricava come indicato di seguito.

```{r}
cbind(pr_oblimin$load[, 1], pr_oblimin$load[, 2])
```

La matrice $\hat{\boldsymbol{\Phi}}$ di inter-correlazione fattoriale è la seguente.

```{r}
pr_oblimin$Phi
```

La matrice di struttura, che riporta le correlazioni tra indicatori e fattori comuni, si ottiene pre-moltiplicando la matrice $\boldsymbol{\Lambda}$ delle saturazioni fattoriali alla matrice $\boldsymbol{\Phi}$ di inter-correlazione fattoriale.

$$
\text{matrice di struttura} = \boldsymbol{\Lambda}\boldsymbol{\Phi}.
$$

Per esempio, la correlazione tra la prima variabile manifesta e il primo fattore si ottiene nel modo seguente.

```{r}
pr_oblimin$load[1, 1] + pr_oblimin$load[1, 2] * pr_oblimin$Phi[2, 1]
```

L'intera matrice di struttura si può trovare eseguendo la moltiplicazione  $\boldsymbol{\Lambda}\boldsymbol{\Phi}$.

```{r}
pr_oblimin$load %*% pr_oblimin$Phi %>% 
  round(3)
```

## Esempio con `semTools`

Presento qui un esempio di uso di vari metodi di estrazione fattoriale. Tra tali  metodi, la rotazione obliqua Geomin è molto popolare ed è il default di M-Plus. 

Iniziamo a caricare il pacchetto `semTools`.

```{r}
suppressPackageStartupMessages(library("semTools")) 
```

Eseguiamo l'analisi fattoriale esplorativa del classico set di dati di Holzinger e Swineford (1939) il quale è costituito dai punteggi dei test di abilità mentale di bambini di seconda e terza media di due scuole diverse (Pasteur e Grant-White). Nel set di dati originale (disponibile nel pacchetto `MBESS`), sono forniti i punteggi di 26 test. Tuttavia, un sottoinsieme più piccolo con 9 variabili è più ampiamente utilizzato in letteratura. Questi sono i dati qui usati. 

Nel presente esempio, verrà eseguita l'analisi fattoriale esplorativa con l'estrazione di tre fattori. Il metodo di estrazione è `mlr`:

> maximum likelihood estimation with robust (Huber-White) standard errors and a scaled test statistic that is (asymptotically) equal to the Yuan-Bentler test statistic. For both complete and incomplete data.

La soluzione iniziale non è ruotata. 

```{r}
unrotated <- efaUnrotate(HolzingerSwineford1939, nf=3, varList=paste0("x", 1:9), estimator="mlr")
summary(unrotated)
```

Si noti che, in assenza di rotazione, è impossibile assegnare un significato ai fattori comuni.

### Orthogonal varimax

Utilizziamo ora la rotazione ortogonale Varimax.

```{r}
out_varimax <- orthRotate(unrotated, method="varimax")
summary(out_varimax, sort=FALSE, suppress=0.3)
```

### Orthogonal Quartimin

Un metodo alternativo per la rotazione ortogonale è Quartimin.

```{r}
out_quartimin <- orthRotate(unrotated, method="quartimin")
summary(out_quartimin, sort=FALSE, suppress=0.3)
```

### Oblique Quartimin

L'algoritmo Quartimin può anche essere usato per una soluzione obliqua.

```{r}
out_oblq <- oblqRotate(unrotated, method="quartimin")
summary(out_oblq, sort=FALSE, suppress=0.3)
```

### Orthogonal Geomin

Consideriamo ora la rotazione Geomin. L'algoritmo Geomin fornisce un metodo di rotazione che riduce al minimo la media geometrica delle saturazioni fattoriali innalzate al quadrato. Qui è usato per ottenere una soluzione ortogonale.

```{r}
out_geomin_orh <- orthRotate(unrotated, method="geomin")
summary(out_geomin_orh, sort=FALSE, suppress=0.3)
```

### Oblique Geomin

La rotazione Geomin può anche essere usata per ottenere una soluzione obliqua.

```{r}
out_geomin_obl <- oblqRotate(unrotated, method="geomin")
summary(out_geomin_obl, sort=FALSE, suppress=0.3)
```

## Interpretazione 

Possiamo chiederci se, per fornire un'interpretazione ai fattori comuni latenti, sia più opportuno usare la matrice pattern o la matrice struttura.  Teoricamente, il fattore individuato dall'analisi fattoriale è una caratteristica latente univariata (una sorta di qualità che esprime l'"essenza" di un fenomeno psicologico). Poiché il fattore è una sorta di "essenza" univariata, dovrebbe essere interpretato come il significato (relativamente semplice) che giace sopra (o "dietro") l'intersezione dei significati/contenuti delle variabili che saturano nel fattore. Nella rotazione obliqua i fattori sono correlati. Vogliamo comunque interpretarli come dimensioni psicologiche distinte. L'etichetta che assegniamo al fattore $F_1$ dovrebbe contribuire a dissociare, in termini teorici, il fenomeno psicologico corrispondente a $F_1$ dal fenomeno denotato dall'etichetta del fattore $F_2$, anche in presenza di una correlazione tra i due, per sottolineare l'individualità di entrambi i fattori, pur riconoscendo il fatto che "nella realtà esterna" i due corrispondenti fenomeni psicologici tendono a covariare. 

Se è questa la strategia interpretativa, allora lo strumento principale per l'interpretazione corrisponde alla matrice pattern. Infatti, i coefficienti della matrice  pattern, in quanto interpretabili come coefficienti parziali di regressione, rivelano l'influenza "causale" del fattore comune nella determinazione delle variabili manifeste. La matrice struttura, invece, descrive le correlazioni tra variabili e fattori. Abbiamo visto che, all'interno di un diagramma di percorso, la correlazione dipende sia dai percorsi diretti ("relazioni causali") sia dai percorsi indiretti (che dipendono dalle correlazioni tra i fattori). Dunque la matrice struttura non descrive gli "effetti diretti", cioè "causali" dei fattori comuni latenti sulle variabili manifeste ma solo, appunto, la covariazione tra i fattori comuni e le variabili manifeste.

Il punto debole della matrice pattern è che è meno stabile da un campione all'altro, come lo sono di solito i coefficienti di regressione rispetto ai coefficienti di correlazione. Di conseguenza, fare affidamento sulla matrice pattern per l'interpretazione richiede  uno studio ben pianificato e un'adeguata dimensione campionaria. Per uno studio pilota e un'interpretazione provvisoria, invece, la matrice struttura potrebbe essere la scelta migliore.

