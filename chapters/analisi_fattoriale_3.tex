% DO NOT COMPILE THIS FILE DIRECTLY!
% This is included by the other .tex files.

%%------------------------------------------------------------
\chapter{Il modello multifattoriale}
\label{ch:mod_multifattoriale}
%%------------------------------------------------------------


% ----------------------------------------------------------------
\section{Modello multifattoriale: fattori ortogonali}
% ----------------------------------------------------------------

La teoria dei due fattori ha orientato per diversi anni le ricerche
sull'intelligenza, finch{\'e} Thurstone (1945) non propose una sua
modifica, conosciuta come teoria multifattoriale. 
Secondo  Thurstone la covariazione tra le variabili manifeste non può  essere spiegata da un unico fattore generale. Invece è necessario ipotizzare l'azione causale di diversi
fattori, definiti comuni, i quali si riferiscono solo ad alcune delle
variabili considerate.

Il modello plurifattoriale assume che ciascuna variabile manifesta sia
espressa come funzione lineare di un certo numero $m$ di fattori
comuni, $\xi_1, \xi_2, \dots, \xi_m$, responsabili della correlazione con le
altre variabili, e di un solo fattore specifico (termine d'errore),
responsabile della variabilit{\`a} della variabile stessa.
Per $p$ variabili manifeste, $Y_1, Y_2, \dots, Y_p$, il modello fattoriale diventa quello indicato dal sistema di equazioni lineari descritto di seguito.  
Idealmente, $m$ dovrebbe essere molto più piccolo di $p$ così da
consentire una descrizione parsimoniosa delle variabili manifeste in
funzione di pochi fattori soggiacenti. 

Le variabili manifeste $Y$ sono indicizzate da $i = 1, \dots, p.$
Le variabili latenti $\xi$ (fattori) sono indicizzate da $j = 1, \dots, m.$ 
I fattori specifici $\delta$ sono indicizzati da $i = 1, \dots, p.$
Le saturazioni fattoriali si distinguono dunque tramite  due indici, $i$ e $j$: il primo indice si riferisce alle variabili manifeste, il secondo si riferisce ai fattori latenti.

Indichiamo con $\mu_i$, con $i=1, \dots, p$ le medie delle $p$ variabili manifeste $Y_1, Y_2, \dots, Y_p$.  
Se non vi è alcun effetto delle variabili comuni latenti, allora la variabile $Y_{ijk}$, dove $k$ è l'indice usato per i soggetti, sarà uguale a:

\begin{equation}
\begin{cases} 
  Y_{1k}    &= \mu_1 + \delta_{1k} \\
&\vdots\\
Y_{ik}   &= \mu_i + \delta_{ik}\\
&\vdots\\
Y_{pk}   &= \mu_p + \delta_{pk} \notag
\end{cases}
\end{equation} 
Se invece le variabili manifeste rappresentano la somma dell'effetto causale di $m$ fattori comuni e di $p$ fattori specifici, allora possiamo scrivere:
\begin{equation}
\begin{cases} 
  Y_1  - \mu_1  &= \lambda_{11}\xi_1 + \dots + \lambda_{1k}\xi_k \dots +\lambda_{1m}\xi_m + \delta_1 \\
&\vdots\\
Y_i -  \mu_i  &= \lambda_{i1}\xi_1 + \dots +  \lambda_{ik}\xi_k \dots +\lambda_{im}\xi_m + \delta_i\\
&\vdots\\
Y_p - \mu_p  &= \lambda_{p1}\xi_1 + \dots +  \lambda_{pk}\xi_k \dots +\lambda_{pm}\xi_m + \delta_p \notag
\end{cases}
\end{equation}
Nel precedente sistema di equazioni lineari, 
\begin{itemize}
\item 
  $\xi_j$, con $j=1, \dots, m$, rappresenta la $j$-esima variabile inosservabile a fattore comune (ossia il $j$-esimo fattore comune a tutte le variabili $Y_i$);
\item 
  $\lambda_{ij}$ rappresenta il parametro, detto \emph{saturazione} o \emph{peso} fattoriale, che riflette l'importanza del $j$-esimo fattore comune nella composizione della $i$-esima variabile osservabile;
\item 
  $\delta_i$ rappresenta il fattore specifico (o unico) di ogni variabile manifesta $Y_i$.
\end{itemize}

In conclusione, secondo il modello multifattoriale, le variabili manifeste $Y_i$, con $i=1, \dots, p$, sono il risultato di una \emph{combinazione lineare} di $m < p$ fattori inosservabili ad esse comuni $\xi_j$, con $j=1, \dots, m$, e di $p$ fattori specifici $\delta_i$, con $i=1, \dots, p$, anch'essi inosservabili e di natura residua.


\subsection{Assunzioni del modello multifattoriale}

Le variabili inosservabili a fattore comune $\xi_j$, con $j=1, \dots, m$, in quanto latenti, non possiedono unità di misura. 
Pertanto, per semplicità  si assume che abbiano media zero, $\Ev (\xi_j)=0$, abbiano varianza unitaria, $\var (\xi_j)= \Ev (\xi_j^2) - [\Ev (\xi_j)]^2=1$, e siano incorrelate tra loro, $\cov(\xi_j, \xi_h)=0$, con $j, h = 1, \dots, m; \;j \neq h$. 
Si assume inoltre che le variabili a fattore specifico $\delta_i$ 
siano tra loro incorrelate, $\cov(\delta_i,\delta_k)=0$, con $i, k = 1, \dots, p, \; i \neq k$, abbiano media zero, $\Ev (\delta_i)=0$, e varianza uguale a $\var (\delta_i) = \psi_{ii}$.
La varianza $\psi_{ii}$ è detta \emph{varianza specifica} o \emph{unicità} della $i$-esima variabile manifesta $Y_i$.
Si assume infine che i fattori specifici siano linearmente incorrelati con i fattori comuni, ovvero  $\cov(\xi_j, \delta_i)=0$ per ogni $j=1, \dots, m$ e per ogni $i=1\dots,p$. 


\subsection{Interpretazione dei parametri del modello}

Quale esempio, consideriamo il caso di $p=5$ variabili osservabili e $m=2$ fattori ortogonali.
Se le variabili manifeste sono `centrate' (ovvero, se a ciascuna di esse sottraiamo la rispettiva media), allora il modello multifattoriale diventa
\begin{align}
  Y_1 &= \lambda_{11} \xi_1 + \lambda_{12} \xi_2 + \delta_1,\notag\\
  Y_2 &= \lambda_{21} \xi_1 + \lambda_{22} \xi_2 + \delta_2,\notag\\
  Y_3 &= \lambda_{31} \xi_1 + \lambda_{32} \xi_2 + \delta_3,\notag\\
  Y_4 &= \lambda_{41} \xi_1 + \lambda_{42} \xi_2 + \delta_4,\notag\\
  Y_5 &= \lambda_{51} \xi_1 + \lambda_{52} \xi_2 + \delta_5.\notag
  \label{plurifattore2}
\end{align}


\subsection{Covarianza tra variabili e fattori}

Nell'ipotesi che le variabili $Y_i$ abbiano media nulla, la covarianza tra $Y_i$ e $\xi_j$ è uguale alla saturazione fattoriale $\lambda_{ij}$:
\begin{align}
  \cov(Y_i, \xi_j) &= \Ev(Y_i \xi_j)\notag\\[10pt]
  &=\Ev\left[(\lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i)\xi_j \right]\notag\\[10pt]
  &= \lambda_{i1}\underbrace{\Ev(\xi_1\xi_j)}_{=0} + \dots + 
\lambda_{ij}\underbrace{\Ev(\xi_j^2)}_{=1} + \dots \notag\\[10pt]
& \; + \lambda_{im}\underbrace{\Ev(\xi_m\xi_j)}_{=0} +
  \underbrace{\Ev(\delta_i \xi_j)}_{=0}\notag\\[10pt]
  &= \lambda_{ij}.\notag
\end{align}
\label{eq:cov_multifatt_orto}
Anche nel modello multifattoriale, dunque, le saturazioni fattoriali rappresentano le covarianze tra le variabili e i fattori:
\begin{equation}
  \cov(Y_i, \xi_j) = \lambda_{ij} \qquad i=1, \dots, p; \quad j= 1, \dots, m. \notag
\end{equation} 
Naturalmente, se le variabili sono standardizzate, le saturazioni fattoriali diventano correlazioni:
\begin{equation}
  r_{ij} = \lambda_{ij}. \notag
\end{equation}


\subsection{Espressione fattoriale della varianza}

Come nel modello monofattoriale, la varianza delle variabili manifeste
si decompone in una componente dovuta ai fattori comuni, chiamata
\emph{comunalità}, e in una componente specifica alle $Y_i$, chiamata
\emph{unicità}. 
Nell'ipotesi che le variabili $Y_i$ abbiano media nulla, la varianza 
di $Y_i$ è uguale a
\begin{align}
  \var (Y_i) 
  &=\Ev\left[ (\lambda_{i1} \xi_1 + \dots +
    \lambda_{im} \xi_m + \delta_i)^2 \right].\notag
\end{align}
\label{eq_var_multifatt}
Come si sviluppa il polinomio precedente?
Il quadrato di un polinomio è uguale alla somma dei quadrati di tutti
i termini più il doppio prodotto di ogni termine per ciascuno di
quelli che lo seguono.
 Il valore atteso del quadrato del primo termine
è uguale a $\lambda_{i1}^2\Ev(\xi_1^2)$ ma, essendo la varianza
di $\xi_1$ uguale a $1$, otteniamo semplicemente $\lambda_{i1}^2$. Lo stesso vale per i quadrati di tutti i termini seguenti tranne
l'ultimo. Infatti, $\Ev(\delta_i^2)=\psi_{ii}$. 
Per quel che riguarda i doppi prodotti, sono tutti nulli. In primo luogo perché, nel caso di fattori ortogonali, la covarianza tra i fattori
comuni è nulla, $\Ev(\xi_j \xi_h)=0$,
con $j \neq h$.
In secondo luogo perché il fattori comuni cono
incorrelati con i fattori specifici, quindi $\Ev(\delta_i
\xi_j)=0$.

In conclusione,
\begin{align}
  \var(Y_i) &= \lambda_{i1}^2 + \lambda_{i2}^2 + \dots + \lambda_{im}^2 + \psi_{ii} \notag\\
  &= \sum_{j=1}^m \lambda_{ij}^2 + \psi_{ii}\notag\\
  &= h_i^2 + \psi_{ii}\notag\\
  &=\text{communalità} + \text{unicità},\notag
\end{align}
la varianza della variabile manifesta $Y_i$ è suddivisa in due parti: il primo addendo è definito comunalità poiché  rappresenta la parte di variabilità della $Y_i$ spiegata dai fattori comuni; il secondo addendo è  invece definito varianza specifica (o unicità) poiché esprime la parte di variabilità della $Y_i$ non spiegata dai fattori comuni.


\subsection{Espressione fattoriale della covarianza}

Per semplificare, consideriamo il caso particolare esaminato prima, ovvero quello con $p=5$ variabili osservabili e $m=2$ fattori ortogonali. 
Nell'ipotesi che le variabili $Y_i$ abbiano media nulla, la covarianza 
tra $Y_1$ e $Y_2$, ad esempio, è uguale a:
\begin{align}
  \cov(Y_1, Y_2) &= \Ev\left( Y_1 Y_2\right) \notag\\
  &= \Ev \left[ 
  (\lambda_{11} \xi_1 + \lambda_{12} \xi_2 + \delta_1)
   (\lambda_{21} \xi_1 + \lambda_{22} \xi_2 +  \delta_2)
  \right]\notag\\
  &= \lambda_{11} \lambda_{21} \Ev (\xi_1^2) +
      \lambda_{11} \lambda_{22} \Ev (\xi_1 \xi_2) +\notag 
      \lambda_{11} \Ev (\xi_1 \delta_2) +\notag\\
    &\quad \lambda_{12} \lambda_{21}\Ev(\xi_1 \xi_2)\, + 
      \lambda_{12} \lambda_{22}\Ev(\xi^2_2)\, + 
      \lambda_{12} \Ev (\xi_2\delta_2) +\notag\\
    &\quad \lambda_{21} \Ev(\xi_1\delta_1) +\notag 
     \lambda_{22} \Ev(\xi_2\delta_1) + \Ev(\delta_1 \delta_2)\notag\\
   &= \lambda_{11} \lambda_{21} + \lambda_{12} \lambda_{22}.\notag
\end{align}

In conclusione, la covarianza tra le variabili manifeste $Y_l$ e $Y_m$ riprodotta dal modello è data dalla somma dei prodotti delle saturazioni $\lambda_l \lambda_m$ nei due fattori.


%------------------------------------------------------------

\begin{exmp}

Si consideri la matrice di correlazione dei voti ottenuti su 6 materie da un  campione di 220 studenti (Lawley e Maxwell, 1971).
  
\begin{lstlisting}
R <- matrix(
  c(
    1.00, 0.44, 0.41, 0.29, 0.33, 0.25,
    0.44, 1.00, 0.35, 0.35, 0.32, 0.33,
    0.41, 0.35, 1.00, 0.16, 0.19, 0.18,
    0.29, 0.35, 0.16, 1.00, 0.59, 0.47,
    0.33, 0.32, 0.19, 0.59, 1.00, 0.46,
    0.25, 0.33, 0.18, 0.47, 0.46, 1.00
  ),
  ncol = 6, byrow = TRUE,
  dimnames = list(
    c(
      "Gaelico", "Inglese", "Storia",
      "Aritmetica", "Algebra", "Geometria"
    ),
    c(
      "Gaelico", "Inglese", "Storia", "Aritmetica",
      "Algebra", "Geometria"
    )
  )
)
\end{lstlisting}
Eseguiamo l'analisi fattoriale con il metodo della massima
verosimiglianza ipotizzando due fattori comuni incorrelati:
\begin{lstlisting}
f <- factanal(
  covmat = R, factors = 2, rotation = 'none', n.obs = 220
)
f
#> Uniquenesses:
#> Gaelico Inglese Storia Aritmetica Algebra Geometria 
#>   0.508   0.595  0.644      0.377   0.440     0.628 
#> 
#> Loadings:
#>           Factor1 Factor2
#> Gaelico     0.558   0.425 
#> Inglese     0.569   0.286 
#> Storia      0.392   0.450 
#> Aritmetica  0.738  -0.279 
#> Algebra     0.718  -0.209 
#> Geometria   0.595  -0.133 
#> 
#>                Factor1 Factor2
#> SS loadings      2.204   0.603
#> Proportion Var   0.367   0.101
#> Cumulative Var   0.367   0.468
\end{lstlisting}
La correlazione riprodotta $r_{12}$ è uguale a $\lambda_{11}\lambda_{21} + \lambda_{12}\lambda_{22}$, ovvero a
\begin{lstlisting}
0.558 * 0.569 + 0.425 * 0.286
#> [1] 0.439052
\end{lstlisting}
La correlazione riprodotta $r_{14}$ è uguale a $\lambda_{11}\lambda_{41} + \lambda_{12}\lambda_{42}$, ovvero a
\begin{lstlisting}
0.558 * 0.738 + 0.425 * -0.279
#> [1] 0.293229
\end{lstlisting}
L'intera matrice di correlazioni riprodotte è $\boldsymbol{\Lambda} \boldsymbol{\Lambda}^{\ensuremath{\mathsf{T}}} + \boldsymbol{\psi}$:


\begin{lstlisting}
round(f$load %*% t(f$load) + diag(f$uniq), 2)
\end{lstlisting}
{\small
\begin{lstlisting}
#>            Gaelico Inglese Storia Aritmetica Algebra Geometria
#> Gaelico       1.00    0.44   0.41       0.29    0.31      0.28
#> Inglese       0.44    1.00   0.35       0.34    0.35      0.30
#> Storia        0.41    0.35   1.00       0.16    0.19      0.17
#> Aritmetica    0.29    0.34   0.16       1.00    0.59      0.48
#> Algebra       0.31    0.35   0.19       0.59    1.00      0.46
#> Geometria     0.28    0.30   0.17       0.48    0.46      1.00
\end{lstlisting}
}
La differenza tra la matrice di correlazioni riprodotte e la matrice
di correlazioni osservate è uguale a:
\begin{lstlisting}
round(R - (f$load %*% t(f$load) + diag(f$uniq)), 2)
\end{lstlisting}
{\small
\begin{lstlisting}
#>            Gaelico Inglese Storia Aritmetica Algebra Geometria
#> Gaelico       0.00    0.00   0.00       0.00    0.02     -0.03
#> Inglese       0.00    0.00   0.00       0.01   -0.03      0.03
#> Storia        0.00    0.00   0.00       0.00    0.00      0.01
#> Aritmetica    0.00    0.01   0.00       0.00    0.00     -0.01
#> Algebra       0.02   -0.03   0.00       0.00    0.00      0.00
#> Geometria    -0.03    0.03   0.01      -0.01    0.00      0.00
\end{lstlisting}
}
\end{exmp}


\section{Modello fattoriale: Fattori obliqui}

Anche nel caso di fattori comuni correlati è possibile esprimere nei
termini dei parametri del modello la covarianza teorica tra una
variabile manifesta $Y_i$ e uno dei fattori comuni, la covarianza teorica tra due variabili manifeste, e la comunalità di ciascuna variabile manifesta. 
Dato però che i fattori comuni risultano correlati, l'espressione fattoriale di tali quantità è più complessa che nel caso di fattori comuni ortogonali.


\subsection{Covarianza teorica tra variabili e fattori}

In base al modello multifattoriale con $m$ fattori comuni la variabile $Y_i$ è
\begin{equation}
Y_i = \lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i.
\label{eq:mod_multifact}
\end{equation}
Poniamoci il problema di trovare la covarianza teorica tra la variabile manifesta $Y_i$ e il fattore comune $\xi_j$. 
Come in precedenza, il problema si riduce a quello di trovare $\Ev(Y_i \xi_j)$. 
%Infatti, la covarianza tra le variabili $Y_i$ e $\xi_j$ è $\cov(Y_i, \xi_j) =\Ev (Y_i \xi_j) - \mu_{Y_i}\mu_{\xi_j}$.  
%La variabile latente è standardizzata ($\mu$ = 0, $\sigma^2$ = 1) e dunque l'espressione della covarianza diventa $\cov(Y_i, \xi_j) =\Ev (Y_i \xi_j)$. 
Ne segue che
\begin{align}
  cov(Y_i, \xi_j) &= \Ev(Y_i \xi_j)\notag\\
  &=\Ev\left[(\lambda_{i1} \xi_1 + \dots + 
  \lambda_{ij} \xi_j + \dots +
  \lambda_{im} \xi_m + \delta_i)\xi_j \right]\notag\\
  &= \lambda_{i1}\underbrace{\Ev(\xi_1\xi_j)}_{\neq 0} + \dots + 
\lambda_{ij}\underbrace{\Ev(\xi_j^2)}_{=1} + \dots \notag\\
& \quad + \lambda_{im}\underbrace{\Ev(\xi_m\xi_j)}_{\neq 0} +
  \underbrace{\Ev(\delta_i \xi_j)}_{=0}\notag\\
  &= \lambda_{ij} +  \lambda_{i1} \cov(\xi_1, \xi_j) + \dots + \lambda_{im} \cov(\xi_m, \xi_j).
\label{eq:cov_multifatt_obli}
\end{align}

Ad esempio, nel caso di  tre fattori comuni $\xi_1, \xi_2, \xi_3$, la covarianza tra $Y_1$ e $\xi_{1}$ diventa
\begin{equation}
\lambda_{11} + \lambda_{12}\cov(\xi_1, \xi_2) + \lambda_{13}\cov(\xi_1, \xi_3).\notag
\end{equation}


\subsection{Espressione fattoriale della varianza}

Poniamoci ora il problema di trovare la varianza teorica della variabile manifesta $Y_i$.
In base al modello fattoriale, la variabile $Y_i$ è specificata come nell'equazione~\eqref{eq:mod_multifact}.
La varianza di $Y_i$ è $\var(Y_i) = \Ev(Y_i^2) -[\Ev(Y_i)]^2$. 
Però, avendo espresso $Y_i$ nei termini della differenza dalla sua media, l'espressione della varianza si riduce a $\var(Y_i) = \Ev(Y_i^2)$.
Dobbiamo dunque sviluppare l'espressione 
\begin{equation}
\Ev(Y_i^2) = \Ev[(\lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i)^2].\notag
\end{equation}

\begin{enumerate}[(a)]
\item 
  Il quadrato del primo termine è $\lambda_{i1}^2 \xi_1^2$.
Applicando l'operatore di valore atteso, abbiamo
\begin{equation}
\Ev(\lambda_{i1}^2 \xi_1^2) = \lambda_{i1}^2 \Ev(\xi_1^2)= \lambda_{i1}^2,\notag
\end{equation}
dato che la varianza delle variabili latenti, $\Ev(\xi_1^2)$, è 1.
Il quadrato di ciascun termine del polinomio~\ref{eq:mod_multifact}, tranne l'ultimo, avrà la stessa forma.
\item 
  Il quadrato dell'ultimo termine del polinomio~\ref{eq:mod_multifact} è 
\begin{equation}
  \Ev(\delta_{i}^2) = \psi_{ii}.\notag
\end{equation} 
Esso corrisponde alla quota di varianza della variabile manifesta $Y_i$ non spiegata dai fattori comuni latenti.
\item 
  Nel caso di fattori comuni correlati, $\cov(\xi_j, \xi_h) \neq 0$, per $j\neq h$; pertanto, sviluppando il quadrato del polinomio~\ref{eq:mod_multifact}, i doppi prodotti che includono il termine $\Ev(\xi_j \xi_h)$ non sono nulli. 
Il doppio prodotto dei primi due termini è $2\lambda_{i1} \lambda_{i2} \xi_1 \xi_2$.
Applicando l'operatore di valore atteso, troviamo
\begin{align}
\Ev(2\lambda_{i1} \lambda_{i2} \xi_1 \xi_2) &= 2\lambda_{i1} \lambda_{i2} \Ev(\xi_1\xi_2) 
= 2\lambda_{i1} \lambda_{i2} \cov(\xi_1, \xi_2),\notag
\end{align}
dato che $\cov(\xi_1, \xi_2) = \Ev(\xi_1\xi2)$ se $\xi_1$ e $\xi_2$ hanno media uguale a 0.
Tutti i doppi prodotti che non includono il termine $\delta_i$ hanno la stessa forma.
\item 
  I doppi prodotti che includono il termine $\delta_i$ sono invece tutti uguali a zero perché $\Ev(\xi_j\delta_i) = 0$ in base alle assunzioni del modello fattoriale.
\end{enumerate}

In conclusione, la varianza teorica di $Y_i$ è uguale a
%\begin{align}
%  \var(Y_i) = &\lambda_{i1}^2 + \lambda_{i2}^2 + \dots + \lambda_{im}^2  + \notag\\
%& 2 \lambda_{i1} \lambda_{i2} \cov(\xi_1, \xi_2) + \dots + 2 \lambda_{i,m-1} \lambda_{im} \cov(\xi_{m-1}, \xi_m) + \notag\\
%&  \psi_{ii}.\notag
%\end{align}
\begin{equation}
\begin{split}
\var(Y_i) &= \lambda_{i1}^2 + \lambda_{i2}^2 + \dots + \lambda_{im}^2  + \\
&\quad 2 \lambda_{i1} \lambda_{i2} \cov(\xi_1, \xi_2) + \dots + 2 \lambda_{i,m-1} \lambda_{im} \cov(\xi_{m-1}, \xi_m) + \\
&\quad \psi_{ii}.\notag
\end{split}
\end{equation}


Ad esempio, nel caso di  tre fattori comuni, $\xi_1, \xi_2, \xi_3$, la varianza di $Y_1$ è
\begin{equation}
\begin{split}
\var(Y_1) = &\lambda_{11}^2 + \lambda_{12}^2 + \lambda_{13}^2 +\\ 
&\quad 2 \lambda_{11} \lambda_{12} \cov(\xi_1, \xi_2) + \\ 
&\quad 2 \lambda_{11} \lambda_{13} \cov(\xi_1, \xi_3) + \\ 
&\quad 2 \lambda_{12} \lambda_{13} \cov(\xi_2, \xi_3) + \\ 
&\quad \psi_{11}. \notag
\end{split}
\end{equation}


\subsection{Covarianza teorica tra due variabili}

Consideriamo ora il caso più semplice di due soli fattori comuni correlati e calcoliamo la covarianza tra $Y_1$ e $Y_2$:
\begin{align}
\Ev(Y_1 Y_2) =\Ev[(&\lambda_{11}\xi_1 + \lambda_{12}\xi_2+\delta_1) (\lambda_{21}\xi_1 + \lambda_{22}\xi_2+\delta_2)]\notag\\
=\Ev( 
&\lambda_{11}\lambda_{21}\xi_1^2 +
\lambda_{11}\lambda_{22}\xi_1\xi_2 +
\lambda_{11}\xi_1\delta_2 +\notag\\
+&\lambda_{12}\lambda_{21}\xi_1\xi_2 +
\lambda_{12}\lambda_{22}\xi_2^2 +
\lambda_{12}\xi_2\delta_2 +\notag\\
+&\lambda_{21}\xi_1\delta_1 +
\lambda_{22}\xi_2\delta_1 +
\delta_1\delta_2).\notag
\end{align}
Distribuendo l'operatore di valore atteso, dato che $\Ev(\xi^2)=1$ e $\Ev(\xi \delta)=0$, otteniamo
\begin{align}
\cov(Y_1, Y_2) = &\lambda_{11} \lambda_{21} + \lambda_{12} \lambda_{22} + 
\lambda_{12} \lambda_{21}cov(\xi_1, \xi_2) +\lambda_{11} \lambda_{22}cov(\xi_1, \xi_2). \notag
\end{align}
In termini matriciali si scrive
\begin{align}
\boldsymbol{\Sigma} &=\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\ensuremath{\mathsf{T}}} + \boldsymbol{\Psi}, \notag
\end{align}
dove $\boldsymbol{\Phi}$ è la matrice di ordine $m \times m$ di varianze e covarianze tra i fattori comuni e $\boldsymbol{\Psi}$ è una matrice diagonale  di ordine $p$ con le unicità delle variabili. 


\begin{exmp}
Usando la funzione \verb+sim.structure()+ del pacchetto \verb+psych+ di \R\,, simuliamo la matrice di correlazioni per un modello a due fattori correlati. È necessario caricare i pacchetti \verb+psych+ e \verb+GPArotation+.
\begin{lstlisting}
library("tidyverse")
library("psych")
library("GPArotation")
set.seed(1234)
fx <- matrix(c(
  0.70, 0.10,
  0.70, 0.20,
  0.75, 0.15,
  0.15, 0.70,
  0.10, 0.80,
  0.10, 0.60
),
ncol = 2,
byrow = TRUE
)
fx
#>      [,1] [,2]
#> [1,] 0.70 0.10
#> [2,] 0.70 0.20
#> [3,] 0.75 0.15
#> [4,] 0.15 0.70
#> [5,] 0.10 0.80
#> [6,] 0.10 0.60
Phi <- matrix(c(
  1.0, 0.5,
  0.5, 1.0
),
ncol = 2,
byrow = TRUE
)
Phi
#>      [,1] [,2]
#> [1,]  1.0  0.5
#> [2,]  0.5  1.0
sim_data <- sim.structure(fx, Phi, n = 1000, raw = TRUE)
R <- cor(sim_data$observed)
round(R, 2)
#>      V1   V2   V3   V4   V5   V6
#> V1 1.00 0.63 0.65 0.44 0.44 0.35
#> V2 0.63 1.00 0.70 0.50 0.53 0.41
#> V3 0.65 0.70 1.00 0.50 0.52 0.41
#> V4 0.44 0.50 0.50 1.00 0.68 0.50
#> V5 0.44 0.53 0.52 0.68 1.00 0.52
#> V6 0.35 0.41 0.41 0.50 0.52 1.00
\end{lstlisting}
Usando la matrice di correlazioni così creata, eseguiamo l'analisi fattoriale con la funzione \verb+fa()+ scegliendo una soluzione a due fattori obliqui.
\begin{lstlisting}
fo <- fa(R, nfactors = 2, n.obs = 200, rotate = "oblimin")
\end{lstlisting}
Dall'output di \verb+fa()+ estraiamo le saturazioni fattoriali.
\begin{lstlisting}
lambda <- cbind(fo$loadings[, 1], fo$loadings[, 2])
lambda
#>            [,1]        [,2]
#> V1  0.809101417 -0.08246318
#> V2  0.719166207  0.11308003
#> V3  0.812078353  0.01845304
#> V4  0.002585344  0.78479849
#> V5  0.019787520  0.81417979
#> V6 -0.012908072  0.69025180
\end{lstlisting}
Estraiamo dall'output di \verb+fa()+ anche la matrice $\Phi$ di intercorrelazioni fattoriali:
\begin{lstlisting}
Phi <- fo$Phi
Phi
#>           MR1       MR2
#> MR1 1.0000000 0.7109463
#> MR2 0.7109463 1.0000000
\end{lstlisting}
In termini matriciali, la matrice di correlazioni riprodotte è data da: 
\begin{align}
\boldsymbol{\Sigma} &=\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\ensuremath{\mathsf{T}}} + \boldsymbol{\Psi}. \notag
\end{align}
Le istruzioni \R\, per ottenere tale risultato sono le seguenti:
\begin{lstlisting}
R_hat <- 
  lambda %*% Phi %*% t(lambda) + diag(fo$uniquenesses)
round(R_hat, 2)
#>      V1   V2   V3   V4   V5   V6
#> V1 1.00 0.60 0.62 0.39 0.42 0.33
#> V2 0.60 1.00 0.66 0.49 0.52 0.42
#> V3 0.62 0.66 1.00 0.47 0.50 0.40
#> V4 0.39 0.49 0.47 1.00 0.65 0.54
#> V5 0.42 0.52 0.50 0.65 1.00 0.56
#> V6 0.33 0.42 0.40 0.54 0.56 1.00
\end{lstlisting}
La matrice di correlazioni residue si trova nel modo seguente:
\begin{lstlisting}
round(R - R_hat, 2)
#>      V1    V2   V3    V4    V5    V6
#> V1 0.00  0.03 0.03  0.05  0.02  0.02
#> V2 0.03  0.00 0.04  0.01  0.00 -0.01
#> V3 0.03  0.04 0.00  0.03  0.02  0.01
#> V4 0.05  0.01 0.03  0.00  0.03 -0.04
#> V5 0.02  0.00 0.02  0.03  0.00 -0.04
#> V6 0.02 -0.01 0.01 -0.04 -0.04  0.00
\end{lstlisting}
Calcoliamo ora la correlazione predetta dal modello fattoriale tra le variabili $Y_1$ e $Y_2$.
\begin{lstlisting}
lambda[1,1] * lambda[2,1] +
lambda[1,2] * lambda[2,2] +
lambda[1,1] * lambda[2,2] * Phi[1,2] +
lambda[1,2] * lambda[2,1] * Phi[1,2]
#> 0.5954377 
\end{lstlisting}
Questo valore riproduce il valore contenuto nell'elemento (1, 2) della matrice di correlazioni predetta dal modello:
\begin{lstlisting}
R_hat[1, 2]
#> 0.5954377
\end{lstlisting}


\end{exmp}


