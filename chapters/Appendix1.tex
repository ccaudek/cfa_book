%---------------------------------------------------------
\chapter{Appendice}
\label{chapter:appendix1} % Fornire sempre un'unica label
%---------------------------------------------------------


%------------------------------------------------------------
\section{Vettori nello spazio euclideo}
%------------------------------------------------------------


Un vettore geometrico {\`e} un segmento orientato
dotato di una lunghezza, una direzione e un verso.
Spesso viene rappresentato con una freccia.

%--fig 1
\begin{center}
\setlength{\unitlength}{0.3 cm}
\begin{picture}(12,12)\thicklines
% axes
\put(0,1){\line(1,0){10}} \put(1,0){\line(0,1){10}}
% ticks on axes
\put(5,1){\line(0,-1){.33}}\put(1,5){\line(-1,0){.33}}
% vector and label
\put(4.9,0){x} \put(0,4.9){y} \put(1,1){\vector(1,1){4}}
\put(5.1,5){(x,y)}
\end{picture}
\end{center}
Dato che i vettori non hanno posizione (ma solo direzione, verso e
intensit{\`a}), sono possibili rappresentazioni multiple dello
stesso vettore.
%--fig 2
\begin{center}
\setlength{\unitlength}{0.3 cm}
\begin{picture}(15,15)\thicklines
% axes
\put(0,1){\line(1,0){14}} \put(1,0){\line(0,1){14}}
\put(5,1){\line(0,-1){.33}}\put(1,5){\line(-1,0){.33}}
\put(4.8,-0.3){1} \put(-0.2,4.9){1} \put(1,1){\vector(1,1){4}}
\put(5.1,5){$\boldsymbol{a}$} \put(3,7){\vector(1,1){4}}
\put(7.1,11){$\boldsymbol{a}$} \put(7,2){\vector(1,1){4}}
\put(11.1,6){$\boldsymbol{a}$}
\end{picture}
\end{center}
Il vettore dal punto $(a,b)$ al punto $(c,d)$ {\`e} $(c-a, d-b)$.
\begin{center}
\setlength{\unitlength}{0.3 cm}
\begin{picture}(12,12)\thicklines
% axes
\put(0,1){\line(1,0){10}} \put(1,0){\line(0,1){10}}
% vector and label
\put(3,3){\vector(1,1){4}} \put(7.1,7){(c,d)} \put(2,2){(a,b)}
\put(3,3){\circle*{.2}} \put(7,7){\circle*{.2}}
\end{picture}
\end{center}
Nella discussione seguente, considereremo soltanto vettori che
hanno origine nel punto (0, 0).  Questo verr{\`a} chiarito
dall'esempio seguente.
La posizione di un punto nel piano pu{\`o} essere espressa nei
termini di una coppia ordinata di numeri ($x, y$), le coordinate
di quel punto.
 Tale coppia di valori rappresenta la distanza verticale
dal punto a ciascuno degli assi coordinati.
A esempio, il punto $P$ con coordinate (2,
3) è rappresentato nella figura seguente.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=5cm]{point.pdf}
\end{center}
\end{figure}

Possiamo anche definire il punto $P$ specificando la distanza e la
direzione di $P$ dall'origine, ovvero nei termini del vettore
$\overrightarrow{OP}$.  
A sua volta, questo vettore pu{\`o} essere espresso nei termini
delle sue componenti nelle direzioni orizzontali e verticali:
\[
\overrightarrow{OP} = \left[ \begin{array}{c}
2\\
3
\end{array}
 \right]
\]
Se volessimo specificare un punto in uno spazio a 3 dimensioni,
avremmo:
\[
\overrightarrow{OP} = \left[ \begin{array}{c}
x\\
y\\
z
\end{array}
 \right]
\]
In generale, un punto $P$ in uno spazio a $n$-dimensioni sar{\`a}
specificato da:
\[
\overrightarrow{OP} = \left[ \begin{array}{c}
v_1\\
v_2\\
\dots\\
v_n
\end{array}
 \right]
\]
Dal punto di vista geometrico, dunque, un vettore
rappresenta un punto in uno spazio $n$-dimensionale.

% Consideriamo ora i dati corrispondenti alle rilevazioni di
%     $n$ soggetti su due variabili, per esempio.
%  Tali dati sono solitamente presentati
%      mediante un sistema di assi coordinati laddove il primo asse rappresenta la
% prima variabile e il secondo asse rappresenta la seconda
% variabile. In tale rappresentazione (diagramma a  dispersione), ciascun punto corrisponde ad un soggetto.
% 
%Possiamo per{\`o} invertire il ruolo di variabili e soggetti: in
% questa nuova rappresentazione ciascun soggetto corrisponde ad un
% asse di uno spazio $n$-dimensionale e ciascuno dei due punti in
% questo spazio a $n$ dimensioni corrisponde a una variabile. 
% Con 3 soggetti ($x, y, z$) misurati su due variabili
% $\boldsymbol{a}'=(a_1, a_2, a_3)$ e $\boldsymbol{b}'=(b_1, b_2,
% b_3)$, per esempio, la rappresentazione sarebbe quella indicata nella figura~\ref{fig:2sub3var}.
% \begin{figure}[h!]
% \centering
% \includegraphics[width=5cm]{3d}
% \caption{Due soggetti misurati su tre variabili.}
% \label{fig:2sub3var}
% \end{figure}

%Pu{\`o} essere facilmente dimostrato che la lunghezza dei vettori $\boldsymbol{a}$ e $\boldsymbol{b}$ {\`e} uguale a $(3-1)^{2}s_a$ e $(3-1)^{2}s_b$ [in generale, $(n-1)^{2}s$], dove $s_a$ e $s_b$ sono  le deviazioni standard delle variabili $\boldsymbol{a}$ e
%$\boldsymbol{b}$. Inoltre, il coseno dell'angolo tra i vettori $\boldsymbol{a}$ e $\boldsymbol{b}$ {\`e} uguale al coefficiente di correlazione tra le variabili $\boldsymbol{a}$ e  $\boldsymbol{b}$.


%------------------------------------------------------------
\subsection{Operazioni sui vettori}
%------------------------------------------------------------

\paragraph{Somma e differenza di vettori}

La somma di due vettori {\`e} definita come
\begin{displaymath}
(a_1, a_2) + (b_1, b_2) = (a_1 + b_1, a_2 + b_2).
\end{displaymath}

La  differenza di due vettori {\`e}
\begin{displaymath}
(a_1, a_2) - (b_1, b_2) = (a_1 - b_1, b_2 - b_2).
\end{displaymath}

In termini geometrici:
%-- fig 4
\begin{center}
\setlength{\unitlength}{0.5 cm}
\begin{picture}(15,3)\thicklines
\put(0,0){\vector(1,2){1}} \put(0,0){\vector(1,0){2}}
\put(-0.2,1){$\boldsymbol{a}$} \put(1,-0.6){$\boldsymbol{b}$}
\put(3.2,2){$\boldsymbol{a}+\boldsymbol{b}$}
\put(1,2){\line(1,0){0.2}} \put(0,0){\vector(3,2){3}}
\put(1.4,2){\line(1,0){0.2}} \put(1.8,2){\line(1,0){0.2}}
\put(2.2,2){\line(1,0){0.2}} \put(2.6,2){\line(1,0){0.2}}
\put(8,1){\vector(1,1){2}} \put(8,1){\vector(2,-1){2}}
\put(10,0){\vector(0,1){3}} \put(8.3,1.9){$\boldsymbol{a}$}
\put(8.3,0){$\boldsymbol{b}$}
\put(10.5,1){$\boldsymbol{a}-\boldsymbol{b}$}
\end{picture}
\end{center}

\paragraph{Moltiplicazione scalare}

La moltiplicazione scalare di un vettore per un numero reale (o
scalare) {\`e} data da
\begin{displaymath}
\rho (a_1, a_2) = (\rho a_1, \rho a_2)
\end{displaymath}
Dal punto di vista geometrico, la moltiplicazione scalare effettua
una estensione o contrazione del vettore $\boldsymbol{a}$,
preservandone la direzione.
%-- fig 5
\begin{center}
\setlength{\unitlength}{0.5 cm}
\begin{picture}(15,3)\thicklines
\put(0,0){\vector(2,3){1}} \put(0,1){$\boldsymbol{a}$}
\put(5,1.5){\vector(-2,-3){1}} \put(3.4,1){$-\boldsymbol{a}$}
\put(8,0){\vector(2,3){2}} \put(8,1.7){$2\boldsymbol{a}$}
\end{picture}
\end{center}


\paragraph{Combinazione lineare}

Dati due scalari $\rho_1$ e $\rho_2$ e due vettori $\boldsymbol{a}$, $\boldsymbol{b}$,
 $$
\rho_1 \boldsymbol{a}_1 + \rho_2 \boldsymbol{a}_2
 $$
rappresenta la combinazione lineare dei vettori
$\boldsymbol{a}$ e $\boldsymbol{b}$  con coefficienti $\rho_1$ e
$\rho_2$.
Dati due vettori $\boldsymbol{a}$ e $\boldsymbol{b}$ di dimensione
$n$ definiamo il loro prodotto scalare come la somma dei
prodotti degli elementi che occupano la stessa posizione:
\begin{displaymath}
(a_1, a_2) \cdot (b_1, b_2) = a_1 b_1 + a_2 b_2,
\end{displaymath}
$$
\boldsymbol{a}'\boldsymbol{b} = \sum_{i=1}^{n}a_i b_i.
$$

%------------------------------------------------------------

\paragraph{Ortogonalit{\`a} tra vettori}

Due vettori si dicono ortogonali, e si scrive $\boldsymbol{a}
 \bot \boldsymbol{b}$, se e solo se il loro prodotto scalare {\`e}
nullo:

$$\boldsymbol{a}'\boldsymbol{b} = 0.$$

\paragraph{Norma o lunghezza di un vettore}

Per il teorema di Pitagora, la norma di un vettore $(a_1,
a_2)$ {\`e} $\sqrt{a_1^2 + a_2^2}$ ed {\`e} denotata da $\| (a_1,
a_2) \|$.
Infatti, se un vettore $\boldsymbol{a}$ (l'ipotenusa) {\`e} la somma di
due vettori ortogonali $\boldsymbol{a}_1$ e $\boldsymbol{a}_2$ (i
cateti), allora la lunghezza al quadrato di $\boldsymbol{a}$ {\`e}
uguale alla somma dei quadrati delle lunghezze di
$\boldsymbol{a}_1$ e $\boldsymbol{a}_2$.

Viene detta norma di $\boldsymbol{a}$ la radice del prodotto
scalare di un vettore per se stesso:

$$
\| \boldsymbol{a} \| = \sqrt{\boldsymbol{a}'\boldsymbol{a}}.
$$

 %------------------------------------------------------------

\paragraph{Distanza tra due punti}

È facile calcolare la  distanza tra due punti misurando la
 lunghezza del vettore che li unisce:
 %-- fig 6
 \begin{center}
 \setlength{\unitlength}{0.5 cm}
 \begin{picture}(20,5)\thicklines
 \put(-.5,0){\line(1,0){10.5}} \put(0,-.5){\line(0,1){5.5}}
 \put(3,4){\vector(2,-3){2}} \put(2.6,4.3){$\boldsymbol{a} = (a_1,
 a_2)$} \put(3,4){\circle*{.2}}
 \put(4.7,2.5){$\boldsymbol{b}-\boldsymbol{a}$}
 \put(5,1){\circle*{.2}} \put(5.5,.6){$\boldsymbol{b}=(b_1, b_2)$}
 \put(10.1,3){$\|\boldsymbol{b}-\boldsymbol{a}\|=\|(b_1-a_1,b_2-a_2)\|$}
 \put(12.68,1.8){$=\sqrt{(b_1-a_1)^2+(b_2-a_2)^2}$.}
 \end{picture}
 \end{center}


%------------------------------------------------------------
\section{Matrici}
%------------------------------------------------------------


Una matrice costituisce un insieme rettangolare di scalari
ordinati per riga e colonna. Pu{\`o} anche essere vista come la
raccolta di $m$ vettori colonna di dimensione $n$ o come la
raccolta di $n$ vettori riga di dimensione $m$.
Per esempio:
\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c c}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23} \end{array} \right]
\end{displaymath}


\paragraph{Dimensioni della matrice}

I numeri interi $m$ ed $n$ si dicono dimensioni della
matrice, ovvero $\boldsymbol{A}$ si dice matrice di 
dimensioni $m \times n$ o di ordine $m \times n$. Nel caso
presente, la matrice $\boldsymbol{A}$ ha dimensioni $2 \times 3$.

\paragraph{Matrice quadrata o rettangolare}

Se $m = n$ allora la matrice $\boldsymbol{A}$ si dice quadrata di
dimensione $n$ o di ordine $n$ altrimenti si dice rettangolare.
%\begin{displaymath}
%\boldsymbol{A} =  \left[ \begin{array}{c c c}
%a_{11} & a_{12} & a_{13}\\
%a_{21} & a_{22} & a_{23} \end{array} \right]
%\end{displaymath}
Le righe di $\boldsymbol{A}$ sono $[a_{11}\
a_{12}\ a_{13}]$ e $[a_{21}\ a_{22}\ a_{23}]$.
 Le colonne di $\boldsymbol{A}$ sono $\left[
\begin{array}{c} a_{11} \\ a_{21} \end{array} \right]$, $\left[
\begin{array}{c} a_{12} \\ a_{22} \end{array} \right]$ e $\left[
\begin{array}{c} a_{13} \\ a_{23} \end{array} \right]$.

\paragraph{Diagonale principale}

Se $i$ e $j$ sono numeri interi con $1 \leq i \leq m$ e $1 \leq j
\leq n$ allora l'elemento della matrice $\boldsymbol{A}$ di
dimensione $m \times n$ che si trova in posizione ($i, j$) viene
indicato con $a_{ij}$.
 Gli elementi $a_{ij}$ di una matrice quadrata $\boldsymbol{A}$ di ordine
 $n$ tali che $i = j$ sono detti elementi principali o diagonali e
formano la cosiddetta \emph{diagonale principale} di
$\boldsymbol{A}$.

\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c c}
a_{11} & a_{12} & a_{13}\\
a_{21} &  a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33} \end{array} \right]
\end{displaymath}


%------------------------------------------------------------
\subsection{Matrici speciali}
%------------------------------------------------------------

\paragraph{Matrice diagonale}

Se gli elementi $a_{ij}$ di una matrice quadrata $\boldsymbol{A}$
sono tali che $a_{ij} =0$ e $a_{ii} \neq 0$, allora la matrice
$\boldsymbol{A}$ viene detta \emph{matrice diagonale}.

\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c c}
a_{11} & 0 & 0\\
0 & a_{22} & 0\\
0 & 0 & a_{33} \end{array} \right]
\end{displaymath}

\paragraph{Matrice identità}

Si definisce \emph{matrice identit{\`a}} di ordine $n$ la
matrice quadrata diagonale $\boldsymbol{I}_n$ avente tutti gli
elementi principali uguali a $1$:

\begin{displaymath}
\boldsymbol{I}_3 =  \left[ \begin{array}{c c c}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1 \end{array} \right]
\end{displaymath}
La matrice identit{\`a} ha la stessa funzione del numero ``1'' nel
sistema dei numeri reali.

\paragraph{Matrici diagonali e triangolari}

Gli elementi di una matrice che si trovano al di sopra della
diagonale principale sono detti  \emph{sopradiagonali}, mentre
quelli che si trovano al di sotto della stessa diagonale
principale sono detti \emph{sottodiagonali}.  Se una matrice ha tutti gli elementi
sopradiagonali e sottodiagonali uguali a zero viene detta
\emph{matrice diagonale}.  Se invece ha solo gli elementi sopradiagonali nulli allora
viene detta \emph{triangolare inferiore}. Se ha gli elementi sottodiagonali nulli allora {\`e} detta \emph{triangolare superiore}.

\subsection{Somma e sottrazione}

La somma e la sottrazione di due matrici sono operazioni
    definite \emph{elemento per elemento}.
Per sommare
    due matrici sommiamo gli elementi corrispondenti.
Per sottrarre due matrici sottraiamo gli elementi
    corrispondenti.
Si noti che queste operazioni hanno senso solo se le due
    matrici hanno \emph{le stesse dimensioni} (altrimenti queste
    operazioni non sono definite).
Per esempio,
\begin{displaymath}
\left[ \begin{array}{c c}
-2 & 5\\
3 & 1\\
7 & -6
\end{array}
 \right]+
\left[ \begin{array}{c c}
3 & -2\\
4 & 5\\
10 & -3
\end{array}
 \right]=
 \left[ \begin{array}{c c}
1 & 3\\
7 & 6\\
17 & -9
\end{array}
 \right]
\end{displaymath}

\begin{displaymath}
\left[ \begin{array}{c c}
-2 & 5\\
3 & 1\\
7 & -6
\end{array}
 \right]-
\left[ \begin{array}{c c}
3 & -2\\
4 & 5\\
10 & -3
\end{array}
 \right]=
 \left[ \begin{array}{c c}
-5 & 7\\
-1 & -4\\
-3 & -3
\end{array}
 \right]
\end{displaymath}

\paragraph{Moltiplicazione di scalari e matrici}

L'effetto della moltiplicazione di una matrice $\boldsymbol{A}$ di
    qualsiasi dimensione per un numero reale \emph{b} (scalare) {\`e}
    quello di moltiplicare ciascun elemento in $\boldsymbol{A}$ per \emph{b}. Questo {\`e} equivalente a sommare $\boldsymbol{A}$ a se stessa  \emph{b} volte.
Per esempio,

\begin{displaymath}
3 \left[ \begin{array}{c c}
-2 & 5\\
3 & 1\\
7 & -6
\end{array}
 \right]=
\left[ \begin{array}{c c}
-6 & 15\\
9 & 3\\
21 & -18
\end{array}
 \right]
\end{displaymath}

\paragraph{Propriet{\`a} della somma e differenza}

{\`E} facile verificare che la somma e la differenza cos{\`\i}
definite godono delle propriet{\`a} commutativa e associativa.
 Siano $k$ uno scalare e $A$ e $B$ due matrici aventi le stesse
dimensioni. Allora

\begin{itemize}

\item $\boldsymbol{A}+ \boldsymbol{B} = \boldsymbol{B} +
\boldsymbol{A}$\quad (Propriet{\`a} commutativa)

\item $\boldsymbol{A} + (\boldsymbol{B} + \boldsymbol{C}) =
(\boldsymbol{A} + \boldsymbol{B}) + \boldsymbol{C}$\quad
(Propriet{\`a} associativa)

\item $k(l\boldsymbol{A}) = (kl)\boldsymbol{A}$

\item $k(\boldsymbol{A} + \boldsymbol{B}) = k\boldsymbol{A} +
k\boldsymbol{B}$\quad (Propriet{\`a} distributiva)

\item $(k+l)\boldsymbol{A} = k\boldsymbol{A} + l\boldsymbol{A}$

\item $1\boldsymbol{A} = \boldsymbol{A}$\quad
\end{itemize}

\paragraph{Matrice trasposta}

Si definisce \emph{matrice trasposta} di $\boldsymbol{A}$, e si
denota con $\boldsymbol{A}'$ oppure $\boldsymbol{A}'$, la matrice
$\boldsymbol{B} = \boldsymbol{A}'$ di ordine $n \times m$ cui
elementi sono:
\[
b_{ij} = a_{ji},  \quad        i = 1 \dots m, j = 1 \dots n
\]
Per esempio,
\begin{displaymath}
\left[ \begin{array}{c c}
-2 & 5\\
3 & 1\\
7 & -6
\end{array}
 \right]'=
\left[ \begin{array}{c c c}
-2 & 3 & 7\\
5 & 1 & -6
\end{array}
 \right]
\end{displaymath}

\paragraph{Matrice simmetrica}

Se accade che $\boldsymbol{A} = \boldsymbol{A}'$ allora la matrice
{\`e} detta \emph{simmetrica}.

\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c c}
7 & 1 & 2\\
1 & 8 & 3\\
2 & 3 & 9 \end{array} \right]
\end{displaymath}

\begin{itemize}

\item $(\boldsymbol{A} + \boldsymbol{B})' = (\boldsymbol{A})' +
(\boldsymbol{B})'$\quad

\item $(\boldsymbol{A} - \boldsymbol{B})' = (\boldsymbol{A})' -
(\boldsymbol{B})'$\quad

\item $(\boldsymbol{a} + \boldsymbol{b})' = (\boldsymbol{a})' +
(\boldsymbol{b})'$\quad

\item $(\boldsymbol{a} - \boldsymbol{b})' = (\boldsymbol{a})' -
(\boldsymbol{b})'$\quad

\end{itemize}

\paragraph{Prodotto di matrici}

La moltiplicazione di matrici non {\`e} un'operazione
    intuitiva come la somma e la differenza, ma fornisce uno strumento
    potente per eseguire una lunga serie di calcoli in un modo molto
    semplice.
  L'ordine {\`e} importante: il numero delle colonne della prima
    matrice deve essere uguale al numero di righe della seconda
    matrice.
 Quando ci{\`o} accade le matrici si dicono
    \emph{conformabili}, altrimenti si dicono \emph{non conformabili}.

Sia $\boldsymbol{A}$ una matrice $m \times p$ e $\boldsymbol{B}$
una matrice $p \times n$. Il prodotto tra le due matrici
$\boldsymbol{C} = \boldsymbol{AB}$  {\`e} la matrice di ordine $m
\times n$ il cui elemento generico {\`e}
\[
c_{ij} = \sum_{k=1}^{p} a_{ik}a_{kj},  \quad        i = 1 \dots m,
j = 1 \dots n
\]
Pertanto, il prodotto si effettua riga per colonna.
 {\`E} facile verificare che il prodotto tra matrici gode della
propriet{\`a} associativa ma in generale non di quella
commutativa.
Vale invece la seguente propriet{\`a}:
\[
(\boldsymbol{AB})' = \boldsymbol{B}'\boldsymbol{A}'
\]

Ad esempio, siano $\boldsymbol{A}$ e $\boldsymbol{B}$ le seguenti matrici

\begin{displaymath}
\left[ \begin{array}{c c c}
-2 & 1 & 1\\
1 & 1 & 4\\
2 & -3 & 2
\end{array}
 \right] \quad \text{e} \quad
\left[ \begin{array}{c c c}
3 & -2 &1\\
4 & 5 & 0\\
1 & -3 & 1
\end{array}
 \right]
\end{displaymath}
Calcoliamo la matrice $\boldsymbol{C} = \boldsymbol{AB}$.
L'elemento $c_{ij}$ {\`e} uguale alla somma dei
prodotti degli elementi della \emph{i}-esima riga di
$\boldsymbol{A}$ per la \emph{j}-esima colonna di
$\boldsymbol{B}$. 
Ovvero
\begin{align}
c_{11} &= (-2) \cdot 3 + 1 \cdot 4 + 1 \cdot 1 = -1,\notag\\
c_{12} &= (-2) \cdot (-2) + 1 \cdot 5 + 1 \cdot (-3) = 6,\notag\\
c_{13} &= (-2) \cdot 3 + 1 \cdot 0 + 1 \cdot 1 = -1,\notag\\
c_{21} &= 1 \cdot 3 + 1 \cdot 4 + 4 \cdot 1 = 11,\notag\\
c_{22} &= 1 \cdot (-2) + 1 \cdot 5 + 4 \cdot (-3) = -9,\notag\\
c_{23} &= 1 \cdot 3 + 1 \cdot 0 + 4 \cdot 1 = 5,\notag\\
c_{31} &= 2 \cdot 3 +(-3) \cdot 4 + 2 \cdot 1 = -4,\notag\\
c_{32} &= 2 \cdot (-2) +(-3) \cdot 5 + 2 \cdot (-3) = -25,\notag\\
c_{33} &= 2 \cdot 1 + (-3) \cdot 0 + 2 \cdot 1 = 4.\notag
\end{align}
In definitiva
\begin{displaymath}
\boldsymbol{C} =  \left[ \begin{array}{c c c}
-1 & 6 & -1\\
11 & -9 & 5\\
-4 & -25 & 4
\end{array}
 \right].
\end{displaymath}

Calcolando il prodotto $\boldsymbol{D} = \boldsymbol{BA}$ si trova
invece:
\begin{displaymath}
\boldsymbol{D} =  \left[ \begin{array}{c c c}
-6 & -2 & -3\\
-3 & 9 & 24\\
-3 & -5 & -9
\end{array}
 \right],
\end{displaymath}
da cui risulta evidente che $\boldsymbol{AB} \neq
\boldsymbol{BA}$.

\subsection{Proprietà del prodotto di matrici}

\begin{itemize}
    \item $\boldsymbol{A}(\boldsymbol{B} + \boldsymbol{C}) = \boldsymbol{AB} +
    \boldsymbol{AC}$
    \item $(\boldsymbol{A} + \boldsymbol{B})\boldsymbol{C} = \boldsymbol{AC} +
    \boldsymbol{BC}$
    \item Per qualunque matrice $\boldsymbol{A}$, $\boldsymbol{A}'\boldsymbol{A}$ sar{\`a} una matrice quadrata.
    \item $(\boldsymbol{AB})' = \boldsymbol{B}'\boldsymbol{A}'$
\end{itemize}

\subsection{Casi particolari}

La matrice identit{\`a} {\`e} l'elemento neutro per il prodotto,
cio{\`e} se $\boldsymbol{I}$ {\`e} una matrice $n \times n$ si ha
\[
\boldsymbol{A} \boldsymbol{I}_n = \boldsymbol{I}_n \boldsymbol{A}
= \boldsymbol{A}.
\]
 Per esempio, 
\[
\boldsymbol{IA} = \left(%
\begin{array}{cc}
  1 & 0 \\
  0 & 1 \\
\end{array}%
\right)
\left(%
\begin{array}{ccc}
  2 & 3 & -1 \\
  1 & 4 & 7 \\
\end{array}%
\right)=
\left(%
\begin{array}{ccc}
  2 & 3 & -1 \\
  1 & 4 & 7 \\
\end{array}%
\right)
\]

Un secondo caso particolare si verifica quando una matrice {\`e}
costituita da un'unica colonna o un'unica riga.
Se la matrice $\boldsymbol{A}$  si riduce ad una sola colonna (o
una sola riga) e viene detta vettore colonna (o riga) ad $m$
elementi o componenti.
 Un  vettore colonna {\`e} una matrice $n \times 1$; un  vettore riga {\`e}
una matrice $1 \times m$.
Se $\boldsymbol{a}$ {\`e} un vettore colonna di $m$ elementi allora
$\boldsymbol{a}'$ {\`e} un vettore riga sempre di $m$ elementi.

Per le operazioni tra vettori valgono le stesse regole viste per le matrici, cio{\`e} la somma e la differenza sono possibili tra vettori dello stesso tipo e con lo stesso numero di componenti. La moltiplicazione {\`e} possibile tra una matrice e un
vettore di dimensioni appropriate, e tra due vettori di dimensioni
appropriate. In questo secondo caso, distinguiamo tra \emph{prodotto interno} e \emph{prodotto esterno}.

\subsection{Operazioni tra vettori}

Il \emph{prodotto interno} (o scalare) di un vettore
$\boldsymbol{a}'$ $1 \times n$ che premoltiplica un vettore
$\boldsymbol{b}$ $n \times 1$ produce uno scalare:
$$
\boldsymbol{a}'\boldsymbol{b} = \sum_{i=1}^{n}a_i b_i
$$

Dati due vettori $\boldsymbol{a}$, $\boldsymbol{b}$ di ordini $n
\times 1$ e $m \times 1$, il \emph{prodotto esterno}
$\boldsymbol{C} = \boldsymbol{ab}'$ {\`e} una matrice $n \times m$ di
elementi $c_{ij} = a_i b_j$.

\subsection{Prodotto interno}

Siano $\boldsymbol{a}$ e $\boldsymbol{b}$ i seguenti vettori:
\begin{displaymath}
\left[ \begin{array}{c}
1 \\
2 \\
3
\end{array}
 \right] \quad e \quad
\left[ \begin{array}{c}
-1 \\
-2 \\
4
\end{array}
 \right]
\end{displaymath}

Il prodotto interno {\`e}:
\[
\boldsymbol{a}'\boldsymbol{b}= 1 \cdot (-1) + 2 \cdot (-2) + 3
\cdot 4 = 7
\]
Osserviamo che tale operazione gode della propriet{\`a}
commutativa, poich{\`e} $\boldsymbol{b}'\boldsymbol{a}=7$.

\subsection{Prodotto esterno}

Il prodotto esterno {\`e} la matrice
\begin{displaymath}
\boldsymbol{C} = \boldsymbol{a}\boldsymbol{b}'= \left[
\begin{array}{c c c}
-1 & -2 & 4\\
-2 & -4 & 8\\
-3 & -6 & 12
\end{array}
 \right]
\end{displaymath}
Tale prodotto non gode della propriet{\`a}
commutativa, infatti:
\begin{displaymath}
\boldsymbol{D} = \boldsymbol{b}\boldsymbol{a}'= \left[
\begin{array}{c c c}
-1 & -2 & -3\\
-2 & -4 & -6\\
4 & 8 & 12
\end{array}
 \right]
\end{displaymath}

\subsection{Traccia di una matrice}

Si definisce \emph{traccia} di una matrice quadrata $\boldsymbol{A}$ $n \times n$, e si denota con $tr(\boldsymbol{A})$ la somma degli elementi sulla diagonale principale di $\boldsymbol{A}$:
\[
tr(\boldsymbol{A}) = \sum_{i=1}^{n} a_{ii}
\]

La traccia gode delle seguenti propriet{\`a}:
\begin{align}
&tr(\rho \boldsymbol{A}) = \rho tr( \boldsymbol{A}) \notag \\
&tr(\boldsymbol{A} + \boldsymbol{B}) =  tr( \boldsymbol{A})+tr( \boldsymbol{B}) \notag \\
&tr(\boldsymbol{A}') =  tr( \boldsymbol{A}) \notag \\
&tr(\boldsymbol{AB}) =  tr( \boldsymbol{BA}) \notag
\end{align}
Per esempio, sia
\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c c}
7 & 1 & 2\\
1 & 8 & 3\\
2 & 3 & 9 \end{array} \right]
\end{displaymath}
allora
\[
tr(\boldsymbol{A}) = 7 + 8 + 9 = 24.
\]

\subsection{Dipendenza lineare}

Si consideri la matrice
    \[
\boldsymbol{A}=
\left(%
\begin{array}{ccc}
  1 & 1 & 1 \\
  3 & 1 & 5 \\
  2 & 3 & 1 \\
\end{array}%
\right)
\]
 Siano  $\boldsymbol{c}_1$,  $\boldsymbol{c}_2$, $\boldsymbol{c}_3$ le
colonne di $\boldsymbol{A}$.
Si noti che
\[
2\boldsymbol{c}_1 + -\boldsymbol{c}_2 + - \boldsymbol{c}_3 =
\boldsymbol{0}
\]
dove  $\boldsymbol{0}$ {\`e} un vettore ($3 \times 1$) di zeri.

Dato che le 3 colonne di $\boldsymbol{A}$ possono essere combinate
linearmente in modo da produrre un vettore $\boldsymbol{0}$ vi {\`e}
chiaramente una qualche forma di relazione, o dipendenza, tra le
informazioni nelle colonne.  Detto in un altro modo, sembra esserci
una qualche duplicazione delle informazione nelle colonne.  In
generale, si dice che $k$ colonne $\boldsymbol{c}_1, \boldsymbol{c}_2,
\dots \boldsymbol{c}_k$ di una matrice sono \emph{linearmente
  dipendenti} se esiste un insieme di valori scalari $\lambda_1,
\dots, \lambda_k$ tale per cui
    \[
    \lambda_1 \boldsymbol{c}_1 + \dots + \lambda_k \boldsymbol{c}_k=\boldsymbol{0}
    \]
    e almeno uno dei valori $\lambda_i$ non {\`e} uguale a 0.

La dipendenza lineare implica che ciascun vettore
    colonna {\`e} una combinazione degli altri.  Per esempio
    \[
   \boldsymbol{c}_k= -(\lambda_1 \boldsymbol{c}_1 + \dots + \lambda_{k-1}
   \boldsymbol{c}_{k-1})/\lambda_k
    \]
    Questo implica che tutta ``l'informazione'' della
    matrice {\`e} contenuta in un sottoinsieme delle colonne -- se
    $k-1$ colonne sono conosciute, l'ultima resta determinata.
È in questo senso che abbiamo detto che l'informazione della  matrice veniva ``duplicata''.

Se l'unico insieme di valori scalari $\lambda_i$ che
    soddisfa l'equazione
    \[
    \lambda_1 \boldsymbol{c}_1 + \dots + \lambda_k \boldsymbol{c}_k=\boldsymbol{0}
    \]
     {\`e} un vettore di zeri, allora
    questo significa che non vi {\`e} alcuna relazione tra le colonne
    della matrice.
  Le colonne si dicono \emph{linearmente indipendenti},
   nel senso che non contengono alcuna ``duplicazione'' di informazione.


\subsection{Rango di una matrice}

Il \emph{rango della matrice} {\`e} il massimo numero
    di vettori colonna linearmente indipendenti che possono essere
    selezionati dalla matrice.
 In maniera equivalente, il rango di una matrice pu{\`o}
    essere definito come il massimo numero di vettori riga linermente
    indipendenti.
 Il rango minimo di una matrice {\`e} 1, il che significa che vi {\`e}
     una colonna tale per cui le altre colonne sono dei multipli
     di questa. Per l'esempio precedente, il rango della matrice $\boldsymbol{A}$ {\`e} 2.

Se la matrice {\`e} quadrata, $\boldsymbol{A}_{n \times n}$, ed
{\`e} costituita da vettori tutti indipendenti tra di loro, allora
il suo rango {\`e} $n$.
Se, invece, la matrice {\`e} rettangolare, $\boldsymbol{A}_{m
\times n}$, allora il suo rango pu{\`o} essere al massimo il
pi{\`u} piccolo tra i due valori \emph{m} ed \emph{n}, cio{\`e}:

\[
  r(\boldsymbol{A}_{m \times n}) \leq min(m,n)
\]

\subsection{Matrice inversa}

 L'inversa di una matrice quadrata {\`e} l'analogo del reciproco per
     gli scalari.
Se $b$ {\`e} uno scalare e $b=0$, allora il reciproco di
     $b$, $1/b$ \emph{non esiste} -- non {\`e} definito. Allo stesso modo, vi sono delle matrici che ``si comportano come lo
     0'' e per le quali l'inversa non {\`e} definita. Tali matrici si dicono  \emph{singolari}.

Sia $\boldsymbol{A}$ una matrice quadrata di dimensione $n$.
Si definisce \emph{matrice inversa} la matrice,
 denotata con $\boldsymbol{A}^{-1}$, che premoltiplicata o postmoltiplicata per
 $\boldsymbol{A}$
fornisce la matrice identit{\`a}:
     \[
         \boldsymbol{A}\boldsymbol{A}^{-1}=\boldsymbol{A}^{-1}\boldsymbol{A}=\boldsymbol{I}
     \]

 La condizione per l'esistenza e l'unicit{\`a} di
$\boldsymbol{A}^{-1}$ {\`e} che le colonne di $\boldsymbol{A}$
siano linearmente indipendenti.

 Nel caso di una matrice diagonale la determinazione della matrice inversa risulta
     immediata: $\boldsymbol{D}^{-1}= diag(1/d_1, \dots, 1/d_n)$.
 Nel caso di una matrice non diagonale, la matrice inversa si trova usando il computer
     dove complicate formule per matrici di qualunque dimensione
     sono implementate in vari software.
 Solo per matrici di piccole dimensioni sono disponibili semplici
     espressioni analitiche per il calcolo della matrice inversa.
     
Per esempio, sia
\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c}
3 & 4 \\
2 & 6
\end{array}
 \right]
\end{displaymath}
allora
\begin{displaymath}
\boldsymbol{A}^{-1} =  \left[ \begin{array}{c c}
.6 & -.4 \\
-.2 & .3
\end{array}
 \right]
\end{displaymath}
e
\begin{displaymath}
\boldsymbol{A}\boldsymbol{A}^{-1} =\left[ \begin{array}{c c}
3 & 4 \\
2 & 6
\end{array}
 \right]
\left[ \begin{array}{c c}
.6 & -.4 \\
-.2 & .3
\end{array}
 \right] =
 \left[ \begin{array}{c c}
1 & 0 \\
0 & 1
\end{array}
 \right]
\end{displaymath}

Se $\boldsymbol{A}$ e $\boldsymbol{B}$ sono due  matrici non
singolari aventi le stesse dimensioni, allora l'inversa del loro
prodotto {\`e} uguale al prodotto delle loro inverse nella
sequenza opposta:
$$(\boldsymbol{AB})^{-1}=\boldsymbol{B}^{-1}\boldsymbol{A}^{-1}$$

L'inversa della trasposta di una matrice non singolare
{\`e} uguale alla trasposta dell'inversa:

    $$(\boldsymbol{A}')^{-1}=(\boldsymbol{A}^{-1})'$$

\subsection{Determinante di una matrice}

 Sia $\boldsymbol{A}$ una matrice quadrata.
Il determinante di $\boldsymbol{A}$ {\`e} uno scalare, $|\boldsymbol{A}|$, il cui
valore assoluto misura il volume del parallelepipedo delimitato
dalle colonne di $\boldsymbol{A}$.
 Nel caso della matrice identit{\`a} il volume {\`e} pari a 1, per
cui $|\boldsymbol{I}| =1$.
 Per una matrice diagonale $\boldsymbol{D} = diag(d_1, \dots,
d_n)$ si ha

$$
|\boldsymbol{D}| =  d_1 \cdot d_2, \dots, d_n = \prod_{i=1}^{n}d_i
$$

Per una matrice $2 \times 2$
\begin{displaymath}
\boldsymbol{A} =  \left[ \begin{array}{c c}
a_{11}& a_{12} \\
a_{21} & a_{22} \end{array} \right]
\end{displaymath}
il determinante di $\boldsymbol{A}$ vale:
\[
 |\boldsymbol{A}| =  a_{11}a_{22}-a_{12}a_{21}
\]

 Per esempio:
\[
 \boldsymbol{A} = \left[ \begin{array}{c c}
1 & -2 \\
3 & 9
\end{array}
 \right] \quad |\boldsymbol{A}| = 1\cdot 9 - (-2) \cdot  3 = 15
\]

 Il determinante {\`e} definito anche per matrici di
dimensioni superiori anche se, in quel caso, i calcoli sono molto
pi{\`u} complessi (una volta ancora, si usi il computer!).

\subsection{Determinante e inversa}

 Vi {\`e} una relazione tra il determinante e l'inversa di
    una matrice.
     Se la matrice $\boldsymbol{A}$ ha dimensioni $2 \times
    2$ l'inversa di $\boldsymbol{A}$ si trova nel modo seguente
\[
\boldsymbol{A}^{-1} = \frac{1}{|\boldsymbol{A}|} \left[
\begin{array}{c c}
a_{22} & -a_{12} \\
-a_{21} & a_{11}
\end{array}
 \right]
\]
 Anche per le matrici di dimensioni maggiori la matrice
    inversa {\`e} definita nei termini del determinante, ma le
    formule di calcolo sono molto pi{\`u} complesse.

 Per esempio, sia
\begin{displaymath}
\boldsymbol{A} = \left[ \begin{array}{c c}
3 & 4 \\
2 & 6
\end{array}
\right]
\end{displaymath}
allora
\begin{displaymath}
\boldsymbol{A}^{-1} = \frac{1}{10} \left[
\begin{array}{c c}
6 & -4 \\
-2 & 3
\end{array}
\right]= \left[ \begin{array}{c c}
.6 & -.4 \\
-.2 & .3
\end{array}
 \right]
\end{displaymath}
 In precedenza abbiamo detto che, in alcuni casi, una
     matrice ``si comporta come lo 0.''
 Il \emph{determinante} di una matrice {\`e} ci dice quando una
     matrice ``si comporta come lo 0.''
$|\boldsymbol{A}| = 0$, infatti, se una riga (o una colonna) {\`e}
    una combinazione lineare di due (o pi{\`u}) righe (o colonne) di $\boldsymbol{A}$.

Per esempio, nel caso di una matrice ($2 \times 2$)

\begin{displaymath}
\boldsymbol{A} =  \left( \begin{array}{c c}
a_{11}& a_{12} \\
a_{21} & a_{22} \end{array} \right)
\end{displaymath}
 supponiamo che
\[
\left(%
\begin{array}{c}
  a_{11} \\
  a_{21} \\
\end{array}%
\right)=2
\left(%
\begin{array}{c}
  a_{12} \\
  a_{22} \\
\end{array}%
\right)
\]
 Allora
\begin{displaymath}
\boldsymbol{A} =  \left( \begin{array}{c c}
2a_{12}& a_{12} \\
2a_{22} & a_{22} \end{array} \right)
\end{displaymath}
e
\[
 |\boldsymbol{A}| = 2a_{12}a_{22}-2a_{12}a_{22}=0
\]
 In conclusione, se il determinante {\`e} uguale a zero, allora
la matrice inversa non esiste.
 Nel caso di una matrice ($2 \times 2$), infatti, la formula
dell'inversa richiede la divisione per $a_{11}a_{22}-a_{12}a_{21}$
che, nel caso di una matrice singolare, {\`e} uguale a zero.

\subsection{Proprietà del determinante}

\begin{itemize}
    \item $|\boldsymbol{A}'| = |\boldsymbol{A}|$.
    \item Se $\boldsymbol{A}$ contiene una colonna o una riga i cui
    elementi sono tutti 0, allora $|\boldsymbol{A}|=0$.
    \item Se $\boldsymbol{A}$ contiene due colonne (o righe) identiche, allora $|\boldsymbol{A}|=0$.
    \item $|\boldsymbol{A}| = 0$ se una riga (o una colonna) {\`e} combinazione lineare di
    due (o pi{\`u}) righe (o colonne) di $\boldsymbol{A}$.
    \item $|\boldsymbol{A}| = 1/|\boldsymbol{A}^{-1}|$.
    \item $|\boldsymbol{I}| = 1$.
    \item $|\boldsymbol{A} \boldsymbol{B}| = |\boldsymbol{A}| |\boldsymbol{B}|$.

\end{itemize}

%Una matrice si dice ortogonale se la matrice inversa {\`e}
%uguale alla trasposta:
%$$
%\boldsymbol{A}'\boldsymbol{A} = \boldsymbol{I}, \qquad
%\boldsymbol{AA}' = \boldsymbol{I}.
%$$
%Se $\boldsymbol{A}$ {\`e} una matrice triangolare
%o diagonale allora
%\[
%|\boldsymbol{A}| = \prod_{i=1}^{n} a_{ii}.
%\]
 Per una matrice quadrata $\boldsymbol{A}$, le seguenti
affermazioni sono equivalenti:
 $\boldsymbol{A}$ {\`e} non singolare,
    $|\boldsymbol{A}|\neq 0$,
   $\boldsymbol{A}^{-1}$ esiste.



\subsection{Radici e vettori latenti}

Dal determinante di una matrice si possono ricavare le \emph{radici latenti} o \emph{autovalori} (denotati da $\lambda_i$) e i \emph{vettori latenti} o \emph{autovettori} della matrice. Alle nozioni di autovalore e autovettore verrà qui fornita un'interpretazione geometrica.

Simuliamo i dati di due variabili associate tra loro:

\begin{lstlisting}
library("car")
set.seed(123456)

npoints <- 20
x <- as.numeric(scale(rnorm(npoints, 0, 1)))
y <- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))
mean(x)
#[1] 1.076511e-17
mean(y)
#[1] -1.872959e-17
cor(x,y)
#[1] 0.8291033
\end{lstlisting}
Disegnamo il diagramma di dispersione con un ellisse che contiene la nube di punti:
\begin{lstlisting}
car::dataEllipse(
  Y[, 1], Y[, 2],
  levels = 0.95,
  lty = 2,
  ylim = c(-3, 3),
  xlim = c(-3, 3)
)
\end{lstlisting}

\begin{center}
 \includegraphics[width=9cm]{eli1}
\end{center}
Se racchiudiamo le osservazioni ($v_1, v_2$) con un'ellisse, allora la lunghezza dei semiassi maggiori e minori dell'ellisse sarà proporzionale a $\sqrt{\lambda_1}$ e $\sqrt{\lambda_2}$.
L'asse maggiore è la linea passante per il punto ($\bar{v_1}, \bar{v_2}$) nella direzione determinata dal primo autovettore $\boldsymbol{a}_1'$ con pendenza uguale a $a_{12}/a_{11}$. 
L'asse minore è la linea passante per il punto ($\bar{v_1}, \bar{v_2}$) nella direzione determinata dal secondo autovettore $\boldsymbol{a}_2$.

Calcoliamo ora gli autovettori e gli autovalori:
\begin{lstlisting}
s <- cov(Y)
ee <- eigen(s)
\end{lstlisting}
Disegniamo gli assi dell'ellisse:
\begin{lstlisting}
k <- 2.65

arrows(
  0, 0, 
  k * sqrt(ee$values[1]) * ee$vectors[1],
  k * sqrt(ee$values[1]) * ee$vectors[2],
  code = 2, 
  col = "red", 
  lwd = 2
)

arrows(
  0, 0, 
  k * sqrt(ee$values[2]) * ee$vectors[1],
  k * sqrt(ee$values[2]) * -ee$vectors[2],
  code = 2, 
  col = "red", 
  lwd = 2
)
\end{lstlisting}

\begin{center}
  \includegraphics[width=9cm]{eli2}
\end{center}

Tale analisi si può estendere a qualunque numero di variabili.
Per esempio, nel caso di tre variabili, possiamo pensare di
disegnare un ellisoide attorno ad una nube di punti nello spazio
tridimensionale. Anche in questo caso, gli autovalori e gli
associati autovettori  corrisponderanno agli assi dell'elissoide.

\subsection{Scomposizione spettrale di una matrice}

 Data una matrice  quadrata e simmetrica  di dimensione $n$,
    $\boldsymbol{A}$,  esistono una matrice diagonale $\boldsymbol{\Lambda}$ e una matrice
    ortogonale $\boldsymbol{V}$ tali che
    \[
     \boldsymbol{A} =\boldsymbol{V} \boldsymbol{\Lambda} \boldsymbol{V}',
    \]
dove 
\begin{itemize}
    \item $\boldsymbol{\Lambda}$ {\`e} una matrice diagonale i cui elementi sono gli autovalori
    di $\boldsymbol{A}$: $\boldsymbol{\Lambda} = diag(\lambda_1, \lambda_2,
    \dots, \lambda_n)$;
    \item $\boldsymbol{V}$ {\`e} una matrice ortogonale le cui colonne $(v_1, v_2, \dots, v_p)$ sono gli autovettori
    di $\boldsymbol{A}$ associati ai rispettivi autovalori.
\end{itemize}
 In maniera equivalente
    \[
     \boldsymbol{A} \boldsymbol{V} =  \boldsymbol{\Lambda} \boldsymbol{V}'.
    \]
 Premoltiplicando entrambi i membri per $\boldsymbol{V}'$ si
ottiene
$$
\boldsymbol{V}'\boldsymbol{A} \boldsymbol{V} =
\boldsymbol{\Lambda},
$$
da cui l'affermazione che la matrice degli autovettori
diagonalizza $\boldsymbol{A}$.

Per esempio, 
\begin{lstlisting}
sigma <- matrix(data = c(1, 0.5, 0.5, 1.25), nrow = 2, ncol = 2)
sigma
#>     [,1] [,2]
#>[1,]  1.0 0.50
#>[2,]  0.5 1.25
out <- eigen(sigma)
out
#>$values
#>[1] 1.6403882 0.6096118
#>
#>$vectors
#>          [,1]       [,2]
#>[1,] 0.6154122  0.7882054
#>[2,] 0.7882054 -0.6154122

Lambda <- diag(out$values)
Lambda
#>         [,1]      [,2]
#>[1,] 1.640388 0.0000000
#>[2,] 0.000000 0.6096118
U <- out$vectors
U
#>          [,1]       [,2]
#>[1,] 0.6154122  0.7882054
#>[2,] 0.7882054 -0.6154122
U %*% Lambda %*% t(U)
#>     [,1] [,2]
#>[1,]  1.0 0.50
#>[2,]  0.5 1.25
\end{lstlisting}

\subsection{Autovalori e determinante}

 Il determinante di una matrice {\`e} il prodotto degli autovalori:
    \begin{align}
    |\boldsymbol{A}| &= \prod_{i=1}^{p} \lambda_i. \notag
    \end{align}

 La traccia di una matrice {\`e} uguale alla somma
degli autovalori:
   \begin{align}
    tr(\boldsymbol{A}) &= \sum_{i=1}^{p} \lambda_i. \notag
    \end{align}

\begin{lstlisting}
sigma <- matrix(data = c(1, 0.5, 0.5, 2), nrow = 2, ncol = 2)
sigma
#>     [,1] [,2]
#>[1,]  1.0  0.5
#>[2,]  0.5  2.0
out <- eigen(sigma)
out
#>$values
#>[1] 2.2071068 0.7928932
#>
#>$vectors
#>          [,1]       [,2]
#>[1,] 0.3826834  0.9238795
#>[2,] 0.9238795 -0.3826834
\end{lstlisting}

La traccia di una matrice è uguale alla somma degli autovalori:

\begin{lstlisting}
sum(out$values)
#>[1] 3
\end{lstlisting}
%$
Il determinante di una matrice è il prodotto degli autovalori:
\begin{lstlisting}
det(sigma)
#>[1] 1.75
out$values[1] * out$values[2]
#>[1] 1.75
\end{lstlisting}

Gli autovalori di $\boldsymbol{A}^{-1}$ sono i reciproci
degli autovalori di $\boldsymbol{A}$; gli autovettori sono coincidenti.

%------------------------------------------------------------



% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Un vettore viene definito mediante la funzione {\tt c()}:

% \begin{lstlisting}
% > y <- c(-5, 3, 1, 4)
% > y
% [1] -5  3  1  4

% > z <- matrix(y, 4, 1)
% > z
%      [,1]
% [1,]   -5
% [2,]    3
% [3,]    1
% [4,]    4
% \end{lstlisting}



% \end{frame}

% %------------------------------------------------------------


% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Una matrice viene definita nel modo seguente:

% \begin{lstlisting}
% > X <- matrix(c(1, -2,  3,
% +               4, -5, -6,
% +               7,  8,  9,
% +               0,  0, 10),
% +              4, 3, byrow=TRUE)
% > X
%      [,1] [,2] [,3]
% [1,]    1   -2    3
% [2,]    4   -5   -6
% [3,]    7    8    9
% [4,]    0    0   10

% > t(X)  # trasposta di una matrice
%      [,1] [,2] [,3] [,4]
% [1,]    1    4    7    0
% [2,]   -2   -5    8    0
% [3,]    3   -6    9   10
% \end{lstlisting}

% \end{frame}

% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Matrice diagonale:

% \begin{lstlisting}
% > diag(c(6, -2, 0, 7))  
%      [,1] [,2] [,3] [,4]
% [1,]    6    0    0    0
% [2,]    0   -2    0    0
% [3,]    0    0    0    0
% [4,]    0    0    0    7
% \end{lstlisting}

% Matrice identità:

% \begin{lstlisting}
% > diag(3)  
%      [,1] [,2] [,3]
% [1,]    1    0    0
% [2,]    0    1    0
% [3,]    0    0    1
% \end{lstlisting}


% \end{frame}

% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}


% Matrice nulla: 

% \begin{lstlisting}
% > matrix(0, 4, 3)  
%      [,1] [,2] [,3]
% [1,]    0    0    0
% [2,]    0    0    0
% [3,]    0    0    0
% [4,]    0    0    0
% \end{lstlisting}

% Vettore unitario:

% \begin{lstlisting}
% > rep(1, 4)  
% [1] 1 1 1 1
% \end{lstlisting}
% \end{frame}


% %------------------------------------------------------------


% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Operazioni sulle matrici:

% \begin{lstlisting}
% > A <- matrix(1:6, 2, 3, byrow=TRUE)
% > A
%      [,1] [,2] [,3]
% [1,]    1    2    3
% [2,]    4    5    6
% > B <- matrix(c(-5, 1,  2,
% +                3, 0, -4),
% +             2, 3, byrow=TRUE)
% > B
%      [,1] [,2] [,3]
% [1,]   -5    1    2
% [2,]    3    0   -4
% \end{lstlisting}



% \end{frame}


% %------------------------------------------------------------


% \begin{frame}[fragile]{Matrici in \textbf{R}}


% Somma:

% \begin{lstlisting}
% > A + B  
%      [,1] [,2] [,3]
% [1,]   -4    3    5
% [2,]    7    5    2
% \end{lstlisting}

%  Differenza di matrici

% \begin{lstlisting}
% > A - B  
%      [,1] [,2] [,3]
% [1,]    6    1    1
% [2,]    1    5   10
% \end{lstlisting}

% \end{frame}

% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Prodotto interno:

% \begin{lstlisting} 
% > a <- c(2, 0, 1, 3)
% > b <- c(-1, 6, 0, 9)
% >
% > a %*% b   
%      [,1]
% [1,]   25
% \end{lstlisting}

% Prodotto esterno:

% \begin{lstlisting}
% > a %o% b  
%      [,1] [,2] [,3] [,4]
% [1,]   -2   12    0   18
% [2,]    0    0    0    0
% [3,]   -1    6    0    9
% [4,]   -3   18    0   27
% \end{lstlisting}
% \end{frame}

% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Prodotto di matrici:

% \begin{lstlisting}
% > A <- matrix(1:4, 2, 2, byrow=TRUE)
% > A
%      [,1] [,2]
% [1,]    1    2
% [2,]    3    4
% >
% > B <- matrix(c(0, 3,  2, 1), 2, 2, byrow=TRUE)
% > B
%      [,1] [,2]
% [1,]    0    3
% [2,]    2    1
% \end{lstlisting}

% \end{frame}


% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Prodotto di matrici:

% \begin{lstlisting}
% > A %*% B  # prodotto di matrici
%      [,1] [,2]
% [1,]    4    5
% [2,]    8   13
% >
% > B %*% A
%      [,1] [,2]
% [1,]    9   12
% [2,]    5    8
% \end{lstlisting}

% \end{frame}


% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Consideriamo un altro esempio:

% \begin{lstlisting}
% > C <- matrix(1:6, 2, 3, byrow=TRUE)
% > C
%      [,1] [,2] [,3]
% [1,]    1    2    3
% [2,]    4    5    6
% > I <- diag(3)
% > I
%      [,1] [,2] [,3]
% [1,]    1    0    0
% [2,]    0    1    0
% [3,]    0    0    1
% > C %*% I
%      [,1] [,2] [,3]
% [1,]    1    2    3
% [2,]    4    5    6
% > I %*% C
% Error in I %*% C : non-conformable arguments
% \end{lstlisting}

% \end{frame}

% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Inversa di una matrice:

% \begin{lstlisting}
% > A <- matrix(c(2, 5,  1, 3), 2, 2, byrow=TRUE)
% > A
%      [,1] [,2]
% [1,]    2    5
% [2,]    1    3
% > solve(A)  # inversa di una matrice
%      [,1] [,2]
% [1,]    3   -5
% [2,]   -1    2
% > solve(A) %*% A
%      [,1] [,2]
% [1,]    1    0
% [2,]    0    1
% > A %*% solve(A)
%      [,1] [,2]
% [1,]    1    0
% [2,]    0    1

% \end{lstlisting}

% \end{frame}

% %------------------------------------------------------------

% \begin{frame}[fragile]{Matrici in \textbf{R}}

% Soluzione di $Ax = b$

% \begin{lstlisting}
% > A
%      [,1] [,2]
% [1,]    2    5
% [2,]    1    3

% > b <- c(4, 5)
% > solve(A) %*% b  # soluzione di Ax = b
%      [,1]
% [1,]  -13
% [2,]    6
% >
% > solve(A, b)  # soluzione equivalente
% [1] -13   6
% \end{lstlisting}
% \end{frame}








