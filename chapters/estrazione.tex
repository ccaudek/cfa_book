\chapter{L'estrazione dei fattori}
\label{ch:estrazione}


\section*{Motivazione}

Lo scopo dell'analisi fattoriale è quello di descrivere in maniera parsimoniosa le relazioni che intercorrono tra un grande numero di item. Ci si chiede se è possibile identificare un piccolo numero di variabili latenti che, quando vengono controllate, rendono uguali a zero le correlazioni parziali tra gli item. Abbiamo visto nel capitolo precedente come sia possibile determinare il numero dei fattori comuni. Chiediamoci ora come sia possibile stimare le saturazioni fattoriali che, come indicato nel capitolo~\ref{ch:mod_unifattoriale}, corrispondono alle correlazioni (o covarianze) tra gli item e i fattori.

In termini matriciali, il modello multifattoriale  si scrive
\begin{align}
\boldsymbol{\Sigma} &=\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\ensuremath{\mathsf{T}}} + \boldsymbol{\Psi} \notag
\end{align}
dove $\boldsymbol{\Phi}$ è la matrice di ordine $m \times m$ di varianze e covarianze tra i fattori comuni e $\boldsymbol{\Psi}$ è una matrice diagonale  di ordine $p$ con le unicità delle variabili. Poniamoci ora il problema di stimare $\boldsymbol{\Lambda}$.


\section{Metodo delle componenti principali}

L'analisi fattoriale eseguita mediante il metodo  delle
    componenti principali, nonostante il nome, non è un'analisi
    delle componenti principali. Il metodo delle componenti principali costituisce invece un'applicazione del teorema di scomposizione spettrale di una matrice.
Il \emph{teorema spettrale} afferma che ``data la matrice simmetrica $\textbf{S}_{p \times p}$, 
è sempre possibile trovare una matrice $\textbf{C}_{p \times p}$ ortogonale tale che 
\begin{equation}
\textbf{S} = \textbf{C}\textbf{D}\textbf{C}^{\ensuremath{\mathsf{T}}}
\end{equation}
con \textbf{D} diagonale.''  
 Il teorema specifica inoltre che gli elementi presenti sulla diagonale 
di  \textbf{D} sono gli autovalori di  \textbf{S}, mentre le colonne di  \textbf{C} rappresentano i 
rispettivi autovettori normalizzati associati agli autovalori di
\textbf{S}. 

Facciamo un esempio numerico utilizzando i dati discussi da
    Rencher(2002).  
 Brown, Williams e Barlow (1984) hanno raccolto le valutazioni di
  una ragazza dodicenne relativamente a sette persone di sua
  conoscenza. 
  Ciascuna persona  veniva valutata su una scala a nove punti
  rispetto a cinque variabili: {\it kind}, {\it intelligent}, {\it
    happy}, {\it likeable} e {\it just}. 
  La matrice di correlazione per tali variabili è riportata di seguito:

\begin{lstlisting}
R <- matrix(c(
   1.000, .296, .881, .995, .545,
   .296, 1.000, -.022, .326, .837,
   .881, -.022, 1.000, .867, .130,
   .995, .326, .867, 1.000, .544,
   .545, .837, .130, .544, 1.000), 
      ncol=5, byrow=T, dimnames = list(
 c("K", "I", "H", "L", "J"),
 c("K", "I", "H", "L", "J")))
R
#>       K      I      H     L     J
#> K 1.000  0.296  0.881 0.995 0.545
#> I 0.296  1.000 -0.022 0.326 0.837
#> H 0.881 -0.022  1.000 0.867 0.130
#> L 0.995  0.326  0.867 1.000 0.544
#> J 0.545  0.837  0.130 0.544 1.000
\end{lstlisting}
Gli autovalori e gli autovettori si calcolano con la funzione {\tt eigen()}:
\medskip
\begin{lstlisting}
e <- eigen(R)
print(e, 3)
#> $values
#> [1] 3.263377 1.538382 0.167969 0.030030 0.000242
#> 
#> $vectors
#>        [,1]   [,2]    [,3]   [,4]   [,5]
#> [1,] -0.537 -0.186 -0.1899 -0.125  0.791
#> [2,] -0.287  0.651  0.6849 -0.120  0.103
#> [3,] -0.434 -0.474  0.4069  0.614 -0.212
#> [4,] -0.537 -0.169 -0.0953 -0.629 -0.527
#> [5,] -0.390  0.538 -0.5658  0.444 -0.204
\end{lstlisting}
Come indicato in precedenza, la matrice \textbf{R} può essere espressa come $\textbf{R} = \textbf{C}\textbf{D}\textbf{C}^{\ensuremath{\mathsf{T}}}$:
\medskip
\begin{lstlisting}
e$vectors %*% diag(e$values) %*% t(e$vectors)
#>       [,1]   [,2]   [,3]  [,4]  [,5]
#> [1,] 1.000  0.296  0.881 0.995 0.545
#> [2,] 0.296  1.000 -0.022 0.326 0.837
#> [3,] 0.881 -0.022  1.000 0.867 0.130
#> [4,] 0.995  0.326  0.867 1.000 0.544
#> [5,] 0.545  0.837  0.130 0.544 1.000
\end{lstlisting}
Esaminiamo ora gli autovalori.  I primi due autovalori spiegano da soli il 96\% della varianza campionaria: 
\medskip
\begin{lstlisting}
(e$values[1] + e$values[2])/5
#> [1] 0.9603517
\end{lstlisting}
Usando i primi due autovalori e i primi due autovettori sarà dunque
possibile riprodurre in maniera soddisfacente la matrice \textbf{R}
operando nel contempo una riduzione di dimensionalità dei dati. 

Per fattorizzare $\textbf{R} = \textbf{C}\textbf{D}\textbf{C}^{\ensuremath{\mathsf{T}}}$ nella forma $\hat{\boldsymbol{\Lambda}} \hat{\boldsymbol{\Lambda}}^{\ensuremath{\mathsf{T}}}$ iniziamo a scrivere
\begin{equation}
\textbf{D}= \textbf{D}^{1/2} \textbf{D}^{1/2} 
\end{equation}
dove
\begin{equation}
\textbf{D}^{1/2} = 
\left[
  \begin{array}{ c c c c }
     \sqrt{\theta_1} & 0 & \dots & 0 \\
     0 & \sqrt{\theta_2} & \dots & 0 \\
     \dots & \dots & & \dots \\
     0 & 0 & \dots &  \sqrt{\theta_p}
  \end{array} 
\right] 
\end{equation}
Viene qui usata la notazione $\theta$ per denotare gli autovalori anziché il tradizionale $\lambda$ per evitare la confusione con la notazione $\lambda_{jl}$ usata per le saturazioni fattoriali.
In questo modo, possiamo scrivere
\begin{align}
\textbf{R} &= \textbf{C}\textbf{D}\textbf{C}^{\ensuremath{\mathsf{T}}}\notag\\
&= \textbf{C}\textbf{D}^{1/2}\textbf{D}^{1/2}\textbf{C}^{\ensuremath{\mathsf{T}}}\notag\\
&= (\textbf{C}\textbf{D}^{1/2}) (\textbf{C}\textbf{D}^{1/2})^{\ensuremath{\mathsf{T}}}
\end{align}

Non possiamo però limiarci a definire
    $\hat{\boldsymbol{\Lambda}}=\textbf{C}\textbf{D}^{1/2}$ in quanto
    $\textbf{C}\textbf{D}^{1/2}$ è di ordine $p \times p$ e non
    otteniamo quindi una riduzione di dimensioni.
  Quello che cerchiamo è  una matrice $\hat{\boldsymbol{\Lambda}}$ di ordine $p \times m$ con $m < p$. 
Dunque, definiamo 
 la matrice $\textbf{D}_1= \text{diag}(\theta_1,
\theta_2, \dots, \theta_m)$ come la la matrice contenente gli $m$
autovalori più grandi di \textbf{R} e 
 $\textbf{C}_1=( \textbf{c}_1,
\textbf{c}_2, \dots,  \textbf{c}_m)$ come la matrice  contenente i
rispettivi autovettori.
Mediante il metodo delle componenti principali, le saturazioni fattoriali $\hat{\boldsymbol{\Lambda}}$ vengono quindi stimate nel modo seguente:
\begin{align}
\hat{\boldsymbol{\Lambda}} &= \textbf{C}_1 \textbf{D}_1^{1/2}\notag\\
&= (\sqrt{\theta_1} \textbf{c}_1, \sqrt{\theta_2} \textbf{c}_2, 
\dots, \sqrt{\theta_m} \textbf{c}_m) 
\end{align}
Per l'esempio presente, con $m=2$ e $p=5$, avremo
\begin{equation}
\left[
  \begin{array}{ c c }
 \hat{\lambda}_{11} & \hat{\lambda}_{12} \\
 \hat{\lambda}_{21} & \hat{\lambda}_{22} \\
 \hat{\lambda}_{31} & \hat{\lambda}_{32} \\
 \hat{\lambda}_{41} & \hat{\lambda}_{42} \\
 \hat{\lambda}_{51} & \hat{\lambda}_{52} 
  \end{array} 
\right] =
\left[
  \begin{array}{ c c }
 c_{11} & c_{12} \\
 c_{21} & c_{22} \\
 c_{31} & c_{32} \\
 c_{41} & c_{42} \\
 c_{51} & c_{52} 
  \end{array} 
\right]
\left[
  \begin{array}{ c c }
 \sqrt{\theta_1} & 0\\
 0 &\sqrt{\theta_2} 
  \end{array} 
\right]
\end{equation}
Le saturazioni fattoriali stimate sono dunque uguali a
\begin{equation}
\left[
  \begin{array}{ c c }
 \sqrt{\theta_1}c_{11} & \sqrt{\theta_2}c_{12} \\
 \sqrt{\theta_1}c_{21} & \sqrt{\theta_2}c_{22} \\
 \sqrt{\theta_1}c_{31} & \sqrt{\theta_2}c_{32} \\
 \sqrt{\theta_1}c_{41} & \sqrt{\theta_2}c_{42} \\
 \sqrt{\theta_1}c_{51} & \sqrt{\theta_2}c_{52} 
  \end{array} 
\right]
\end{equation}
Svolgendo i calcoli con \R\, otteniamo:
\begin{lstlisting}
L <- cbind(e$vectors[,1]*sqrt(e$values[1]), 
           e$vectors[,2]*sqrt(e$values[2]))

round(L, 3)
#>        [,1]   [,2]
#> [1,] -0.970 -0.231
#> [2,] -0.519  0.807
#> [3,] -0.785 -0.588
#> [4,] -0.971 -0.210
#> [5,] -0.704  0.667
\end{lstlisting}
La matrice di correlazione riprodotta (con le comunalità sulla diagonale principale) diventa 
\begin{lstlisting}
round(L %*% t(L), 3)
#>       [,1]   [,2]   [,3]  [,4]  [,5]
#> [1,] 0.993  0.317  0.896 0.990 0.529
#> [2,] 0.317  0.921 -0.067 0.335 0.904
#> [3,] 0.896 -0.067  0.961 0.885 0.160
#> [4,] 0.990  0.335  0.885 0.987 0.543
#> [5,] 0.529  0.904  0.160 0.543 0.940
\end{lstlisting}

\noindent
Possiamo ora capire il motivo del nome ``metodo delle componenti principali.''  Le saturazioni fattoriali sono proporzionali agli autovettori di $\textbf{R}$. Tuttavia, dopo la rotazione delle saturazioni fattoriali, l'interpretazione dei fattori è diversa da quella che viene assegnata ai risultai dell'analisi delle componenti principali. 
  
Replichiamo ora la soluzione che abbiamo trovato utilizzando la funzione {\tt principal()} contenuta nel pacchetto {\tt psych}: 
\begin{lstlisting}
principal(R, 2, rotate=FALSE)
#> 
#> Uniquenesses:
#>     K     I     H     L     J 
#> 0.007 0.079 0.039 0.013 0.060 
#> 
#> Loadings:
#>     PC1   PC2  
#> K  0.97 -0.23
#> I  0.52  0.81
#> H  0.78 -0.59
#> L  0.97 -0.21
#> J  0.70  0.67
\end{lstlisting}

\begin{lstlisting}
#>                  PC1  PC2
#> SS loadings    3.251 1.55
#> Proportion Var 0.650 0.31
#> Cumulative Var 0.650 0.96
\end{lstlisting}

È possibile condurre l'analisi fattoriale con il metodo delle componenti principali sia utilizzando la matrice $\textbf{S}$ di varianze-co\-va\-rian\-ze sia la matrice  $\textbf{R}$ delle correlazioni. Tuttavia, le soluzioni ottenute usando $\textbf{S}$ o $\textbf{R}$  non sono legate da una relazione algebrica: il metodo delle componenti principali non è invariante rispetto ai cambiamenti di scala delle osservazioni. Un altro svantaggio del metodo delle componenti principali è che non fornisce un test di bontà di adattamento. Tale test può essere invece svolto quando la soluzione viene trovata con il metodo della massima verosimiglianza.


\section{Metodo dei fattori principali}

Il \textit{metodo dei fattori principali} (\textit{principal factor method}, anche detto \textit{principal axis method}) è uno dei metodi maggiormente usati per la stima delle saturazioni fattoriali e delle comunalità.  
Il metodo delle componenti principali trascura la specificità  $\boldsymbol{\Psi}$  e  si limita a fattorializzare le covarianze di \textbf{S} o le correlazioni di  \textbf{R}. 
Il metodo dei fattori principali utilizza una procedura simile
  al metodo delle componenti principali, utilizzando però una
  matrice ridotta di varianze-covarianze $\textbf{S} -
  \hat{\boldsymbol{\Psi}}$ in cui 
  una stima delle
  comunalità viene sostituita alle varianze presenti sulla diagonale principale.
Nel caso della matrice ridotta di correlazioni
    $\textbf{R} - \hat{\boldsymbol{\Psi}}$, per la comunalità
    $i$-esima $\sum_{j}\lambda_{ij}^2$ si sceglie il quadrato del
    coefficiente di correlazione multipla  tra $Y_i$ e tutte le altre
    $p-1$ variabili. 
 Tale valore si può trovare nel modo seguente:
 
\begin{equation}
\hat{h}^2_i=R^2_i=1-\frac{1}{r^{ii}}
\end{equation}
dove $r^{ii}$ è l'elemento diagonale  $i$-esimo di $\textbf{R}^{-1}$.
Nel caso di una matrice ridotta di varianze-covarianze $\textbf{S} - \hat{\boldsymbol{\Psi}}$, le comunalità possono essere stimate calcolando

\begin{equation}
\hat{h}_i^2=s_{ii}-\frac{1}{r^{ii}}
\end{equation}
dove $s_{ii}$  è l'elemento diagonale  $i$-esimo di $\textbf{S}$.

Affinché le stime comunalità possano essere calcolate come
    descritto sopra, la  matrice  $\textbf{R}$ deve essere non
    singolare. 
 Nel caso in cui $\textbf{R}$ sia singolare, per la stima della comunalità $i$-esima, $ \hat{h}^2_i$, si utilizza il valore assoluto del più elevato coefficiente di correlazione lineare tra $Y_i$ e le altre variabili. 
 
Scelta la stima della comunalità, la matrice ridotta di varianze-covarianze  si ottiene sostituendo alle varianze sulla diagonale principale le stime delle comunalità:

\begin{equation}
\textbf{S} - \hat{\boldsymbol{\Psi}} = 
\left[
  \begin{array}{ c c c c }
    \hat{h}^2_1 & s_{12} & \dots & s_{1p} \\
    s_{21} & \hat{h}^2_2 & \dots & s_{2p} \\
    \dots & \dots &           & \dots\\
    s_{p1} &  s_{p2} & \dots & \hat{h}^2_p
  \end{array} 
\right] 
\end{equation}
In maniera equivalente, la matrice ridotta di correlazioni si ottiene nel modo seguente:

\begin{equation}
\textbf{R} - \hat{\boldsymbol{\Psi}} = 
\left[
  \begin{array}{ c c c c }
    \hat{h}^2_1 & r_{12} & \dots & r_{1p} \\
    r_{21} & \hat{h}^2_2 & \dots & r_{2p} \\
    \dots & \dots &           & \dots\\
    r_{p1} &  r_{p2} & \dots & \hat{h}^2_p
  \end{array} 
\right] 
\end{equation}

Verranno ora svolti i calcoli necessari per la stima dei coefficienti di saturazione con il metodo dei fattori principali utilizzando   la
matrice di correlazione dell'esempio precedente. 
 Quale stima della comunalità $i$-esima, verrà utilizzato 
il valore assoluto più
elevato nella riga $i$-esima della
matrice \textbf{R}.  Per i dati dell'esempio, le stime delle comunalità sono dunque pari a $0.995$, $0.837$, $0.881$, $0.995$ e $0.837$. 

Inserendo tali valori nella diagonale principale, otteniamo la matrice ridotta delle correlazioni $\textbf{R} - \hat{\boldsymbol{\Psi}}$:
\begin{lstlisting}
h.hat <- c(.995, .837, .881, .995, .837)
R[cbind(1:5,1:5)] <- h.hat
R
#>       K      I      H     L     J
#> K 0.995  0.296  0.881 0.995 0.545
#> I 0.296  0.837 -0.022 0.326 0.837
#> H 0.881 -0.022  0.881 0.867 0.130
#> L 0.995  0.326  0.867 0.995 0.544
#> J 0.545  0.837  0.130 0.544 0.837
\end{lstlisting}
Gli autovalori della matrice ridotta di correlazioni $\textbf{R} - \hat{\boldsymbol{\Psi}}$ sono:
\begin{lstlisting}
round(ee$values, 3)
#> [1]  3.202  1.394  0.029  0.000 -0.080
\end{lstlisting}
La somma degli autovalori è uguale a
\begin{lstlisting}
sum(ee$values)
#> [1] 4.545
\end{lstlisting}
I primi due autovalori di  $\textbf{R} - \hat{\boldsymbol{\Psi}}$ sono:
\begin{lstlisting}
round(ee$vectors[,1:2], 3)
#>        [,1]   [,2]
#> [1,] -0.548 -0.177
#> [2,] -0.272  0.656
#> [3,] -0.431 -0.461
#> [4,] -0.549 -0.159
#> [5,] -0.373  0.549
\end{lstlisting}
Moltiplicando tali valori per la radice quadrata dei rispettivi autovalori si ottengono le stime delle saturazioni fattoriali:
\begin{lstlisting}
round(ee$vectors[,1:2] %*% sqrt(diag(ee$values[1:2])), 3)
#>        [,1]   [,2]
#> [1,] -0.981 -0.209
#> [2,] -0.487  0.774
#> [3,] -0.772 -0.544
#> [4,] -0.982 -0.187
#> [5,] -0.667  0.648
\end{lstlisting}
Tale risultato replica quello riportato da Rencher (2002). 


\section{Metodo dei fattori principali iterato}

Solitamente, per migliorare la stima della comunalità, la
 diagonale della matrice $\textbf{S} - \hat{\boldsymbol{\Psi}}$ o
 $\textbf{R} - \hat{\boldsymbol{\Psi}}$  viene  ottenuta per
 iterazione. 
 Dopo avere trovato $\hat{\boldsymbol{\Lambda}}$ a partire da  $\textbf{S} - \hat{\boldsymbol{\Psi}}$ o $\textbf{R} - \hat{\boldsymbol{\Psi}}$ come indicato in precedenza, utilizzando le stime delle saturazioni fattoriali  così ottenute possiamo stimare le comunalità nel modo seguente:
 
\begin{equation}
\hat{h}^2_i = \sum_{i=1}^m \hat{\lambda}_{ij}^2.
\end{equation}
I valori di $\hat{h}^2_i$ vengono quindi sostituiti nella
    diagonale della matrice ridotta $\textbf{S} - \hat{\boldsymbol{\Psi}}$ o $\textbf{R} - \hat{\boldsymbol{\Psi}}$.  A partire da questa nuova matrice, usando il metodo descritto in precedenza, possiamo così ottenere una nuova stima delle saturazioni fattoriali $\hat{\boldsymbol{\Lambda}}$.
 Mediante questa nuova stima di $\hat{\boldsymbol{\Lambda}}$, possiamo procedere ad una nuova stima delle comunalità.  Tale processo continua iterativamente sino alla convergenza. 
 Gli autovalori e gli autovettori della versione finale di $\textbf{S} - \hat{\boldsymbol{\Psi}}$ o $\textbf{R} - \hat{\boldsymbol{\Psi}}$ vengono infine usati per  stimare i pesi fattoriali.
Il metodo dei fattori principali iterato e  il metodo delle componenti principali producono risultati molto simili quando
$m$ assume un piccolo valore (questo si verifica quando le correlazioni sono alte) e quando   $p$ (il numero delle variabili) è grande.

\subsection{Casi di Heywood}

Tra gli inconvenienti del metodo dei fattori principali iterato vi è il fatto che può talvolta 
portare a soluzioni inammissibili (quando viene fattorizzata la
matrice \textbf{R}) caratterizzate da valori di comunalità maggiori di
uno  (\emph{caso di Heywood}).
 Se $\hat{h}^2_i > 1$ allora $\hat{\psi}_i < 0$ il che è
  chiaramente assurdo in quanto una varianza non può assumere un
  valore negativo.  
 Solitamente, quando la stima di una comunalità è maggiore di uno, il processo iterativo viene interrotto e il programma riporta che  non può essere trovata una soluzione. 

Nell'esempio presente viene utilizzata la funzione \texttt{factor.pa()} contenuta nel pacchetto \texttt{psych} per trovare la soluzione dei fattori principali mediante il metodo iterativo:
\begin{lstlisting}
pa <- fa(R, nfactors = 2, rotate = "none", fm = "pa")
pa
#> Standardized loadings (pattern matrix) based upon correlation matrix
#>    PA1   PA2   h2       u2 com
#> K 0.97 -0.27 1.02 -0.01816 1.2
#> I 0.57  0.79 0.95  0.04930 1.8
#> H 0.79 -0.63 1.03 -0.02554 1.9
#> L 0.97 -0.25 1.00  0.00061 1.1
#> J 0.78  0.70 1.10 -0.10348 2.0
#> 
#>                        PA1  PA2
#> SS loadings           3.45 1.65
#> Proportion Var        0.69 0.33
#> Cumulative Var        0.69 1.02
#> Proportion Explained  0.68 0.32
#> Cumulative Proportion 0.68 1.00
\end{lstlisting}
Si noti che, in questo caso, le unicità assumono valori negativi, il che suggerisce che la soluzione è impropria.


\section{Metodo di massima verosimiglianza}

L'applicazione del metodo di massima verosimiglianza è  indicata quando si può assumere che le variabili manifeste seguono una distribuzione 
normale multivariata.
  Sotto tali condizioni, tale metodo produce le stime dei pesi
  fattoriali che più verosimilmente hanno prodotto le correlazioni
  osservate.
  Gli stimatori di massima verosimiglianza sono preferibili a quelli ottenuti con altri metodi, sempre che siano pienamente realizzate le premesse.
La funzione $F$ da minimizzare rappresenta una misura di ``distanza''  tra la matrice di covarianza osservata e quella predetta dal modello.  
Uguagliando a zero le derivate di $F$ rispetto a $\boldsymbol{\Lambda}$ e $\boldsymbol{\Psi}$ si ottengono le equazioni per le stime di massima verosimiglianza di $\hat{\boldsymbol{\Lambda}}$ e $\hat{\boldsymbol{\Psi}}$. 
Risolvendo tali equazioni rispetto alle incognite $\hat{\boldsymbol{\Lambda}}$ e $\hat{\boldsymbol{\Psi}}$ si ricavano le stime di massima verosimiglianza. 

Non esistendo una soluzione  analitica per queste 
equazioni, si ricorre a procedimenti numerici iterativi che 
talvolta presentano problemi di convergenza.
  La soluzione, pur
presentando la possibilità di fornire delle stime di comunalità
superiori a 1 (caso di Heywood), è equivariante rispetto a cambiamenti
di scala: le stime di massima verosimiglianza sono indipendenti
dall'unità di misura delle variabili manifeste.
  Pertanto, si ottiene
la stessa soluzione sia che si analizzi la matrice delle varianze e
covarianze, sia che si analizzi la matrice delle correlazioni. 

Consideriamo nuovamente i dati dell'esempio precedente.
  Le istruzioni \R sono le seguenti:
\begin{lstlisting}
factanal(covmat=R, factors=2)
#> Uniquenesses:
#> [1] 0.005 0.268 0.055 0.008 0.005
#> 
#> Loadings:
#>      Factor1 Factor2
#> [1,]  0.946   0.317 
#> [2,]          0.855 
#> [3,]  0.965  -0.117 
#> [4,]  0.944   0.318 
#> [5,]  0.252   0.965 
#> 
#>                Factor1 Factor2
#> SS loadings      2.784   1.877
#> Proportion Var   0.557   0.375
#> Cumulative Var   0.557   0.932
\end{lstlisting}

