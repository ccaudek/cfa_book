# Il modello multifattoriale 

```{r, include = FALSE}
source("_common.R")
library("lavaan")
library("semPlot")
library("knitr")
library("kableExtra")
library("tidyr")
library("corrplot")
```

## Modello multifattoriale: fattori ortogonali

La teoria dei due fattori ha orientato per diversi anni le ricerche
sull'intelligenza, finché Thurstone (1945) non propose una sua modifica,
conosciuta come teoria multifattoriale. Secondo Thurstone la
covariazione tra le variabili manifeste non può essere spiegata da un
unico fattore generale. Invece è necessario ipotizzare l'azione causale
di diversi fattori, definiti comuni, i quali si riferiscono solo ad
alcune delle variabili considerate.

Il modello plurifattoriale assume che ciascuna variabile manifesta sia
espressa come funzione lineare di un certo numero $m$ di fattori comuni,
$\xi_1, \xi_2, \dots, \xi_m$, responsabili della correlazione con le
altre variabili, e di un solo fattore specifico (termine d'errore),
responsabile della variabilità della variabile stessa. Per $p$ variabili
manifeste, $Y_1, Y_2, \dots, Y_p$, il modello fattoriale diventa quello
indicato dal sistema di equazioni lineari descritto di seguito.
Idealmente, $m$ dovrebbe essere molto più piccolo di $p$ così da
consentire una descrizione parsimoniosa delle variabili manifeste in
funzione di pochi fattori soggiacenti.

Le variabili manifeste $Y$ sono indicizzate da $i = 1, \dots, p.$ Le
variabili latenti $\xi$ (fattori) sono indicizzate da $j = 1, \dots, m.$
I fattori specifici $\delta$ sono indicizzati da $i = 1, \dots, p.$ Le
saturazioni fattoriali si distinguono dunque tramite due indici, $i$ e
$j$: il primo indice si riferisce alle variabili manifeste, il secondo
si riferisce ai fattori latenti.

Indichiamo con $\mu_i$, con $i=1, \dots, p$ le medie delle $p$ variabili
manifeste $Y_1, Y_2, \dots, Y_p$. Se non vi è alcun effetto delle
variabili comuni latenti, allora la variabile $Y_{ijk}$, dove $k$ è
l'indice usato per i soggetti, sarà uguale a:

\begin{equation}
\begin{cases} 
  Y_{1k}    &= \mu_1 + \delta_{1k} \\
&\vdots\\
Y_{ik}   &= \mu_i + \delta_{ik}\\
&\vdots\\
Y_{pk}   &= \mu_p + \delta_{pk} \notag
\end{cases}
\end{equation}

Se invece le variabili manifeste rappresentano la somma
dell'effetto causale di $m$ fattori comuni e di $p$ fattori specifici,
allora possiamo scrivere: 

\begin{equation}
\begin{cases} 
  Y_1  - \mu_1  &= \lambda_{11}\xi_1 + \dots + \lambda_{1k}\xi_k \dots +\lambda_{1m}\xi_m + \delta_1 \\
&\vdots\\
Y_i -  \mu_i  &= \lambda_{i1}\xi_1 + \dots +  \lambda_{ik}\xi_k \dots +\lambda_{im}\xi_m + \delta_i\\
&\vdots\\
Y_p - \mu_p  &= \lambda_{p1}\xi_1 + \dots +  \lambda_{pk}\xi_k \dots +\lambda_{pm}\xi_m + \delta_p \notag
\end{cases}
\end{equation}

Nel precedente sistema di equazioni lineari,

-   $\xi_j$, con $j=1, \dots, m$, rappresenta la $j$-esima variabile
    inosservabile a fattore comune (ossia il $j$-esimo fattore comune a
    tutte le variabili $Y_i$);
-   $\lambda_{ij}$ rappresenta il parametro, detto *saturazione* o
    *peso* fattoriale, che riflette l'importanza del $j$-esimo fattore
    comune nella composizione della $i$-esima variabile osservabile;
-   $\delta_i$ rappresenta il fattore specifico (o unico) di ogni
    variabile manifesta $Y_i$.

In conclusione, secondo il modello multifattoriale, le variabili
manifeste $Y_i$, con $i=1, \dots, p$, sono il risultato di una
*combinazione lineare* di $m < p$ fattori inosservabili ad esse comuni
$\xi_j$, con $j=1, \dots, m$, e di $p$ fattori specifici $\delta_i$, con
$i=1, \dots, p$, anch'essi inosservabili e di natura residua.

### Assunzioni del modello multifattoriale

Le variabili inosservabili a fattore comune $\xi_j$, con
$j=1, \dots, m$, in quanto latenti, non possiedono unità di misura.
Pertanto, per semplicità si assume che abbiano media zero,
$\mathbb{E}(\xi_j)=0$, abbiano varianza unitaria,
$\mathbb{V} (\xi_j)= \mathbb{E}(\xi_j^2) - [\mathbb{E}(\xi_j)]^2=1$, e siano incorrelate
tra loro, $\mbox{Cov}(\xi_j, \xi_h)=0$, con $j, h = 1, \dots, m; \;j \neq h$.
Si assume inoltre che le variabili a fattore specifico $\delta_i$ siano
tra loro incorrelate, $\mbox{Cov}(\delta_i,\delta_k)=0$, con
$i, k = 1, \dots, p, \; i \neq k$, abbiano media zero,
$\mathbb{E}(\delta_i)=0$, e varianza uguale a $\mathbb{V} (\delta_i) = \psi_{ii}$.
La varianza $\psi_{ii}$ è detta *varianza specifica* o *unicità* della
$i$-esima variabile manifesta $Y_i$. Si assume infine che i fattori
specifici siano linearmente incorrelati con i fattori comuni, ovvero
$\mbox{Cov}(\xi_j, \delta_i)=0$ per ogni $j=1, \dots, m$ e per ogni
$i=1\dots,p$.

### Interpretazione dei parametri del modello

Quale esempio, consideriamo il caso di $p=5$ variabili osservabili e
$m=2$ fattori ortogonali. Se le variabili manifeste sono 'centrate'
(ovvero, se a ciascuna di esse sottraiamo la rispettiva media), allora
il modello multifattoriale diventa +

\begin{equation}
\begin{aligned}
  Y_1 &= \lambda_{11} \xi_1 + \lambda_{12} \xi_2 + \delta_1,\notag\\
  Y_2 &= \lambda_{21} \xi_1 + \lambda_{22} \xi_2 + \delta_2,\notag\\
  Y_3 &= \lambda_{31} \xi_1 + \lambda_{32} \xi_2 + \delta_3,\notag\\
  Y_4 &= \lambda_{41} \xi_1 + \lambda_{42} \xi_2 + \delta_4,\notag\\
  Y_5 &= \lambda_{51} \xi_1 + \lambda_{52} \xi_2 + \delta_5.\notag
\end{aligned}
\end{equation}

### Covarianza tra variabili e fattori

Nell'ipotesi che le variabili $Y_i$ abbiano media nulla, la covarianza
tra $Y_i$ e $\xi_j$ è uguale alla saturazione fattoriale $\lambda_{ij}$:

$$
\begin{equation}
\begin{aligned}
  \mbox{Cov}(Y_i, \xi_j) &= \mathbb{E}(Y_i \xi_j)\notag\\
  &=\mathbb{E}\left[(\lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i)\xi_j \right]\notag\\
  &= \lambda_{i1}\underbrace{\mathbb{E}(\xi_1\xi_j)}_{=0} + \dots + 
\lambda_{ij}\underbrace{\mathbb{E}(\xi_j^2)}_{=1} + \dots \notag\\
& \; + \lambda_{im}\underbrace{\mathbb{E}(\xi_m\xi_j)}_{=0} +
  \underbrace{\mathbb{E}(\delta_i \xi_j)}_{=0}\notag\\
  &= \lambda_{ij}.\notag
\end{aligned}
\end{equation}
$$

Anche nel modello multifattoriale, dunque, le saturazioni fattoriali rappresentano le covarianze tra le variabili e i fattori:

$$
\mbox{Cov}(Y_i, \xi_j) = \lambda_{ij} \qquad i=1, \dots, p; \quad j= 1, \dots, m. 
$$

Naturalmente, se le variabili sono standardizzate, le saturazioni fattoriali diventano correlazioni: 

$$
r_{ij} = \lambda_{ij}. 
$$

### Espressione fattoriale della varianza

Come nel modello monofattoriale, la varianza delle variabili manifeste
si decompone in una componente dovuta ai fattori comuni, chiamata
*comunalità*, e in una componente specifica alle $Y_i$, chiamata
*unicità*. Nell'ipotesi che le variabili $Y_i$ abbiano media nulla, la
varianza di $Y_i$ è uguale a 

\begin{equation}
\begin{aligned}
  \mathbb{V} (Y_i) 
  &=\mathbb{E}\left[ (\lambda_{i1} \xi_1 + \dots +
    \lambda_{im} \xi_m + \delta_i)^2 \right].
\end{aligned}
(\#eq:eq-var-multifatt)
\end{equation}

Come si sviluppa il polinomio precedente? Il quadrato di un polinomio è uguale alla somma
dei quadrati di tutti i termini più il doppio prodotto di ogni termine
per ciascuno di quelli che lo seguono. Il valore atteso del quadrato del
primo termine è uguale a $\lambda_{i1}^2\mathbb{E}(\xi_1^2)$ ma, essendo la
varianza di $\xi_1$ uguale a $1$, otteniamo semplicemente
$\lambda_{i1}^2$. Lo stesso vale per i quadrati di tutti i termini
seguenti tranne l'ultimo. Infatti, $\mathbb{E}(\delta_i^2)=\psi_{ii}$. Per quel
che riguarda i doppi prodotti, sono tutti nulli. In primo luogo perché,
nel caso di fattori ortogonali, la covarianza tra i fattori comuni è
nulla, $\mathbb{E}(\xi_j \xi_h)=0$, con $j \neq h$. In secondo luogo perché il
fattori comuni cono incorrelati con i fattori specifici, quindi
$\mathbb{E}(\delta_i \xi_j)=0$.

In conclusione, 

\begin{equation}
\begin{aligned}
  \mathbb{V}(Y_i) &= \lambda_{i1}^2 + \lambda_{i2}^2 + \dots + \lambda_{im}^2 + \psi_{ii} \notag\\
  &= \sum_{j=1}^m \lambda_{ij}^2 + \psi_{ii}\notag\\
  &= h_i^2 + \psi_{ii}\notag\\
  &=\text{communalità} + \text{unicità},\notag
\end{aligned}
\end{equation}
  
la varianza della variabile manifesta $Y_i$ è suddivisa in due parti: il
primo addendo è definito comunalità poiché rappresenta la parte di
variabilità della $Y_i$ spiegata dai fattori comuni; il secondo addendo
è invece definito varianza specifica (o unicità) poiché esprime la parte
di variabilità della $Y_i$ non spiegata dai fattori comuni.

### Espressione fattoriale della covarianza

Per semplificare, consideriamo il caso particolare esaminato prima,
ovvero quello con $p=5$ variabili osservabili e $m=2$ fattori
ortogonali. Nell'ipotesi che le variabili $Y_i$ abbiano media nulla, la
covarianza tra $Y_1$ e $Y_2$, ad esempio, è uguale a: 

\begin{equation}
\begin{aligned}
  \mbox{Cov}(Y_1, Y_2) &= \mathbb{E}\left( Y_1 Y_2\right) \notag\\
  &= \mathbb{E}\left[ 
  (\lambda_{11} \xi_1 + \lambda_{12} \xi_2 + \delta_1)
   (\lambda_{21} \xi_1 + \lambda_{22} \xi_2 +  \delta_2)
  \right]\notag\\
  &= \lambda_{11} \lambda_{21} \mathbb{E}(\xi_1^2) +
      \lambda_{11} \lambda_{22} \mathbb{E}(\xi_1 \xi_2) +\notag 
      \lambda_{11} \mathbb{E}(\xi_1 \delta_2) +\notag\\
    &\quad \lambda_{12} \lambda_{21}\mathbb{E}(\xi_1 \xi_2)\, + 
      \lambda_{12} \lambda_{22}\mathbb{E}(\xi^2_2)\, + 
      \lambda_{12} \mathbb{E}(\xi_2\delta_2) +\notag\\
    &\quad \lambda_{21} \mathbb{E}(\xi_1\delta_1) +\notag 
     \lambda_{22} \mathbb{E}(\xi_2\delta_1) + \mathbb{E}(\delta_1 \delta_2)\notag\\
   &= \lambda_{11} \lambda_{21} + \lambda_{12} \lambda_{22}.\notag
\end{aligned}
\end{equation}

In conclusione, la covarianza tra le variabili manifeste $Y_l$ e $Y_m$
riprodotta dal modello è data dalla somma dei prodotti delle saturazioni
$\lambda_l \lambda_m$ nei due fattori.

::: {.exercise}
Consideriamo i dati riportati da @brown2015confirmatory, ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia. Le scale sono le seguenti:

- anxiety (N1), 
- hostility (N2), 
- depression (N3), 
- self-consciousness (N4), 
- warmth (E1), 
- gregariousness (E2), 
- assertiveness (E3), 
- positive emotions (E4). 


```{r}
varnames <- c("N1", "N2", "N3", "N4", "E1", "E2", "E3", "E4")
sds <- '5.7  5.6  6.4  5.7  6.0  6.2  5.7  5.6'

cors <- '
 1.000
 0.767  1.000 
 0.731  0.709  1.000 
 0.778  0.738  0.762  1.000 
-0.351  -0.302  -0.356  -0.318  1.000 
-0.316  -0.280  -0.300  -0.267  0.675  1.000 
-0.296  -0.289  -0.297  -0.296  0.634  0.651  1.000 
-0.282  -0.254  -0.292  -0.245  0.534  0.593  0.566  1.000'

psychot_cor_mat <- getCov(cors, names = varnames)
n <- 250
```

Eseguiamo l'analisi fattoriale esplorativa con il metodo della massima verosimiglianza ipotizzando due fattori comuni incorrelati:

```{r}
n_facs <- 2
fit_efa <- factanal(
  covmat = psychot_cor_mat,
  factors = n_facs,
  rotation = "varimax",
  n.obs = n
)
```

Esaminiamo le saturazioni fattoriali:

```{r}
lambda <- fit_efa$loadings
lambda
```

La soluzione fattoriale conferma la presenza di due fattori: il primo fattore satura sulle scale di neutoricismo, il secono sulle scale di estroversione.

La correlazione riprodotta $r_{12}$ è uguale a $\lambda_{11}\lambda_{21} + \lambda_{12}\lambda_{22}$ 

```{r}
lambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2]
```

e corrisponde da vicino alla correlazione osservata 0.767.

L'intera matrice di correlazioni riprodotte è
$\boldsymbol{\Lambda} \boldsymbol{\Lambda}^{\ensuremath{\mathsf{T}}} + \boldsymbol{\psi}$:

```{r}
Rr <- lambda %*% t(lambda) + diag(fit_efa$uniq)
Rr %>% 
  round(3)
```

La differenza tra la matrice di correlazioni riprodotte e la matrice di
correlazioni osservate è uguale a:

```{r}
(psychot_cor_mat - Rr) %>% 
  round(3)
```
:::


::: {.exercise}
Consideriamo nuovamente i dati precedenti ma, in questo caso, eseguiamo un'analisi fattoriale confermativa. Usando `lavaan` il modello diventa:

```{r}
cfa_mod <- "
  N =~ N1 + N2 + N3 + N4
  E =~ E1 + E2 + E3 + E4
"
```

```{r}
fit_cfa <- lavaan::cfa(
  cfa_mod,
  sample.cov = psychot_cor_mat,
  sample.nobs = n,
  orthogonal = TRUE,
  std.lv = TRUE
)
```

Il path diagram si ottiene nel modo seguente:

```{r}
semPaths(
  fit_cfa,
  "std", 
  posCol = c("black"),
  edge.label.cex = 1.2, 
  sizeMan = 7
  )
```

Esaminiamo le saturazioni fattoriali:

```{r}
parameterEstimates(fit_cfa, standardized = TRUE) %>%
  dplyr::filter(op == "=~") %>%
  dplyr::select(
    "Latent Factor" = lhs, 
    Indicator = rhs, 
    B = est, 
    SE = se, 
    Z = z, 
    "p-value" = pvalue, 
    Beta = std.all) %>%
  knitr::kable(digits = 3, booktabs = TRUE, format = "markdown", 
               caption = "Factor Loadings")
```

Il risultato sembra sensato: le saturazioni su ciascun fattore sono molto alte. Tuttavia, la matrice delle correlazioni residue

```{r}
cor_table <- residuals(fit_cfa, type = "cor")$cov
knitr::kable(
  cor_table, 
  digits = 3, 
  format = "markdown", 
  booktabs = TRUE
)
```

rivela che il modello ipotizzato dall'analisi fattoriale confermativa non è adeguato.
:::

## Modello fattoriale: Fattori obliqui

Anche nel caso di fattori comuni correlati è possibile esprimere nei
termini dei parametri del modello la covarianza teorica tra una
variabile manifesta $Y_i$ e uno dei fattori comuni, la covarianza
teorica tra due variabili manifeste, e la comunalità di ciascuna
variabile manifesta. Dato però che i fattori comuni risultano correlati,
l'espressione fattoriale di tali quantità è più complessa che nel caso
di fattori comuni ortogonali.

### Covarianza teorica tra variabili e fattori

In base al modello multifattoriale con $m$ fattori comuni la variabile
$Y_i$ è

$$
Y_i = \lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i.
(\#eq:mod-multifact)
$$ 

Poniamoci il problema di trovare la
covarianza teorica tra la variabile manifesta $Y_i$ e il fattore comune
$\xi_j$. Come in precedenza, il problema si riduce a quello di trovare
$\mathbb{E}(Y_i \xi_j)$. Ne segue che 

$$
\begin{equation}
\begin{aligned}
  \mbox{Cov}(Y_i, \xi_j) &= \mathbb{E}(Y_i \xi_j)\notag\\
  &=\mathbb{E}\left[(\lambda_{i1} \xi_1 + \dots + \lambda_{ij} \xi_j + \dots + \lambda_{im} \xi_m + \delta_i)\xi_j \right]\notag\\
  &= \lambda_{i1}\underbrace{\mathbb{E}(\xi_1\xi_j)}_{\neq 0} + \dots + \lambda_{ij}\underbrace{\mathbb{E}(\xi_j^2)}_{=1} + \dots \notag\\
& \quad + \lambda_{im}\underbrace{\mathbb{E}(\xi_m\xi_j)}_{\neq 0} + \underbrace{\mathbb{E}(\delta_i \xi_j)}_{=0}\notag\\
  &= \lambda_{ij} + \lambda_{i1} \mbox{Cov}(\xi_1, \xi_j) + \dots + \lambda_{im} \mbox{Cov}(\xi_m, \xi_j).
\end{aligned}
\end{equation}
$$

Ad esempio, nel caso di tre fattori comuni $\xi_1, \xi_2, \xi_3$, la
covarianza tra $Y_1$ e $\xi_{1}$ diventa

$$
\lambda_{11} + \lambda_{12}\mbox{Cov}(\xi_1, \xi_2) + \lambda_{13}\mbox{Cov}(\xi_1, \xi_3).
$$

### Espressione fattoriale della varianza

Poniamoci ora il problema di trovare la varianza teorica della variabile
manifesta $Y_i$. In base al modello fattoriale, la variabile $Y_i$ è
specificata come nella \@ref(eq:mod-multifact). La varianza di $Y_i$ è
$\mathbb{V}(Y_i) = \mathbb{E}(Y_i^2) -[\mathbb{E}(Y_i)]^2$. Però, avendo espresso $Y_i$ nei
termini della differenza dalla sua media, l'espressione della varianza
si riduce a $\mathbb{V}(Y_i) = \mathbb{E}(Y_i^2)$. Dobbiamo dunque sviluppare
l'espressione

$$
\mathbb{E}(Y_i^2) = \mathbb{E}[(\lambda_{i1} \xi_1 + \dots + \lambda_{im} \xi_m + \delta_i)^2].
$$

In conclusione, la varianza teorica di $Y_i$ è uguale a 

\begin{equation}
\begin{split}
\mathbb{V}(Y_i) &= \lambda_{i1}^2 + \lambda_{i2}^2 + \dots + \lambda_{im}^2  + \\
&\quad 2 \lambda_{i1} \lambda_{i2} \mbox{Cov}(\xi_1, \xi_2) + \dots + 2 \lambda_{i,m-1} \lambda_{im} \mbox{Cov}(\xi_{m-1}, \xi_m) + \\
&\quad \psi_{ii}.\notag
\end{split}
\end{equation}

Ad esempio, nel caso di tre fattori comuni, $\xi_1, \xi_2, \xi_3$, la
varianza di $Y_1$ è 

\begin{equation}
\begin{split}
\mathbb{V}(Y_1) = &\lambda_{11}^2 + \lambda_{12}^2 + \lambda_{13}^2 +\\ 
&\quad 2 \lambda_{11} \lambda_{12} \mbox{Cov}(\xi_1, \xi_2) + \\ 
&\quad 2 \lambda_{11} \lambda_{13} \mbox{Cov}(\xi_1, \xi_3) + \\ 
&\quad 2 \lambda_{12} \lambda_{13} \mbox{Cov}(\xi_2, \xi_3) + \\ 
&\quad \psi_{11}. \notag
\end{split}
\end{equation}

### Covarianza teorica tra due variabili

Consideriamo ora il caso più semplice di due soli fattori comuni
correlati e calcoliamo la covarianza tra $Y_1$ e $Y_2$:

\begin{equation}
\begin{aligned}
\mathbb{E}(Y_1 Y_2) =\mathbb{E}[(&\lambda_{11}\xi_1 + \lambda_{12}\xi_2+\delta_1) (\lambda_{21}\xi_1 + \lambda_{22}\xi_2+\delta_2)]\notag\\
=\mathbb{E}( 
&\lambda_{11}\lambda_{21}\xi_1^2 +
\lambda_{11}\lambda_{22}\xi_1\xi_2 +
\lambda_{11}\xi_1\delta_2 +\notag\\
+&\lambda_{12}\lambda_{21}\xi_1\xi_2 +
\lambda_{12}\lambda_{22}\xi_2^2 +
\lambda_{12}\xi_2\delta_2 +\notag\\
+&\lambda_{21}\xi_1\delta_1 +
\lambda_{22}\xi_2\delta_1 +
\delta_1\delta_2).\notag
\end{aligned}
\end{equation}

Distribuendo l'operatore di valore atteso, dato che $\mathbb{E}(\xi^2)=1$ e $\mathbb{E}(\xi \delta)=0$, otteniamo

$$
\mbox{Cov}(Y_1, Y_2) = \lambda_{11} \lambda_{21} + \lambda_{12} \lambda_{22} + 
\lambda_{12} \lambda_{21}\mbox{Cov}(\xi_1, \xi_2) +\lambda_{11} \lambda_{22}\mbox{Cov}(\xi_1, \xi_2).
$$

In termini matriciali si scrive 

$$
\boldsymbol{\Sigma} =\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\mathsf{T}} + \boldsymbol{\Psi}, 
$$

dove $\boldsymbol{\Phi}$ è la matrice di ordine $m \times m$ di varianze
e covarianze tra i fattori comuni e $\boldsymbol{\Psi}$ è una matrice
diagonale di ordine $p$ con le unicità delle variabili.

::: {.exercise}
Consideriamo nuovamente i dati esaminati negli esercizi precedenti, ma questa volta il modello consente una correlazione tra i due fattori comuni:

```{r}
fit2_cfa <- lavaan::cfa(
  cfa_mod,
  sample.cov = psychot_cor_mat,
  sample.nobs = n,
  orthogonal = FALSE,
  std.lv = TRUE
)
```

Visualizziamo il modello nel modo seguente:

```{r}
semPaths(
  fit2_cfa,
  "std", 
  posCol = c("black"),
  edge.label.cex = 1.1, 
  sizeMan = 7
  )
```

Esaminiamo le saturazioni fattoriali:

```{r}
parameterEstimates(fit2_cfa, standardized = TRUE) %>%
  dplyr::filter(op == "=~") %>%
  dplyr::select(
    "Latent Factor" = lhs, 
    Indicator = rhs, 
    B = est, 
    SE = se, 
    Z = z, 
    "p-value" = pvalue, 
    Beta = std.all) %>%
  knitr::kable(digits = 3, booktabs = TRUE, format = "markdown", 
               caption = "Factor Loadings")
```

Le saturazioni sono simili a quelle che abbiamo trovato in precedenza, In questo caso, però, la matrice delle correlazioni residue è adeguata:

```{r}
cor_table <- residuals(fit2_cfa, type = "cor")$cov
knitr::kable(
  cor_table, 
  digits = 3, 
  format = "markdown", 
  booktabs = TRUE
)
```
:::


::: {.exercise}
Esaminiamo più da vicino la matrice di correlazioni riprodotta dal modello, nel caso di fattori obliqui. Le saturazioni fattoriali sono:

```{r}
lambda <- inspect(fit2_cfa, what="std")$lambda
lambda
```

La matrice di intercorrelazoni fattoriali è

```{r}
Phi <- inspect(fit2_cfa, what="std")$psi
Phi
```

Le varianze residue sono:

```{r}
Psi <- inspect(fit2_cfa, what="std")$theta
Psi
```

Mediante i parametri del modello  la matrice di correlazione si riproduce nel modo seguente:

$$
\boldsymbol{\Sigma} =\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\mathsf{T}} + \boldsymbol{\Psi}. 
$$

Le corrispondenti istruzioni $\textsf{R}$ sono:

```{r}
R_hat <- lambda %*% Phi %*% t(lambda) + Psi
R_hat %>% 
  round(3)
```

Le correlazioni residue sono:

```{r}
(psychot_cor_mat - R_hat) %>% 
  round(3)
```

Per fare un esempio, calcoliamo la correlazione predetta dal modello tra le
variabili $Y_1$ e $Y_2$:

```{r}
lambda[1, 1] * lambda[2, 1] + lambda[1, 2] * lambda[2, 2] +
  lambda[1, 1] * lambda[2, 2] * Phi[1, 2] + 
  lambda[1, 2] * lambda[2, 1] * Phi[1, 2]
```

Questo valore si avvicina al valore contenuto dell'elemento (1, 2) della
matrice di correlazioni osservate:

```{r}
psychot_cor_mat[1, 2]
```

Usando le funzonalità di `lavaan` la matrice di correlazione predetta si ottiene con:

```{r}
fitted(fit2_cfa)$cov
```

La matrice dei residui è

```{r}
resid(fit2_cfa)$cov
```

La matrice dei residui standardizzati è

```{r}
resid(fit2_cfa, type = "standardized")$cov
```
:::

## EFA con `lavaan`

Una funzionalità sperimentale di `lavaan` (ancora non ufficiale) è quella che consente di svolgere l'analisi fattoriale esplorativa con la funzione `efa()`. Consideriamo nuovamente i dati di @brown2015confirmatory, ovvero otto misure di personalità raccolte su un campione di 250 pazienti che hanno concluso un programma di psicoterapia.

```{r}
# 1-factor model
f1 <- '
efa("efa")*f1 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4
'
# 2-factor model
f2 <- '
efa("efa")*f1 +
efa("efa")*f2 =~ N1 + N2 + N3 + N4 + E1 + E2 + E3 + E4
'
```

```{r}
efa_f1 <-
  cfa(
    model = f1,
    sample.cov = psychot_cor_mat,
    sample.nobs = 250,
    rotation = "oblimin"
  )
```

```{r}
summary(
  efa_f1, 
  fit.measures = TRUE, 
  standardized = TRUE, 
  rsquare = TRUE
)
```


```{r}
efa_f2 <-
  cfa(
    model = f2,
    sample.cov = psychot_cor_mat,
    sample.nobs = 250,
    rotation = "oblimin"
  )
```

```{r}
summary(
  efa_f2, 
  fit.measures = TRUE, 
  standardized = TRUE, 
  rsquare = TRUE
)
```

```{r}
# define the fit measures
fit_measures_robust <- c(
  "chisq", "df", "pvalue", 
  "cfi", "rmsea", "srmr"
)

# collect them for each model
rbind(
  fitmeasures(efa_f1, fit_measures_robust),
  fitmeasures(efa_f2, fit_measures_robust)
) %>%
  # wrangle
  data.frame() %>%
  mutate(
    chisq = round(chisq, digits = 0),
    df = as.integer(df),
    pvalue = ifelse(pvalue == 0, "< .001", pvalue)
  ) %>%
  mutate_at(vars(cfi:srmr), ~ round(., digits = 3))
```

I risultati mostrano come la funzione `efa()` sia effettivamente in grado di distinguere in maniera chiara tra i due fattori. Il confronto tra modelli mostra la superiorità del modello a due fattori.

