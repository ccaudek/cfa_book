# L'estrazione dei fattori {#ch:estrazione}

```{r, include = FALSE}
source("_common.R")
library("psych")
```

## Motivazione {#motivazione .unnumbered}

Lo scopo dell'analisi fattoriale è quello di descrivere in maniera
parsimoniosa le relazioni che intercorrono tra un grande numero di item.
Ci si chiede se è possibile identificare un piccolo numero di variabili
latenti che, quando vengono controllate, rendono uguali a zero le
correlazioni parziali tra gli item. Abbiamo visto nel capitolo
precedente come sia possibile determinare il numero dei fattori comuni.
Chiediamoci ora come sia possibile stimare le saturazioni fattoriali
che corrispondono alle correlazioni (o
covarianze) tra gli item e i fattori.

In termini matriciali, il modello multifattoriale si scrive

$$
\boldsymbol{\Sigma} =\boldsymbol{\Lambda} \boldsymbol{\Phi} \boldsymbol{\Lambda}^{\mathsf{T}} + \boldsymbol{\Psi} 
$$

dove $\boldsymbol{\Phi}$ è la matrice di ordine $m \times m$ di varianze
e covarianze tra i fattori comuni e $\boldsymbol{\Psi}$ è una matrice
diagonale di ordine $p$ con le unicità delle variabili. Poniamoci ora il
problema di stimare $\boldsymbol{\Lambda}$.

## Metodo delle componenti principali

L'analisi fattoriale eseguita mediante il metodo delle componenti
principali, nonostante il nome, non è un'analisi delle componenti
principali. Il metodo delle componenti principali costituisce invece
un'applicazione del teorema di scomposizione spettrale di una matrice.
Il *teorema spettrale* afferma che "data la matrice simmetrica
$\textbf{S}_{p \times p}$, è sempre possibile trovare una matrice
$\textbf{C}_{p \times p}$ ortogonale tale che

$$
\textbf{S} = \textbf{C}\textbf{D}\textbf{C}^{\mathsf{T}}
$$
con **D** diagonale." Il teorema specifica inoltre che gli elementi
presenti sulla diagonale di **D** sono gli autovalori di **S**, mentre
le colonne di **C** rappresentano i rispettivi autovettori normalizzati
associati agli autovalori di **S**.

Facciamo un esempio numerico utilizzando i dati discussi da
Rencher(2002). Brown, Williams e Barlow (1984) hanno raccolto le
valutazioni di una ragazza dodicenne relativamente a sette persone di
sua conoscenza. Ciascuna persona veniva valutata su una scala a nove
punti rispetto a cinque variabili: *kind*, *intelligent*, *happy*,
*likeable* e *just*. La matrice di correlazione per tali variabili è
riportata di seguito:

```{r}
R <- matrix(c(
  1.000, .296, .881, .995, .545,
  .296, 1.000, -.022, .326, .837,
  .881, -.022, 1.000, .867, .130,
  .995, .326, .867, 1.000, .544,
  .545, .837, .130, .544, 1.000
),
ncol = 5, byrow = T, dimnames = list(
  c("K", "I", "H", "L", "J"),
  c("K", "I", "H", "L", "J")
)
)
```

Gli autovalori e gli autovettori si calcolano con la funzione `eigen()`:

```{r}
e <- eigen(R)
print(e, 3)
```

Come indicato in precedenza, la matrice **R** può essere espressa come
$\textbf{R} = \textbf{C}\textbf{D}\textbf{C}^{\ensuremath{\mathsf{T}}}$:

```{r}
e$vectors %*% diag(e$values) %*% t(e$vectors)
```

Esaminiamo ora gli autovalori. I primi due autovalori spiegano da soli
il 96% della varianza campionaria:

```{r}
(e$values[1] + e$values[2]) / 5
```

Usando i primi due autovalori e i primi due autovettori sarà dunque
possibile riprodurre in maniera soddisfacente la matrice **R** operando
nel contempo una riduzione di dimensionalità dei dati.

Per fattorizzare
$\textbf{R} = \textbf{C}\textbf{D}\textbf{C}^{\ensuremath{\mathsf{T}}}$
nella forma
$\hat{\boldsymbol{\Lambda}} \hat{\boldsymbol{\Lambda}}^{\ensuremath{\mathsf{T}}}$
iniziamo a scrivere 

$$\textbf{D}= \textbf{D}^{1/2} \textbf{D}^{1/2}$$

dove 

$$
\textbf{D}^{1/2} = 
\left[
  \begin{array}{ c c c c }
     \sqrt{\theta_1} & 0 & \dots & 0 \\
     0 & \sqrt{\theta_2} & \dots & 0 \\
     \dots & \dots & & \dots \\
     0 & 0 & \dots &  \sqrt{\theta_p}
  \end{array} 
\right]
$$ 

Viene qui usata la notazione $\theta$ per denotare gli
autovalori anziché il tradizionale $\lambda$ per evitare la confusione
con la notazione $\lambda_{jl}$ usata per le saturazioni fattoriali. In
questo modo, possiamo scrivere 

\begin{equation}
\begin{aligned}
\textbf{R} &= \textbf{C}\textbf{D}\textbf{C}^{\mathsf{T}}\notag\\
&= \textbf{C}\textbf{D}^{1/2}\textbf{D}^{1/2}\textbf{C}^{\mathsf{T}}\notag\\
&= (\textbf{C}\textbf{D}^{1/2}) (\textbf{C}\textbf{D}^{1/2})^{\mathsf{T}}
\end{aligned}
\end{equation}

Non possiamo però limiarci a definire
$\hat{\boldsymbol{\Lambda}}=\textbf{C}\textbf{D}^{1/2}$ in quanto
$\textbf{C}\textbf{D}^{1/2}$ è di ordine $p \times p$ e non otteniamo
quindi una riduzione di dimensioni. Quello che cerchiamo è una matrice
$\hat{\boldsymbol{\Lambda}}$ di ordine $p \times m$ con $m < p$. Dunque,
definiamo la matrice $\textbf{D}_1= \text{diag}(\theta_1,
\theta_2, \dots, \theta_m)$ come la la matrice contenente gli $m$
autovalori più grandi di **R** e $\textbf{C}_1=( \textbf{c}_1,
\textbf{c}_2, \dots,  \textbf{c}_m)$ come la matrice contenente i
rispettivi autovettori. Mediante il metodo delle componenti principali,
le saturazioni fattoriali $\hat{\boldsymbol{\Lambda}}$ vengono quindi
stimate nel modo seguente: 

\begin{equation}
\begin{aligned}
\hat{\boldsymbol{\Lambda}} &= \textbf{C}_1 \textbf{D}_1^{1/2}\notag\\
&= (\sqrt{\theta_1} \textbf{c}_1, \sqrt{\theta_2} \textbf{c}_2, 
\dots, \sqrt{\theta_m} \textbf{c}_m) 
\end{aligned}
\end{equation}

Per l'esempio presente, con $m=2$ e $p=5$, avremo 

$$
\left[
  \begin{array}{ c c }
 \hat{\lambda}_{11} & \hat{\lambda}_{12} \\
 \hat{\lambda}_{21} & \hat{\lambda}_{22} \\
 \hat{\lambda}_{31} & \hat{\lambda}_{32} \\
 \hat{\lambda}_{41} & \hat{\lambda}_{42} \\
 \hat{\lambda}_{51} & \hat{\lambda}_{52} 
  \end{array} 
\right] =
\left[
  \begin{array}{ c c }
 c_{11} & c_{12} \\
 c_{21} & c_{22} \\
 c_{31} & c_{32} \\
 c_{41} & c_{42} \\
 c_{51} & c_{52} 
  \end{array} 
\right]
\left[
  \begin{array}{ c c }
 \sqrt{\theta_1} & 0\\
 0 &\sqrt{\theta_2} 
  \end{array} 
\right]
$$ 

Le saturazioni fattoriali stimate sono dunque uguali a

$$
\left[
  \begin{array}{ c c }
 \sqrt{\theta_1}c_{11} & \sqrt{\theta_2}c_{12} \\
 \sqrt{\theta_1}c_{21} & \sqrt{\theta_2}c_{22} \\
 \sqrt{\theta_1}c_{31} & \sqrt{\theta_2}c_{32} \\
 \sqrt{\theta_1}c_{41} & \sqrt{\theta_2}c_{42} \\
 \sqrt{\theta_1}c_{51} & \sqrt{\theta_2}c_{52} 
  \end{array} 
\right]
$$ 

Svolgendo i calcoli con $\textsf{R}$ otteniamo:

```{r}
L <- cbind(
  e$vectors[, 1] * sqrt(e$values[1]),
  e$vectors[, 2] * sqrt(e$values[2])
)

round(L, 3)
```

La matrice di correlazione riprodotta (con le comunalità sulla diagonale
principale) diventa

```{r}
round(L %*% t(L), 3)
```

Possiamo ora capire il motivo del nome "metodo delle componenti
principali." Le saturazioni fattoriali sono proporzionali agli
autovettori di $\textbf{R}$. Tuttavia, dopo la rotazione delle
saturazioni fattoriali, l'interpretazione dei fattori è diversa da
quella che viene assegnata ai risultai dell'analisi delle componenti
principali.

È possibile condurre l'analisi fattoriale con il metodo delle componenti
principali sia utilizzando la matrice $\textbf{S}$ di
varianze-covarianze sia la matrice $\textbf{R}$ delle correlazioni.
Tuttavia, le soluzioni ottenute usando $\textbf{S}$ o $\textbf{R}$ non
sono legate da una relazione algebrica: il metodo delle componenti
principali non è invariante rispetto ai cambiamenti di scala delle
osservazioni. Un altro svantaggio del metodo delle componenti principali
è che non fornisce un test di bontà di adattamento. Tale test può essere
invece svolto quando la soluzione viene trovata con il metodo della
massima verosimiglianza.

## Metodo dei fattori principali

Il *metodo dei fattori principali* (*principal factor method*, anche
detto *principal axis method*) è uno dei metodi maggiormente usati per
la stima delle saturazioni fattoriali e delle comunalità. Il metodo
delle componenti principali trascura la specificità $\boldsymbol{\Psi}$
e si limita a fattorializzare le covarianze di **S** o le correlazioni
di **R**. Il metodo dei fattori principali utilizza una procedura simile
al metodo delle componenti principali, utilizzando però una matrice
ridotta di varianze-covarianze $\textbf{S} -
  \hat{\boldsymbol{\Psi}}$ in cui una stima delle comunalità viene
sostituita alle varianze presenti sulla diagonale principale. Nel caso
della matrice ridotta di correlazioni
$\textbf{R} - \hat{\boldsymbol{\Psi}}$, per la comunalità $i$-esima
$\sum_{j}\lambda_{ij}^2$ si sceglie il quadrato del coefficiente di
correlazione multipla tra $Y_i$ e tutte le altre $p-1$ variabili. Tale
valore si può trovare nel modo seguente:

$$\hat{h}^2_i=R^2_i=1-\frac{1}{r^{ii}}$$ 

dove $r^{ii}$ è l'elemento
diagonale $i$-esimo di $\textbf{R}^{-1}$. Nel caso di una matrice
ridotta di varianze-covarianze $\textbf{S} - \hat{\boldsymbol{\Psi}}$,
le comunalità possono essere stimate calcolando

$$\hat{h}_i^2=s_{ii}-\frac{1}{r^{ii}}$$ 

dove $s_{ii}$ è l'elemento diagonale $i$-esimo di $\textbf{S}$.

Affinché le stime comunalità possano essere calcolate come descritto
sopra, la matrice $\textbf{R}$ deve essere non singolare. Nel caso in
cui $\textbf{R}$ sia singolare, per la stima della comunalità $i$-esima,
$\hat{h}^2_i$, si utilizza il valore assoluto del più elevato
coefficiente di correlazione lineare tra $Y_i$ e le altre variabili.

Scelta la stima della comunalità, la matrice ridotta di
varianze-covarianze si ottiene sostituendo alle varianze sulla diagonale
principale le stime delle comunalità:

$$
\textbf{S} - \hat{\boldsymbol{\Psi}} = 
\left[
  \begin{array}{ c c c c }
    \hat{h}^2_1 & s_{12} & \dots & s_{1p} \\
    s_{21} & \hat{h}^2_2 & \dots & s_{2p} \\
    \dots & \dots &           & \dots\\
    s_{p1} &  s_{p2} & \dots & \hat{h}^2_p
  \end{array} 
\right]
$$ 

In maniera equivalente, la matrice ridotta di correlazioni si
ottiene nel modo seguente:

$$
\textbf{R} - \hat{\boldsymbol{\Psi}} = 
\left[
  \begin{array}{ c c c c }
    \hat{h}^2_1 & r_{12} & \dots & r_{1p} \\
    r_{21} & \hat{h}^2_2 & \dots & r_{2p} \\
    \dots & \dots &           & \dots\\
    r_{p1} &  r_{p2} & \dots & \hat{h}^2_p
  \end{array} 
\right]
$$

Verranno ora svolti i calcoli necessari per la stima dei coefficienti di
saturazione con il metodo dei fattori principali utilizzando la matrice
di correlazione dell'esempio precedente. Quale stima della comunalità
$i$-esima, verrà utilizzato il valore assoluto più elevato nella riga
$i$-esima della matrice **R**. Per i dati dell'esempio, le stime delle
comunalità sono dunque pari a $0.995$, $0.837$, $0.881$, $0.995$ e
$0.837$.

Inserendo tali valori nella diagonale principale, otteniamo la matrice
ridotta delle correlazioni $\textbf{R} - \hat{\boldsymbol{\Psi}}$:

```{r}
R1 <- R
h.hat <- c(.995, .837, .881, .995, .837)
R1[cbind(1:5,1:5)] <- h.hat
R1
```

Gli autovalori della matrice ridotta di correlazioni
$\textbf{R} - \hat{\boldsymbol{\Psi}}$ sono:

```{r}
ee <- eigen(R1)
round(ee$values, 3)
```

La somma degli autovalori è uguale a

```{r}
sum(ee$values)
```

I primi due autovalori di $\textbf{R} - \hat{\boldsymbol{\Psi}}$ sono:

```{r}
round(ee$vectors[,1:2], 3)
```

Moltiplicando tali valori per la radice quadrata dei rispettivi
autovalori si ottengono le stime delle saturazioni fattoriali:

```{r}
round(ee$vectors[,1:2] %*% sqrt(diag(ee$values[1:2])), 3)
```

Tale risultato replica quello riportato da Rencher (2002).

## Metodo dei fattori principali iterato

Solitamente, per migliorare la stima della comunalità, la diagonale
della matrice $\textbf{S} - \hat{\boldsymbol{\Psi}}$ o
$\textbf{R} - \hat{\boldsymbol{\Psi}}$ viene ottenuta per iterazione.
Dopo avere trovato $\hat{\boldsymbol{\Lambda}}$ a partire da
$\textbf{S} - \hat{\boldsymbol{\Psi}}$ o
$\textbf{R} - \hat{\boldsymbol{\Psi}}$ come indicato in precedenza,
utilizzando le stime delle saturazioni fattoriali così ottenute possiamo
stimare le comunalità nel modo seguente:

$$\hat{h}^2_i = \sum_{i=1}^m \hat{\lambda}_{ij}^2.$$ 

I valori di $\hat{h}^2_i$ vengono quindi sostituiti nella diagonale della matrice
ridotta $\textbf{S} - \hat{\boldsymbol{\Psi}}$ o
$\textbf{R} - \hat{\boldsymbol{\Psi}}$. A partire da questa nuova
matrice, usando il metodo descritto in precedenza, possiamo così
ottenere una nuova stima delle saturazioni fattoriali
$\hat{\boldsymbol{\Lambda}}$. Mediante questa nuova stima di
$\hat{\boldsymbol{\Lambda}}$, possiamo procedere ad una nuova stima
delle comunalità. Tale processo continua iterativamente sino alla
convergenza. Gli autovalori e gli autovettori della versione finale di
$\textbf{S} - \hat{\boldsymbol{\Psi}}$ o
$\textbf{R} - \hat{\boldsymbol{\Psi}}$ vengono infine usati per stimare
i pesi fattoriali. Il metodo dei fattori principali iterato e il metodo
delle componenti principali producono risultati molto simili quando $m$
assume un piccolo valore (questo si verifica quando le correlazioni sono
alte) e quando $p$ (il numero delle variabili) è grande.

### Casi di Heywood

Tra gli inconvenienti del metodo dei fattori principali iterato vi è il
fatto che può talvolta portare a soluzioni inammissibili (quando viene
fattorizzata la matrice **R**) caratterizzate da valori di comunalità
maggiori di uno (*caso di Heywood*). Se $\hat{h}^2_i > 1$ allora
$\hat{\psi}_i < 0$ il che è chiaramente assurdo in quanto una varianza
non può assumere un valore negativo. Solitamente, quando la stima di una
comunalità è maggiore di uno, il processo iterativo viene interrotto e
il programma riporta che non può essere trovata una soluzione.

Nell'esempio presente viene utilizzata la funzione `factor.pa()`
contenuta nel pacchetto `psych` per trovare la soluzione dei fattori
principali mediante il metodo iterativo:

```{r}
pa <- fa(R, nfactors = 2, rotate = "none", fm = "pa")
pa
```

Si noti che, in questo caso, le unicità assumono valori negativi, il che
suggerisce che la soluzione è impropria.

## Metodo di massima verosimiglianza

L'applicazione del metodo di massima verosimiglianza è indicata quando
si può assumere che le variabili manifeste seguono una distribuzione
normale multivariata. Sotto tali condizioni, tale metodo produce le
stime dei pesi fattoriali che più verosimilmente hanno prodotto le
correlazioni osservate. Gli stimatori di massima verosimiglianza sono
preferibili a quelli ottenuti con altri metodi, sempre che siano
pienamente realizzate le premesse. La funzione $F$ da minimizzare
rappresenta una misura di "distanza" tra la matrice di covarianza
osservata e quella predetta dal modello. Uguagliando a zero le derivate
di $F$ rispetto a $\boldsymbol{\Lambda}$ e $\boldsymbol{\Psi}$ si
ottengono le equazioni per le stime di massima verosimiglianza di
$\hat{\boldsymbol{\Lambda}}$ e $\hat{\boldsymbol{\Psi}}$. Risolvendo
tali equazioni rispetto alle incognite $\hat{\boldsymbol{\Lambda}}$ e
$\hat{\boldsymbol{\Psi}}$ si ricavano le stime di massima
verosimiglianza.

Non esistendo una soluzione analitica per queste equazioni, si ricorre a
procedimenti numerici iterativi che talvolta presentano problemi di
convergenza. La soluzione, pur presentando la possibilità di fornire
delle stime di comunalità superiori a 1 (caso di Heywood), è
equivariante rispetto a cambiamenti di scala: le stime di massima
verosimiglianza sono indipendenti dall'unità di misura delle variabili
manifeste. Pertanto, si ottiene la stessa soluzione sia che si analizzi
la matrice delle varianze e covarianze, sia che si analizzi la matrice
delle correlazioni.

Consideriamo nuovamente i dati dell'esempio precedente. Le istruzioni
sono le seguenti:

```{r}
factanal(covmat=R, factors=2, rotation="none", n.obs=225)
```

